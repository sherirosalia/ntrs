title,score,search term,categories,abstract
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,124.28405,jpeg quality estimate,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,112.05164,jpeg quality estimate,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
A visual detection model for DCT coefficient quantization,80.16956,jpeg quality estimate,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
MPEG-2 Over Asynchronous Transfer Mode (ATM) Over Satellite Quality of Service (QoS) Experiments: Laboratory Tests,70.96164,jpeg quality estimate,"['Space Communications, Spacecraft Communications, Command and Tracking']","Asynchronous transfer mode (ATM) quality of service (QoS) experiments were performed using MPEG-2 (ATM application layer 5, AAL5) over ATM over an emulated satellite link. The purpose of these experiments was to determine the free-space link quality necessary to transmit high-quality multimedia information by using the ATM protocol. The detailed test plan and test configuration are described herein as are the test results. MPEG-2 transport streams were baselined in an errored environment, followed by a series of tests using, MPEG-2 over ATM. Errors were created both digitally as well as in an IF link by using a satellite modem and commercial gaussian noise test set for two different MPEG-2 decoder implementations. The results show that ITU-T Recommendation 1.356 Class 1, stringent ATM applications will require better link quality than currently specified; in particular, cell loss ratios of better than 1.0 x 10(exp -8) and cell error ratios of better than 1.0 x 10(exp -7) are needed. These tests were conducted at the NASA Lewis Research Center in support of satellite-ATM interoperability research."
BOREAS RSS-3 Imagery and Snapshots from a Helicopter-Mounted Video Camera,68.98389,jpeg quality estimate,['Earth Resources and Remote Sensing'],"The BOREAS RSS-3 team collected helicopter-based video coverage of forested sites acquired during BOREAS as well as single-frame ""snapshots"" processed to still images. Helicopter data used in this analysis were collected during all three 1994 IFCs (24-May to 16-Jun, 19-Jul to 10-Aug, and 30-Aug to 19-Sep), at numerous tower and auxiliary sites in both the NSA and the SSA. The VHS-camera observations correspond to other coincident helicopter measurements. The field of view of the camera is unknown. The video tapes are in both VHS and Beta format. The still images are stored in JPEG format."
BOREAS Level-0 C-130 Aerial Photography,68.973145,jpeg quality estimate,['Earth Resources and Remote Sensing'],"For BOReal Ecosystem-Atmosphere Study (BOREAS), C-130 and other aerial photography was collected to provide finely detailed and spatially extensive documentation of the condition of the primary study sites. The NASA C-130 Earth Resources aircraft can accommodate two mapping cameras during flight, each of which can be fitted with 6- or 12-inch focal-length lenses and black-and-white, natural-color, or color-IR film, depending upon requirements. Both cameras were often in operation simultaneously, although sometimes only the lower resolution camera was deployed. When both cameras were in operation, the higher resolution camera was often used in a more limited fashion. The acquired photography covers the period of April to September 1994. The aerial photography was delivered as rolls of large format (9 x 9 inch) color transparency prints, with imagery from multiple missions (hundreds of prints) often contained within a single roll. A total of 1533 frames were collected from the C-130 platform for BOREAS in 1994. Note that the level-0 C-130 transparencies are not contained on the BOREAS CD-ROM set. An inventory file is supplied on the CD-ROM to inform users of all the data that were collected. Some photographic prints were made from the transparencies. In addition, BORIS staff digitized a subset of the tranparencies and stored the images in JPEG format. The CD-ROM set contains a small subset of the collected aerial photography that were the digitally scanned and stored as JPEG files for most tower and auxiliary sites in the NSA and SSA. See Section 15 for information about how to acquire additional imagery."
Improved image decompression for reduced transform coding artifacts,66.610664,jpeg quality estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
High performance compression of science data,65.33191,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
"NASA Tech Briefs, September 2008",65.218864,jpeg quality estimate,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
System considerations for efficient communication and storage of MSTI image data,58.328014,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
An efficient system for reliably transmitting image and video data over low bit rate noisy channels,57.86446,jpeg quality estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"This research project is intended to develop an efficient system for reliably transmitting image and video data over low bit rate noisy channels. The basic ideas behind the proposed approach are the following: employ statistical-based image modeling to facilitate pre- and post-processing and error detection, use spare redundancy that the source compression did not remove to add robustness, and implement coded modulation to improve bandwidth efficiency and noise rejection. Over the last six months, progress has been made on various aspects of the project. Through our studies of the integrated system, a list-based iterative Trellis decoder has been developed. The decoder accepts feedback from a post-processor which can detect channel errors in the reconstructed image. The error detection is based on the Huber Markov random field image model for the compressed image. The compression scheme used here is that of JPEG (Joint Photographic Experts Group). Experiments were performed and the results are quite encouraging. The principal ideas here are extendable to other compression techniques. In addition, research was also performed on unequal error protection channel coding, subband vector quantization as a means of source coding, and post processing for reducing coding artifacts. Our studies on unequal error protection (UEP) coding for image transmission focused on examining the properties of the UEP capabilities of convolutional codes. The investigation of subband vector quantization employed a wavelet transform with special emphasis on exploiting interband redundancy. The outcome of this investigation included the development of three algorithms for subband vector quantization. The reduction of transform coding artifacts was studied with the aid of a non-Gaussian Markov random field model. This results in improved image decompression. These studies are summarized and the technical papers included in the appendices."
Photographic Volume Estimation of CPAS Main Parachutes,57.86125,jpeg quality estimate,['Aerodynamics'],"Capsule Parachute Assembly System (CPAS) flight tests regularly stage a helicopter to observe inflation of 116 ft D o ringsail Main parachutes. These side views can be used to generate 3-D models of inflating canopies to estimate enclosed volume. Assuming a surface of revolution is inadequate because reefed canopies in a cluster are elongated due to mutual aerodynamic interference. A method was developed to combine the side views with upward looking HD video to account for non-circular cross sections. Approximating the cross sections as elliptical greatly improves accuracy. But since that correction requires manually tracing projected outlines, the actual irregular shapes can be used to generate high fidelity models. Compensation is also made for apparent tilt angle. Validation was accomplished by comparing perimeter and projected area with known line lengths and/or high quality photogrammetry. "
Transform coding for space applications,56.996887,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"Data compression coding requirements for aerospace applications differ somewhat from the compression requirements for entertainment systems. On the one hand, entertainment applications are bit rate driven with the goal of getting the best quality possible with a given bandwidth. Science applications are quality driven with the goal of getting the lowest bit rate for a given level of reconstruction quality. In the past, the required quality level has been nothing less than perfect allowing only the use of lossless compression methods (if that). With the advent of better, faster, cheaper missions, an opportunity has arisen for lossy data compression methods to find a use in science applications as requirements for perfect quality reconstruction runs into cost constraints. This paper presents a review of the data compression problem from the space application perspective. Transform coding techniques are described and some simple, integer transforms are presented. The application of these transforms to space-based data compression problems is discussed. Integer transforms have an advantage over conventional transforms in computational complexity. Space applications are different from broadcast or entertainment in that it is desirable to have a simple encoder (in space) and tolerate a more complicated decoder (on the ground) rather than vice versa. Energy compaction with new transforms are compared with the Walsh-Hadamard (WHT), Discrete Cosine (DCT), and Integer Cosine (ICT) transforms."
COxSwAIN: Compressive Sensing for Advanced Imaging and Navigation,49.400455,jpeg quality estimate,"['Mathematical and Computer Sciences (General)', 'Communications and Radar']","The COxSwAIN project focuses on building an image and video compression scheme that can be implemented in a small or low-power satellite. To do this, we used Compressive Sensing, where the compression is performed by matrix multiplications on the satellite and reconstructed on the ground. Our paper explains our methodology and demonstrates the results of the scheme, being able to achieve high quality image compression that is robust to noise and corruption."
Synthetic aperture radar signal data compression using block adaptive quantization,48.53174,jpeg quality estimate,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
Image-adapted visually weighted quantization matrices for digital image compression,47.379807,jpeg quality estimate,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
"Denoising with Three Dimensional Fourier Transform for Three Dimensional Images, Including Image Sequences",47.289116,jpeg quality estimate,['Instrumentation and Photography'],"A method of mitigating noise in source image data representing pixels of a 3-D image. The ""3-D image"" may be any type of 3-D image, regardless of whether the third dimension is spatial, temporal, or some other parameter. The 3-D image is divided into three-dimensional chunks of pixels. These chunks are apodized and a three-dimensional Fourier transform is performed on each chunk, thereby producing a three-dimensional spectrum of each chunk. The transformed chunks are processed to estimate a noise floor based on spectral values of the pixels within each chunk. A noise threshold is then determined, and the spectrum of each chunk is filtered with a denoising filter based on the noise threshold. The chunks are then inverse transformed, and recombined into a denoised 3-D image."
Image data compression having minimum perceptual error,46.088017,jpeg quality estimate,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Learning random networks for compression of still and moving images,45.623283,jpeg quality estimate,['CYBERNETICS'],"Image compression for both still and moving images is an extremely important area of investigation, with numerous applications to videoconferencing, interactive education, home entertainment, and potential applications to earth observations, medical imaging, digital libraries, and many other areas. We describe work on a neural network methodology to compress/decompress still and moving images. We use the 'point-process' type neural network model which is closer to biophysical reality than standard models, and yet is mathematically much more tractable. We currently achieve compression ratios of the order of 120:1 for moving grey-level images, based on a combination of motion detection and compression. The observed signal-to-noise ratio varies from values above 25 to more than 35. The method is computationally fast so that compression and decompression can be carried out in real-time. It uses the adaptive capabilities of a set of neural networks so as to select varying compression ratios in real-time as a function of quality achieved. It also uses a motion detector which will avoid retransmitting portions of the image which have varied little from the previous frame. Further improvements can be achieved by using on-line learning during compression, and by appropriate compensation of nonlinearities in the compression/decompression scheme. We expect to go well beyond the 250:1 compression level for color images with good quality levels."
Compressing subbanded image data with Lempel-Ziv-based coders,45.39139,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"A method of improving the compression of image data using Lempel-Ziv-based coding is presented. Image data is first processed with a simple transform, such as the Walsh Hadamard Transform, to produce subbands. The subbanded data can be rounded to eight bits or it can be quantized for higher compression at the cost of some reduction in the quality of the reconstructed image. The data is then run-length coded to take advantage of the large runs of zeros produced by quantization. Compression results are presented and contrasted with a subband compression method using quantization followed by run-length coding and Huffman coding. The Lempel-Ziv-based coding in conjunction with run-length coding produces the best compression results at the same reconstruction quality (compared with the Huffman-based coding) on the image data used."
Sub-band/transform compression of video sequences,43.95893,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Clementine High Resolution Camera Mosaicking Project,42.40429,jpeg quality estimate,['Lunar and Planetary Science and Exploration'],"This report constitutes the final report for NASA Contract NASW-5054. This project processed Clementine I high resolution images of the Moon, mosaicked these images together, and created a 22-disk set of compact disk read-only memory (CD-ROM) volumes. The mosaics were produced through semi-automated registration and calibration of the high resolution (HiRes) camera's data against the geometrically and photometrically controlled Ultraviolet/Visible (UV/Vis) Basemap Mosaic produced by the US Geological Survey (USGS). The HiRes mosaics were compiled from non-uniformity corrected, 750 nanometer (""D"") filter high resolution nadir-looking observations. The images were spatially warped using the sinusoidal equal-area projection at a scale of 20 m/pixel for sub-polar mosaics (below 80 deg. latitude) and using the stereographic projection at a scale of 30 m/pixel for polar mosaics. Only images with emission angles less than approximately 50 were used. Images from non-mapping cross-track slews, which tended to have large SPICE errors, were generally omitted. The locations of the resulting image population were found to be offset from the UV/Vis basemap by up to 13 km (0.4 deg.). Geometric control was taken from the 100 m/pixel global and 150 m/pixel polar USGS Clementine Basemap Mosaics compiled from the 750 nm Ultraviolet/Visible Clementine imaging system. Radiometric calibration was achieved by removing the image nonuniformity dominated by the HiRes system's light intensifier. Also provided are offset and scale factors, achieved by a fit of the HiRes data to the corresponding photometrically calibrated UV/Vis basemap, that approximately transform the 8-bit HiRes data to photometric units. The sub-polar mosaics are divided into tiles that cover approximately 1.75 deg. of latitude and span the longitude range of the mosaicked frames. Images from a given orbit are map projected using the orbit's nominal central latitude. Polar mosaics are tiled into squares 2250 pixels on a side, which spans approximately 2.2 deg. Two mosaics are provided for each pole: one corresponding to data acquired while periapsis was in the south, the other while periapsis was in the north. The CD-ROMs also contain ancillary data files that support the HiRes mosaic. These files include browse images with UV/Vis context stored in a Joint Photographic Experts Group (JPEG) format, index files ('imgindx.tab' and 'srcindx.tab') that tabulate the contents of the CD, and documentation files."
"The Spatial Vision Tree: A Generic Pattern Recognition Engine- Scientific Foundations, Design Principles, and Preliminary Tree Design",41.927414,jpeg quality estimate,['Instrumentation and Photography'],"New foundational ideas are used to define a novel approach to generic visual pattern recognition. These ideas proceed from the starting point of the intrinsic equivalence of noise reduction and pattern recognition when noise reduction is taken to its theoretical limit of explicit matched filtering. This led us to think of the logical extension of sparse coding using basis function transforms for both de-noising and pattern recognition to the full pattern specificity of a lexicon of matched filter pattern templates. A key hypothesis is that such a lexicon can be constructed and is, in fact, a generic visual alphabet of spatial vision. Hence it provides a tractable solution for the design of a generic pattern recognition engine. Here we present the key scientific ideas, the basic design principles which emerge from these ideas, and a preliminary design of the Spatial Vision Tree (SVT). The latter is based upon a cryptographic approach whereby we measure a large aggregate estimate of the frequency of occurrence (FOO) for each pattern. These distributions are employed together with Hamming distance criteria to design a two-tier tree. Then using information theory, these same FOO distributions are used to define a precise method for pattern representation. Finally the experimental performance of the preliminary SVT on computer generated test images and complex natural images is assessed."
1994 Science Information Management and Data Compression Workshop,37.051563,jpeg quality estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Video transmission on ATM networks,36.811398,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
Distant Operational Care Centre: Design Project Report,35.84636,jpeg quality estimate,['Astronautics (General)'],"The goal of this project is to outline the design of the Distant Operational Care Centre (DOCC), a modular medical facility to maintain human health and performance in space, that is adaptable to a range of remote human habitats. The purpose of this project is to outline a design, not to go into a complete technical specification of a medical facility for space. This project involves a process to produce a concise set of requirements, addressing the fundamental problems and issues regarding all aspects of a space medical facility for the future. The ideas presented here are at a high level, based on existing, researched, and hypothetical technologies. Given the long development times for space exploration, the outlined concepts from this project embodies a collection of identified problems, and corresponding proposed solutions and ideas, ready to contribute to future space exploration efforts. In order to provide a solid extrapolation and speculation in the context of the future of space medicine, the extent of this project's vision is roughly within the next two decades. The Distant Operational Care Centre (DOCC) is a modular medical facility for space. That is, its function is to maintain human health and performance in space environments, through prevention, diagnosis, and treatment. Furthermore, the DOCC must be adaptable to meet the environmental requirements of different remote human habitats, and support a high quality of human performance. To meet a diverse range of remote human habitats, the DOCC concentrates on a core medical capability that can then be adapted. Adaptation would make use of the DOCC's functional modularity, providing the ability to replace, add, and modify core functions of the DOCC by updating hardware, operations, and procedures. Some of the challenges to be addressed by this project include what constitutes the core medical capability in terms of hardware, operations, and procedures, and how DOCC can be adapted to different remote habitats."
Concept report: Experimental vector magnetograph (EXVM) operational configuration balloon flight assembly,35.614174,jpeg quality estimate,['AERODYNAMICS'],"The observational limitations of earth bound solar studies has prompted a great deal of interest in recent months in being able to gain new scientific perspectives through, what should prove to be, relatively low cost flight of the magnetograph system. The ground work done by TBE for the solar balloon missions (originally planned for SOUP and GRID) as well as the rather advanced state of assembly of the EXVM has allowed the quick formulation of a mission concept for the 30 cm system currently being assembled. The flight system operational configuration will be discussed as it is proposed for short duration flight (on the order of one day) over the continental United States. Balloon hardware design requirements used in formulation of the concept are those set by the National Science Balloon Facility (NSBF), the support agency under NASA contract for flight services. The concept assumes that the flight hardware assembly would come together from three development sources: the scientific investigator package, the integration contractor package, and the NSBF support system. The majority of these three separate packages can be independently developed; however, the computer control interfaces and telemetry links would require extensive preplanning and coordination. A special section of this study deals with definition of a dedicated telemetry link to be provided by the integration contractor for video image data for pointing system performance verification. In this study the approach has been to capitalize to the maximum extent possible on existing hardware and system design. This is the most prudent step that can be taken to reduce eventual program cost for long duration flights. By fielding the existing EXVM as quickly as possible, experience could be gained from several short duration flight tests before it became necessary to commit to major upgrades for long duration flights of this system or of the larger 60 cm version being considered for eventual development."
Image Registration Workshop Proceedings,35.060715,jpeg quality estimate,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
The 1993 Space and Earth Science Data Compression Workshop,34.357193,jpeg quality estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",34.306858,jpeg quality estimate,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
Conditional Entropy-Constrained Residual VQ with Application to Image Coding,34.076298,jpeg quality estimate,['Computer Programming and Software'],"This paper introduces an extension of entropy-constrained residual vector quantization (VQ) where intervector dependencies are exploited. The method, which we call conditional entropy-constrained residual VQ, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. When applied to coding images, the proposed method is shown to achieve better rate-distortion performance than that of entropy-constrained residual vector quantization with less computational complexity and lower memory requirements. Moreover, it can be designed to support progressive transmission in a natural way. It is also shown to outperform some of the best predictive and finite-state VQ techniques reported in the literature. This is due partly to the joint optimization between the residual vector quantizer and a high-order conditional entropy coder as well as the efficiency of the multistage residual VQ structure and the dynamic nature of the prediction."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,34.018925,jpeg quality estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
Studies on image compression and image reconstruction,33.922443,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
The 1995 Science Information Management and Data Compression Workshop,33.206856,jpeg quality estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,32.632263,jpeg quality estimate,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
The Space and Earth Science Data Compression Workshop,31.898085,jpeg quality estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
Searching for patterns in remote sensing image databases using neural networks,31.61334,jpeg quality estimate,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
The importance of robust error control in data compression applications,31.559937,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
Spinoff 2013,31.460157,jpeg quality estimate,['Man/System Technology and Life Support'],"Topics covered include: Innovative Software Tools Measure Behavioral Alertness; Miniaturized, Portable Sensors Monitor Metabolic Health; Patient Simulators Train Emergency Caregivers; Solar Refrigerators Store Life-Saving Vaccines; Monitors Enable Medication Management in Patients' Homes; Handheld Diagnostic Device Delivers Quick Medical Readings; Experiments Result in Safer, Spin-Resistant Aircraft; Interfaces Visualize Data for Airline Safety, Efficiency; Data Mining Tools Make Flights Safer, More Efficient; NASA Standards Inform Comfortable Car Seats; Heat Shield Paves the Way for Commercial Space; Air Systems Provide Life Support to Miners; Coatings Preserve Metal, Stone, Tile, and Concrete; Robots Spur Software That Lends a Hand; Cloud-Based Data Sharing Connects Emergency Managers; Catalytic Converters Maintain Air Quality in Mines; NASA-Enhanced Water Bottles Filter Water on the Go; Brainwave Monitoring Software Improves Distracted Minds; Thermal Materials Protect Priceless, Personal Keepsakes; Home Air Purifiers Eradicate Harmful Pathogens; Thermal Materials Drive Professional Apparel Line; Radiant Barriers Save Energy in Buildings; Open Source Initiative Powers Real-Time Data Streams; Shuttle Engine Designs Revolutionize Solar Power; Procedure-Authoring Tool Improves Safety on Oil Rigs; Satellite Data Aid Monitoring of Nation's Forests; Mars Technologies Spawn Durable Wind Turbines; Programs Visualize Earth and Space for Interactive Education; Processor Units Reduce Satellite Construction Costs; Software Accelerates Computing Time for Complex Math; Simulation Tools Prevent Signal Interference on Spacecraft; Software Simplifies the Sharing of Numerical Models; Virtual Machine Language Controls Remote Devices; Micro-Accelerometers Monitor Equipment Health; Reactors Save Energy, Costs for Hydrogen Production; Cameras Monitor Spacecraft Integrity to Prevent Failures; Testing Devices Garner Data on Insulation Performance; Smart Sensors Gather Information for Machine Diagnostics; Oxygen Sensors Monitor Bioreactors and Ensure Health and Safety; Vision Algorithms Catch Defects in Screen Displays; and Deformable Mirrors Capture Exoplanet Data, Reflect Lasers. "
Fast image decompression for telebrowsing of images,31.147066,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Progressive image transmission (PIT) is often used to reduce the transmission time of an image telebrowsing system. A side effect of the PIT is the increase of computational complexity at the viewer's site. This effect is more serious in transform domain techniques than in other techniques. Recent attempts to reduce the side effect are futile as they create another side effect, namely, the discontinuous and unpleasant image build-up. Based on a practical assumption that image blocks to be inverse transformed are generally sparse, this paper presents a method to minimize both side effects simultaneously."
Data compression for full motion video transmission,30.986658,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
Aeronautics and Aviation Science: Careers and Opportunities Project,30.986198,jpeg quality estimate,['Social Sciences (General)'],"The National Aeronautics and Space Administration funded project, Aeronautics and Aviation Science: Careers and Opportunities has been in operation since July, 1995. This project operated as a collaboration with Massachusetts Corporation for Educational Telecommunications, the Federal Aviation Administration, Bridgewater State College and four targeted ""core sites"" in the greater Boston area. In its first and second years, a video series on aeronautics and aviation science was developed and broadcast via ""live, interactive"" satellite feed. Accompanying teacher and student supplementary instructional materials for grades 6-9 were produced and disseminated by the Massachusetts Corporation for Educational Telecommunications (MCET). In the MCET grant application it states that project Take Off! in its initial phase would recruit and train teachers at ""core"" sites in the greater Boston area, as well as opening participation to other on-line users of MCET's satellite feeds. ""Core site"" classrooms would become equipped so that teachers and students might become engaged in an interactive format which aimed at not only involving the students during the ""live"" broadcast of the instructional video series, but which would encourage participation in electronic information gathering and sharing among participants. As a Take Off! project goal, four schools with a higher than average proportion of minority and underrepresented youth were invited to become involved with the project to give these students the opportunity to consider career exploration and development in the field of science aviation and aeronautics. The four sites chosen to participate in this project were: East Boston High School, Dorchester High School, Randolph Junior-Senior High School and Malden High School. In year 3 Dorchester was unable to continue to fully participate and exited out. Danvers was added to the ""core site"" list in year 3. In consideration of Goals 2000, the National Science Foundation standards for quality of teaching, and an educational agenda that promotes high standards for all students, Aeronautics and Aviation Science: Careers and Opportunities had as its aim to deliver products to schools, both in and outside the project sites, which attempt to incorporate multi-disciplined approaches in the presentation of a curriculum which would be appropriate in any classroom, while also aiming to appeal to young women and minorities. The curriculum was developed to provide students with fundamentals of aeronautics and aviation science. The curriculum also encouraged involving students and teachers in research projects, and further information gathering via electronic bulletin boards and internet capabilities. Though not entirely prescriptive, the curriculum was designed to guide teachers through recommended activities to supplement MCET's live telecast video presentations. Classroom teachers were encouraged to invite local pilots, meteorologists, and others from the field of aviation and aeronautics, particularly women and minorities to visit schools and to field questions from the students."
Technology Transfer Report,30.944477,jpeg quality estimate,['Technology Utilization and Surface Transportation'],"Since its inception, Goddard has pursued a commitment to technology transfer and commercialization. For every space technology developed, Goddard strives to identify secondary applications. Goddard then provides the technologies, as well as NASA expertise and facilities, to U.S. companies, universities, and government agencies. These efforts are based in Goddard's Technology Commercialization Office. This report presents new technologies, commercialization success stories, and other Technology Commercialization Office activities in 1999."
Planning/scheduling techniques for VQ-based image compression,30.371078,jpeg quality estimate,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
The 1992 4th NASA SERC Symposium on VLSI Design,30.246048,jpeg quality estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"Papers from the fourth annual NASA Symposium on VLSI Design, co-sponsored by the IEEE, are presented. Each year this symposium is organized by the NASA Space Engineering Research Center (SERC) at the University of Idaho and is held in conjunction with a quarterly meeting of the NASA Data System Technology Working Group (DSTWG). One task of the DSTWG is to develop new electronic technologies that will meet next generation electronic data system needs. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The NASA SERC is proud to offer, at its fourth symposium on VLSI design, presentations by an outstanding set of individuals from national laboratories, the electronics industry, and universities. These speakers share insights into next generation advances that will serve as a basis for future VLSI design."
Advanced Shipboard Communications Demonstrations with ACTS,29.777143,jpeg quality estimate,['Communications and Radar'],"For ships at sea. satellites provide the only option for high data rate (HDR), long haul communications. Furthermore the demand for HDR satellite communications (SATCOM) for military and commercial ships. and other offshore platforms is increasing. Presently the bulk of this maritime HDR SATCOM connectivity is provided via C-band and X-band. However, the shipboard antenna sizes required to achieve a data rate of, say T 1 (1.544 Mbps) with present C-/X-band SATCOM systems range from seven to ten feet in diameter. This limits the classes of ships to which HDR services can be provided to those which are large enough to accommodate the massive antennas. With its high powered K/Ka-band spot beams, the National Aeronautics and Space Administration's (NASA) Advanced Communications Technology Satellite (ACTS) was able to provide T I and higher rate services to ships at sea using much smaller shipboard antennas. This paper discusses three shipboard HDR SATCOM demonstrations that were conducted with ACTS between 1996 and 1998. The first demonstration involved a 2 Mbps link provided to the seismic survey ship MN Geco Diamond equipped with a 16-inch wide, 4.5-inch tall, mechanically steered slotted waveguide array antenna developed by the Jet Propulsion Laboratory. In this February 1996 demonstration ACTS allowed supercomputers ashore to process Geco Diamond's voluminous oceanographic seismic data in near real time. This capability allowed the ship to adjust its search parameters on a daily basis based on feedback from the processed data, thereby greatly increasing survey efficiency. The second demonstration was conducted on the US Navy cruiser USS Princeton (CG 59) with the same antenna used on Geco Diamond. Princeton conducted a six-month (January-July 1997) Western Hemisphere solo deployment during which time T1 connectivity via ACTS provided the ship with a range of valuable tools for operational, administrative and quality-of-life tasks. In one instance, video teleconferencing (VTC) via ACTS allowed the ship to provide life-saving emergency medical aid, assisted by specialists ashore. to a fellow mariner - the Master of a Greek cargo ship. The third demonstration set what is believed to be the all-time SATCOM data rate record to a ship at sea, 45 Mbps in October 1998. This Lake Michigan (Chicago area) demonstration employed one of ACTS' fixed beams and involved the smallest of the three vessels, the 45-foot Bayliner M/V Entropy equipped with a modified commercial-off-the-shelf one-meter antenna. A variety of multi-media services were provided to Entropy through a stressing range of sea states. These three demonstrations provided a preview of the capabilities that could be provided to future mariners on a more routine basis when K/Ka-band SATCOM systems are widely deployed."
"Robo-line storage: Low latency, high capacity storage systems over geographically distributed networks",29.49691,jpeg quality estimate,['COMPUTER OPERATIONS AND HARDWARE'],"Rapid advances in high performance computing are making possible more complete and accurate computer-based modeling of complex physical phenomena, such as weather front interactions, dynamics of chemical reactions, numerical aerodynamic analysis of airframes, and ocean-land-atmosphere interactions. Many of these 'grand challenge' applications are as demanding of the underlying storage system, in terms of their capacity and bandwidth requirements, as they are on the computational power of the processor. A global view of the Earth's ocean chlorophyll and land vegetation requires over 2 terabytes of raw satellite image data. In this paper, we describe our planned research program in high capacity, high bandwidth storage systems. The project has four overall goals. First, we will examine new methods for high capacity storage systems, made possible by low cost, small form factor magnetic and optical tape systems. Second, access to the storage system will be low latency and high bandwidth. To achieve this, we must interleave data transfer at all levels of the storage system, including devices, controllers, servers, and communications links. Latency will be reduced by extensive caching throughout the storage hierarchy. Third, we will provide effective management of a storage hierarchy, extending the techniques already developed for the Log Structured File System. Finally, we will construct a protototype high capacity file server, suitable for use on the National Research and Education Network (NREN). Such research must be a Cornerstone of any coherent program in high performance computing and communications."
Photogrammetry of a 5m Inflatable Space Antenna With Consumer Digital Cameras,29.398233,jpeg quality estimate,['Structural Mechanics'],"This paper discusses photogrammetric measurements of a 5m-diameter inflatable space antenna using four Kodak DC290 (2.1 megapixel) digital cameras. The study had two objectives: 1) Determine the photogrammetric measurement precision obtained using multiple consumer-grade digital cameras and 2) Gain experience with new commercial photogrammetry software packages, specifically PhotoModeler Pro from Eos Systems, Inc. The paper covers the eight steps required using this hardware/software combination. The baseline data set contained four images of the structure taken from various viewing directions. Each image came from a separate camera. This approach simulated the situation of using multiple time-synchronized cameras, which will be required in future tests of vibrating or deploying ultra-lightweight space structures. With four images, the average measurement precision for more than 500 points on the antenna surface was less than 0.020 inches in-plane and approximately 0.050 inches out-of-plane."
Image compression system and method having optimized quantization tables,29.282547,jpeg quality estimate,['Instrumentation and Photography'],"A digital image compression preprocessor for use in a discrete cosine transform-based digital image compression device is provided. The preprocessor includes a gathering mechanism for determining discrete cosine transform statistics from input digital image data. A computing mechanism is operatively coupled to the gathering mechanism to calculate a image distortion array and a rate of image compression array based upon the discrete cosine transform statistics for each possible quantization value. A dynamic programming mechanism is operatively coupled to the computing mechanism to optimize the rate of image compression array against the image distortion array such that a rate-distortion-optimal quantization table is derived. In addition, a discrete cosine transform-based digital image compression device and a discrete cosine transform-based digital image compression and decompression system are provided. Also, a method for generating a rate-distortion-optimal quantization table, using discrete cosine transform-based digital image compression, and operating a discrete cosine transform-based digital image compression and decompression system are provided."
Study and simulation of low rate video coding schemes,28.964878,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
The Telecommunications and Data Acquisition Report,28.614256,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"A compilation is presented of articles on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition. In space communications, radio navigation, radio science, and ground based radio and radar astronomy, activities of the Deep Space Network are reported in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA. In the search for extraterrestrial intelligence (SETI), implementation and operations are reported for searching the microwave spectrum."
Remote sensing and the Mississippi high accuracy reference network,28.531721,jpeg quality estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Since 1986, NASA's Commercial Remote Sensing Program (CRSP) at Stennis Space Center has supported commercial remote sensing partnerships with industry. CRSP's mission is to maximize U.S. market exploitation of remote sensing and related space-based technologies and to develop advanced technical solutions for spatial information requirements. Observation, geolocation, and communications technologies are converging and their integration is critical to realize the economic potential for spatial informational needs. Global positioning system (GPS) technology enables a virtual revolution in geopositionally accurate remote sensing of the earth. A majority of states are creating GPS-based reference networks, or high accuracy reference networks (HARN). A HARN can be defined for a variety of local applications and tied to aerial or satellite observations to provide an important contribution to geographic information systems (GIS). This paper details CRSP's experience in the design and implementation of a HARN in Mississippi and the design and support of future applications of integrated earth observations, geolocation, and communications technology."
Advances in Remote Sensing for Vegetation Dynamics and Agricultural Management,28.48367,jpeg quality estimate,['Earth Resources and Remote Sensing'],"Spaceborne remote sensing has led to great advances in the global monitoring of vegetation. For example, the NASA Global Inventory Modeling and Mapping Studies (GIMMS) group has developed widely used datasets from the Advanced Very High Resolution Radiometer (AVHRR) sensors as well as the Moderate Resolution Imaging Spectroradiometer (MODIS) map imagery and normalized difference vegetation index datasets. These data are valuable for analyzing vegetation trends and variability at the regional and global levels. Numerous studies have investigated such trends and variability for both natural vegetation (e.g., re-greening of the Sahel, shifts in the Eurasian boreal forest, Amazonian drought sensitivity) and crops (e.g., impacts of extremes on agricultural production). Here, a critical overview is presented on recent developments and opportunities in the use of remote sensing for monitoring vegetation and crop dynamics.


"
Low Resolution Picture Transmission (LRPT) Demonstration System,28.260418,jpeg quality estimate,['Communications and Radar'],"Low-Resolution Picture Transmission (LRPT) is a proposed standard for direct broadcast transmission of satellite weather images. This standard is a joint effort by the European Organization for the Exploitation of Meteorological Satellites (EUMETSAT) and NOAA. As a digital transmission scheme, its purpose is to replace the current analog Automatic Picture Transmission (APT) system for use in the Meteorological Operational (METOP) satellites. GSFC has been tasked to build an LRPT Demonstration System (LDS). Its main objective is to develop or demonstrate the feasibility of a low-cost receiver utilizing a PC as the primary processing component and determine the performance of the protocol in the simulated Radio Frequency (RF) environment. The approach would consist of two phases."
GIF Animation of Mode Shapes and Other Data on the Internet,28.094185,jpeg quality estimate,['Structural Mechanics'],"The World Wide Web abounds with animated cartoons and advertisements competing for our attention. Most of these figures are animated Graphics Interchange Format (GIF) files. These files contain a series of ordinary GIF images plus control information, and they provide an exceptionally simple, effective way to animate on the Internet. To date, however, this format has rarely been used for technical data, although there is no inherent reason not to do so. This paper describes a procedure for creating high-resolution animated GIFs of mode shapes and other types of structural dynamics data with readily available software. The paper shows three example applications using recent modal test data and video footage of a high-speed sled run. A fairly detailed summary of the GIF file format is provided in the appendix. All of the animations discussed in the paper are posted on the Internet available through the following address: http://sdb-www.larc.nasa.gov/."
Studies and simulations of the DigiCipher system,27.93914,jpeg quality estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this period the development of simulators for the various high definition television (HDTV) systems proposed to the FCC was continued. The FCC has indicated that it wants the various proposers to collaborate on a single system. Based on all available information this system will look very much like the advanced digital television (ADTV) system with major contributions only from the DigiCipher system. The results of our simulations of the DigiCipher system are described. This simulator was tested using test sequences from the MPEG committee. The results are extrapolated to HDTV video sequences. Once again, some caveats are in order. The sequences used for testing the simulator and generating the results are those used for testing the MPEG algorithm. The sequences are of much lower resolution than the HDTV sequences would be, and therefore the extrapolations are not totally accurate. One would expect to get significantly higher compression in terms of bits per pixel with sequences that are of higher resolution. However, the simulator itself is a valid one, and should HDTV sequences become available, they could be used directly with the simulator. A brief overview of the DigiCipher system is given. Some coding results obtained using the simulator are looked at. These results are compared to those obtained using the ADTV system. These results are evaluated in the context of the CCSDS specifications and make some suggestions as to how the DigiCipher system could be implemented in the NASA network. Simulations such as the ones reported can be biased depending on the particular source sequence used. In order to get more complete information about the system one needs to obtain a reasonable set of models which mirror the various kinds of sources encountered during video coding. A set of models which can be used to effectively model the various possible scenarios is provided. As this is somewhat tangential to the other work reported, the results are included as an appendix."
ARES Biennial Report 2012 Final,27.86633,jpeg quality estimate,['Ground Support Systems and Facilities (Space)'],"Since the return of the first lunar samples, what is now the Astromaterials Research and Exploration Science (ARES) Directorate has had curatorial responsibility for all NASA-held extraterrestrial materials. Originating during the Apollo Program (1960s), this capability at Johnson Space Center (JSC) included scientists who were responsible for the science planning and training of astronauts for lunar surface activities as well as experts in the analysis and preservation of the precious returned samples. Today, ARES conducts research in basic and applied space and planetary science, and its scientific staff represents a broad diversity of expertise in the physical sciences (physics, chemistry, geology, astronomy), mathematics, and engineering organized into three offices (figure 1): Astromaterials Research (KR), Astromaterials Acquisition and Curation (KT), and Human Exploration Science (KX). Scientists within the Astromaterials Acquisition and Curation Office preserve, protect, document, and distribute samples of the current astromaterials collections. Since the return of the first lunar samples, ARES has been assigned curatorial responsibility for all NASA-held extraterrestrial materials (Apollo lunar samples, Antarctic meteorites - some of which have been confirmed to have originated on the Moon and on Mars - cosmic dust, solar wind samples, comet and interstellar dust particles, and space-exposed hardware). The responsibilities of curation consist not only of the longterm care of the samples, but also the support and planning for future sample collection missions and research and technology to enable new sample types. Curation provides the foundation for research into the samples. The Lunar Sample Facility and other curation clean rooms, the data center, laboratories, and associated instrumentation are unique NASA resources that, together with our staff's fundamental understanding of the entire collection, provide a service to the external research community, which relies on access to the samples. The curation efforts are greatly enhanced by a strong group of planetary scientists who conduct peerreviewed astromaterials research. Astromaterials Research Office scientists conduct peer-reviewed research as Principal or Co-Investigators in planetary science (e. g., cosmochemistry, origins of solar systems, Mars fundamental research, planetary geology and geophysics) and participate as Co-Investigators or Participating Scientists in many of NASA's robotic planetary missions. Since the last report, ARES has achieved several noteworthy milestones, some of which are documented in detail in the sections that follow. Within the Human Exploration Science Office, ARES is a world leader in orbital debris research, modeling and monitoring the debris environment, designing debris shielding, and developing policy to control and mitigate the orbital debris population. ARES has aggressively pursued refinements in knowledge of the debris environment and the hazard it presents to spacecraft. Additionally, the ARES Image Science and Analysis Group has been recognized as world class as a result of the high quality of near-real-time analysis of ascent and on-orbit inspection imagery to identify debris shedding, anomalies, and associated potential damage during Space Shuttle missions. ARES Earth scientists manage and continuously update the database of astronaut photography that is predominantly from Shuttle and ISS missions, but also includes the results of 40 years of human spaceflight. The Crew Earth Observations Web site (http://eol.jsc.nasa.gov/Education/ESS/crew.htm) continues to receive several million hits per month. ARES scientists are also influencing decisions in the development of the next generation of human and robotic spacecraft and missions through laboratory tests on the optical qualities of materials for windows, micrometeoroid/orbital debris shielding technology, and analog activities to assess surface science operations. ARES serves as host to numerous students and visiting scientists as part of the services provided to the research community and conducts a robust education and outreach program. ARES scientists are recognized nationally and internationally by virtue of their success in publishing in peer-reviewed journals and winning competitive research proposals. ARES scientists have won every major award presented by the Meteoritical Society, including the Leonard Medal, the most prestigious award in planetary science and cosmochemistry; the Barringer Medal, recognizing outstanding work in the field of impact cratering; the Nier Prize for outstanding research by a young scientist; and several recipients of the Nininger Meteorite Award. One of our scientists received the Department of Defense (DoD) Joint Meritorious Civilian Service Award (the highest civilian honor given by the DoD). ARES has established numerous partnerships with other NASA Centers, universities, and national laboratories. ARES scientists serve as journal editors, members of advisory panels and review committees, and society officers, and several scientists have been elected as Fellows in their professional societies. This biennial report summarizes a subset of the accomplishments made by each of the ARES offices and highlights participation in ongoing human and robotic missions, development of new missions, and planning for future human and robotic exploration of the solar system beyond low Earth orbit."
"Power, Avionics and Software - Phase 1.0:",27.787762,jpeg quality estimate,"['Space Communications, Spacecraft Communications, Command and Tracking', 'Computer Programming and Software']","This report describes Power, Avionics and Software (PAS) 1.0 subsystem integration testing and test results that occurred in August and September of 2013. This report covers the capabilities of each PAS assembly to meet integration test objectives for non-safety critical, non-flight, non-human-rated hardware and software development. This test report is the outcome of the first integration of the PAS subsystem and is meant to provide data for subsequent designs, development and testing of the future PAS subsystems. The two main objectives were to assess the ability of the PAS assemblies to exchange messages and to perform audio testing of both inbound and outbound channels. This report describes each test performed, defines the test, the data, and provides conclusions and recommendations."
"A Scalable, Out-of-Band Diagnostics Architecture for International Space Station Systems Support",27.774376,jpeg quality estimate,"['Spacecraft Design, Testing and Performance']","The computational infrastructure of the International Space Station (ISS) is a dynamic system that supports multiple vehicle subsystems such as Caution and Warning, Electrical Power Systems and Command and Data Handling (C&DH), as well as scientific payloads of varying size and complexity. The dynamic nature of the ISS configuration coupled with the increased demand for payload support places a significant burden on the inherently resource constrained computational infrastructure of the ISS. Onboard system diagnostics applications are hosted on computers that are elements of the avionics network while ground-based diagnostic applications receive only a subset of available telemetry, down-linked via S-band communications. In this paper we propose a scalable, out-of-band diagnostics architecture for ISS systems support that uses a read-only connection for C&DH data acquisition, which provides a lower cost of deployment and maintenance (versus a higher criticality readwrite connection). The diagnostics processing burden is off-loaded from the avionics network to elements of the on-board LAN that have a lower overall cost of operation and increased computational capacity. A superset of diagnostic data, richer in content than the configured telemetry, is made available to Advanced Diagnostic System (ADS) clients running on wireless handheld devices, affording the crew greater mobility for troubleshooting and providing improved insight into vehicle state. The superset of diagnostic data is made available to the ground in near real-time via an out-of band downlink, providing a high level of fidelity between vehicle state and test, training and operational facilities on the ground."
The Fifth NASA Symposium on VLSI Design,26.93227,jpeg quality estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"The fifth annual NASA Symposium on VLSI Design had 13 sessions including Radiation Effects, Architectures, Mixed Signal, Design Techniques, Fault Testing, Synthesis, Signal Processing, and other Featured Presentations. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The presentations share insights into next generation advances that will serve as a basis for future VLSI design."
Progressive transmission and compression images,26.810379,jpeg quality estimate,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
"NASAwide electronic publishing system: Prototype STI electronic document distribution, stage-4 evaluation report",26.553596,jpeg quality estimate,['Documentation and Information Science'],"This evaluation report contains an introduction, seven chapters, and five appendices. The Introduction describes the purpose, conceptual frame work, functional description, and technical report server of the STI Electronic Document Distribution (EDD) project. Chapter 1 documents the results of the prototype STI EDD in actual operation. Chapter 2 documents each NASA center's post processing publication processes. Chapter 3 documents each center's STI software, hardware, and communications configurations. Chapter 7 documents STI EDD policy, practices, and procedures. The appendices, which arc contained in Part 2 of this document, consist of (1) STI EDD Project Plan, (2) Team members, (3) Phasing Schedules, (4) Accessing On-line Reports, and (5) Creating an HTML File and Setting Up an xTRS. In summary, Stage 4 of the NASAwide Electronic Publishing System is the final phase of its implementation through the prototyping and gradual integration of each NASA center's electronic printing systems, desktop publishing systems, and technical report servers to be able to provide to NASA's engineers, researchers, scientists, and external users the widest practicable and appropriate dissemination of information concerning its activities and the result thereof to their work stations."
Progressive Transmission and Compression of Images,26.483028,jpeg quality estimate,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Manufacturing Planning Guide,26.4324,jpeg quality estimate,['Administration and Management'],"Manufacturing process, milestones and inputs are unknowns to first-time users of the manufacturing facilities. The Manufacturing Planning Guide aids in establishing expectations for both NASA and non-NASA facility customers. The potential audience for this guide includes both internal and commercial spaceflight hardware/software developers. It is intended to assist their project engineering personnel in manufacturing planning and execution. Material covered includes a roadmap of the manufacturing process, roles and responsibilities of facility and user, major milestones, facility capabilities, and inputs required by the facility. Samples of deliverables, products, and inputs necessary to define test scope, cost, and schedule are included as an appendix to the guide."
"Recent Advances in Registration, Integration and Fusion of Remotely Sensed Data: Redundant Representations and Frames",26.012455,jpeg quality estimate,"['Earth Resources and Remote Sensing', 'Mathematical and Computer Sciences (General)']","In recent years, sophisticated mathematical techniques have been successfully applied to the field of remote sensing to produce significant advances in applications such as registration, integration and fusion of remotely sensed data. Registration, integration and fusion of multiple source imagery are the most important issues when dealing with Earth Science remote sensing data where information from multiple sensors, exhibiting various resolutions, must be integrated. Issues ranging from different sensor geometries, different spectral responses, differing illumination conditions, different seasons, and various amounts of noise need to be dealt with when designing an image registration, integration or fusion method. This tutorial will first define the problems and challenges associated with these applications and then will review some mathematical techniques that have been successfully utilized to solve them. In particular, we will cover topics on geometric multiscale representations, redundant representations and fusion frames, graph operators, diffusion wavelets, as well as spatial-spectral and operator-based data fusion. All the algorithms will be illustrated using remotely sensed data, with an emphasis on current and operational instruments."
A Review of the New AVIRIS Data Processing System,25.94793,jpeg quality estimate,['Documentation and Information Science'],"The processing of AVIRIS data - from the Metrum Very Large Data Store (VLDS) flight tape to delivered data products - has traditionally been performed in essentially the same way, from the beginning of the AVIRIS project up to and including the 1996 flight season. Starting with the 1997 flight season, a drastically different paradigm has been used for the processing of AVIRIS data. This change was made possible by the recent development of and related availability of affordable data storage devices."
Microhard MHX 2420 Orbital Performance Evaluation Using RT Logic T400CS,25.873638,jpeg quality estimate,['Communications and Radar'],"A major upfront cost of building low cost Nanosatellites is the communications sub-system. Most radios built for space missions cost over $4,000 per unit. This exceeds many budgets. One possible cost effective solution is the Microhard MHX2420, a commercial off-the-shelf transceiver with a unit cost under $1000. This paper aims to support the Nanosatellite community seeking an inexpensive radio by characterizing Microhard's performance envelope. Though not intended for space operations, the ability to test edge cases and increase average data transfer speeds through optimization positions this radio as a solution for Nanosatellite communications by expanding usage to include more missions. The second objective of this paper is to test and verify the optimal radio settings for the most common cases to improve downlinking. All tests were conducted with the aid of the RT Logic T400CS, a hardware-in-the-loop channel simulator designed to emulate real-world radio frequency (RF) link effects. This study provides recommended settings to optimize the downlink speed as well as the environmental parameters that cause the link to fail."
"Third International Symposium on Space Mission Operations and Ground Data Systems, part 1",25.63312,jpeg quality estimate,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Under the theme of 'Opportunities in Ground Data Systems for High Efficiency Operations of Space Missions,' the SpaceOps '94 symposium included presentations of more than 150 technical papers spanning five topic areas: Mission Management, Operations, Data Management, System Development, and Systems Engineering. The papers focus on improvements in the efficiency, effectiveness, productivity, and quality of data acquisition, ground systems, and mission operations. New technology, techniques, methods, and human systems are discussed. Accomplishments are also reported in the application of information systems to improve data retrieval, reporting, and archiving; the management of human factors; the use of telescience and teleoperations; and the design and implementation of logistics support for mission operations."
"More Than the Sum of the Parts: Satellite Aerosol Remote Sensing, and Its Relationship to Sub-Orbital Measurements and Models",25.08325,jpeg quality estimate,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']","Space-borne instruments are providing increasing amounts of data relating to global aerosol spectral optical depth, horizontal and vertical distribution, and very loose, but spatially and temporally extensive, constraints on particle micro-physical properties. The data sets, and many of the underlying techniques, are evolving rapidly. They represent a vast amount of information, potentially useful to the AAAR community. However, there are also issues, some quite subtle, that scientific users must take into consideration. This tutorial will provide one view of the answers to the following four questions: 1) What satellite-derived aerosol products are available? 2) What are their strengths and limitations? 3) How are they being used now? 4) How might they be used in conjunction with each other, with sub-orbital measurements, and with models to address cutting-edge aerosol questions?"
"NASA Tech Briefs, September 2012",24.856356,jpeg quality estimate,['Man/System Technology and Life Support'],"Topics covered include: Beat-to-Beat Blood Pressure Monitor; Measurement Techniques for Clock Jitter; Lightweight, Miniature Inertial Measurement System; Optical Density Analysis of X-Rays Utilizing Calibration Tooling to Estimate Thickness of Parts; Fuel Cell/Electrochemical Cell Voltage Monitor; Anomaly Detection Techniques with Real Test Data from a Spinning Turbine Engine-Like Rotor; Measuring Air Leaks into the Vacuum Space of Large Liquid Hydrogen Tanks; Antenna Calibration and Measurement Equipment; Glass Solder Approach for Robust, Low-Loss, Fiber-to-Waveguide Coupling; Lightweight Metal Matrix Composite Segmented for Manufacturing High-Precision Mirrors; Plasma Treatment to Remove Carbon from Indium UV Filters; Telerobotics Workstation (TRWS) for Deep Space Habitats; Single-Pole Double-Throw MMIC Switches for a Microwave Radiometer; On Shaft Data Acquisition System (OSDAS); ASIC Readout Circuit Architecture for Large Geiger Photodiode Arrays; Flexible Architecture for FPGAs in Embedded Systems; Polyurea-Based Aerogel Monoliths and Composites; Resin-Impregnated Carbon Ablator: A New Ablative Material for Hyperbolic Entry Speeds; Self-Cleaning Particulate Prefilter Media; Modular, Rapid Propellant Loading System/Cryogenic Testbed; Compact, Low-Force, Low-Noise Linear Actuator; Loop Heat Pipe with Thermal Control Valve as a Variable Thermal Link; Process for Measuring Over-Center Distances; Hands-Free Transcranial Color Doppler Probe; Improving Balance Function Using Low Levels of Electrical Stimulation of the Balance Organs; Developing Physiologic Models for Emergency Medical Procedures Under Microgravity; PMA-Linked Fluorescence for Rapid Detection of Viable Bacterial Endospores; Portable Intravenous Fluid Production Device for Ground Use; Adaptation of a Filter Assembly to Assess Microbial Bioburden of Pressurant Within a Propulsion System; Multiplexed Force and Deflection Sensing Shell Membranes for Robotic Manipulators; Whispering Gallery Mode Optomechanical Resonator; Vision-Aided Autonomous Landing and Ingress of Micro Aerial Vehicles; Self-Sealing Wet Chemistry Cell for Field Analysis; General MACOS Interface for Modeling and Analysis for Controlled Optical Systems; Mars Technology Rover with Arm-Mounted Percussive Coring Tool, Microimager, and Sample-Handling Encapsulation Containerization Subsystem; Fault-Tolerant, Real-Time, Multi-Core Computer System; Water Detection Based on Object Reflections; SATPLOT for Analysis of SECCHI Heliospheric Imager Data; Plug-in Plan Tool v3.0.3.1; Frequency Correction for MIRO Chirp Transformation Spectroscopy Spectrum; Nonlinear Estimation Approach to Real-Time Georegistration from Aerial Images; Optimal Force Control of Vibro-Impact Systems for Autonomous Drilling Applications; Low-Cost Telemetry System for Small/Micro Satellites; Operator Interface and Control Software for the Reconfigurable Surface System Tri-ATHLETE; and Algorithms for Determining Physical Responses of Structures Under Load."
The Telecommunications and Data Acquisition Report,24.748909,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"This quarterly publication provides archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA). In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, supporting research and technology, implementation, and operations. Also included are standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA."
Applied Information Systems Research Program (AISRP) Workshop 3 meeting proceedings,24.671694,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],The third Workshop of the Applied Laboratory Systems Research Program (AISRP) met at the Univeristy of Colorado's Laboratory for Atmospheric and Space Physics in August of 1993. The presentations were organized into four sessions: Artificial Intelligence Techniques; Scientific Visualization; Data Management and Archiving; and Research and Technology.
Overview of research in progress at the Center of Excellence,24.669777,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Center of Excellence (COE) was created nine years ago to facilitate active collaboration between the scientists at Ames Research Center and the Stanford Psychology Department. Significant interchange of ideas and personnel continues between Stanford and participating groups at NASA-Ames; the COE serves its function well. This progress report is organized into sections divided by project. Each section contains a list of investigators, a background statement, progress report, and a proposal for work during the coming year. The projects are: Algorithms for development and calibration of visual systems, Visually optimized image compression, Evaluation of advanced piloting displays, Spectral representations of color, Perception of motion in man and machine, Automation and decision making, and Motion information used for navigation and control."
NASA Earth Science Technology Office (ESTO) Advanced Information Systems Technology (AIST) New Observing Strategies (NOS) Annual Technical Reviews,24.569262,jpeg quality estimate,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']",
Evolution of Scientific and Technical Information Distribution,24.470694,jpeg quality estimate,['Documentation and Information Science'],"World Wide Web (WWW) and related information technologies are transforming the distribution of scientific and technical information (STI). We examine 11 recent, functioning digital libraries focusing on the distribution of STI publications, including journal articles, conference papers, and technical reports. We introduce 4 main categories of digital library projects: based on the architecture (distributed vs. centralized) and the contributor (traditional publisher vs. authoring individual/organization). Many digital library prototypes merely automate existing publishing practices or focus solely on the digitization of the publishing cycle output, not sampling and capturing elements of the input. Still others do not consider for distribution the large body of ""gray literature."" We address these deficiencies in the current model of STI exchange by suggesting methods for expanding the scope and target of digital libraries by focusing on a greater source of technical publications and using ""buckets,"" an object-oriented construct for grouping logically related information objects, to include holdings other than technical publications."
A Wideband Satcom Based Avionics Network with CDMA Uplink and TDM Downlink,24.088892,jpeg quality estimate,['Communications and Radar'],"The purpose of this paper is to describe some key technical ideas behind our vision of a future satcom based digital communication network for avionics applications The key features of our design are as follows: (a) Packetized transmission to permit efficient use of system resources for multimedia traffic; (b) A time division multiplexed (TDM) satellite downlink whose physical layer is designed to operate the satellite link at maximum power efficiency. We show how powerful turbo codes (invented originally for linear modulation) can be used with nonlinear constant envelope modulation, thus permitting the satellite amplifier to operate in a power efficient nonlinear regime; (c) A code division multiple access (CDMA) satellite uplink, which permits efficient access to the satellite from multiple asynchronous users. Closed loop power control is difficult for bursty packetized traffic, especially given the large round trip delay to the satellite. We show how adaptive interference suppression techniques can be used to deal with the ensuing near-far problem; (d) Joint source-channel coding techniques are required both at the physical and the data transport layer to optimize the end-to-end performance. We describe a novel approach to multiple description image encoding at the data transport layer in this paper."
"Technology 2003: The Fourth National Technology Transfer Conference and Exposition, volume 2",23.97781,jpeg quality estimate,['GENERAL'],"Proceedings from symposia of the Technology 2003 Conference and Exposition, Dec. 7-9, 1993, Anaheim, CA, are presented. Volume 2 features papers on artificial intelligence, CAD&E, computer hardware, computer software, information management, photonics, robotics, test and measurement, video and imaging, and virtual reality/simulation."
The Telecommunications and Data Acquisition Report,23.87165,jpeg quality estimate,['COMMUNICATIONS AND RADAR'],"Archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA) are provided. In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information. In the search for extraterrestrial intelligence (SETI), the TDA Progress Report reports on implementation and operations for searching the microwave spectrum. Topics covered include tracking and ground-based navigation; communications, spacecraft-ground; station control and system technology; capabilities for new projects; network upgrade and sustaining; network operations and operations support; and TDA program management and analysis."
"Buckets: Aggregative, Intelligent Agents for Publishing",23.799038,jpeg quality estimate,['Documentation and Information Science'],"Buckets are an aggregative, intelligent construct for publishing in digital libraries. The goal of research projects is to produce information. This information is often instantiated in several forms, differentiated by semantic types (report, software, video, datasets, etc.). A given semantic type can be further differentiated by syntactic representations as well (PostScript version, PDF version, Word version, etc.). Although the information was created together and subtle relationships can exist between them, different semantic instantiations are generally segregated along currently obsolete media boundaries. Reports are placed in report archives, software might go into a software archive, but most of the data and supporting materials are likely to be kept in informal personal archives or discarded altogether. Buckets provide an archive-independent container construct in which all related semantic and syntactic data types and objects can be logically grouped together, archived, and manipulated as a single object. Furthermore, buckets are active archival objects and can communicate with each other, people, or arbitrary network services."
"Third International Symposium on Space Mission Operations and Ground Data Systems, part 2",23.789738,jpeg quality estimate,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Under the theme of 'Opportunities in Ground Data Systems for High Efficiency Operations of Space Missions,' the SpaceOps '94 symposium included presentations of more than 150 technical papers spanning five topic areas: Mission Management, Operations, Data Management, System Development, and Systems Engineering. The symposium papers focus on improvements in the efficiency, effectiveness, and quality of data acquisition, ground systems, and mission operations. New technology, methods, and human systems are discussed. Accomplishments are also reported in the application of information systems to improve data retrieval, reporting, and archiving; the management of human factors; the use of telescience and teleoperations; and the design and implementation of logistics support for mission operations. This volume covers expert systems, systems development tools and approaches, and systems engineering issues."
Telecommunications issues of intelligent database management for ground processing systems in the EOS era,23.605064,jpeg quality estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Future NASA earth science missions, including the Earth Observing System (EOS), will be generating vast amounts of data that must be processed and stored at various locations around the world. Here we present a stepwise-refinement of the intelligent database management (IDM) of the distributed active archive center (DAAC - one of seven regionally-located EOSDIS archive sites) architecture, to showcase the telecommunications issues involved. We develop this architecture into a general overall design. We show that the current evolution of protocols is sufficient to support IDM at Gbps rates over large distances. We also show that network design can accommodate a flexible data ingestion storage pipeline and a user extraction and visualization engine, without interference between the two."
STRIPE: Remote Driving Using Limited Image Data,23.57951,jpeg quality estimate,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
Data Understanding Applied to Optimization,23.411549,jpeg quality estimate,['Computer Programming and Software'],"The goal of this research is to explore and develop software for supporting visualization and data analysis of search and optimization. Optimization is an ever-present problem in science. The theory of NP-completeness implies that the problems can only be resolved by increasingly smarter problem specific knowledge, possibly for use in some general purpose algorithms. Visualization and data analysis offers an opportunity to accelerate our understanding of key computational bottlenecks in optimization and to automatically tune aspects of the computation for specific problems. We will prototype systems to demonstrate how data understanding can be successfully applied to problems characteristic of NASA's key science optimization tasks, such as central tasks for parallel processing, spacecraft scheduling, and data transmission from a remote satellite."
"NASA Tech Briefs, August 2006",23.01887,jpeg quality estimate,['Man/System Technology and Life Support'],"Topics covered include: Measurement and Controls Data Acquisition System IMU/GPS System Provides Position and Attitude Data Using Artificial Intelligence to Inform Pilots of Weather Fast Lossless Compression of Multispectral-Image Data Developing Signal-Pattern-Recognition Programs Implementing Access to Data Distributed on Many Processors Compact, Efficient Drive Circuit for a Piezoelectric Pump; Dual Common Planes for Time Multiplexing of Dual-Color QWIPs; MMIC Power Amplifier Puts Out 40 mW From 75 to 110 GHz; 2D/3D Visual Tracker for Rover Mast; Adding Hierarchical Objects to Relational Database General-Purpose XML-Based Information Managements; Vaporizable Scaffolds for Fabricating Thermoelectric Modules; Producing Quantum Dots by Spray Pyrolysis; Mobile Robot for Exploring Cold Liquid/Solid Environments; System Would Acquire Core and Powder Samples of Rocks; Improved Fabrication of Lithium Films Having Micron Features; Manufacture of Regularly Shaped Sol-Gel Pellets; Regulating Glucose and pH, and Monitoring Oxygen in a Bioreactor; Satellite Multiangle Spectropolarimetric Imaging of Aerosols; Interferometric System for Measuring Thickness of Sea Ice; Microscale Regenerative Heat Exchanger Protocols for Handling Messages Between Simulation Computers Statistical Detection of Atypical Aircraft Flights NASA's Aviation Safety and Modeling Project Multimode-Guided-Wave Ultrasonic Scanning of Materials Algorithms for Maneuvering Spacecraft Around Small Bodies Improved Solar-Radiation-Pressure Models for GPS Satellites Measuring Attitude of a Large, Flexible, Orbiting Structure"
Novel High Barrier Films for Packaging Food and Medicine,22.951557,jpeg quality estimate,['Composite Materials'],"Long manned missions require food packaging to maintain food safety, nutrition, and acceptability for the length of 3-5 years1,2 while the shelf life assigned by NASA for current provisions is 18-24 months1. The focus of this design project was to make a polymer film specifically to function as a high oxygen barrier to later be included in an improved multi-layer packaging system of other specialized polymers with capabilities to allow for a 5-year shelf-life. To qualify as a high oxygen barrier film and to be a successful design for the future packaging, the resulting film is required to have an oxygen transmission rate (OTR) less than 0.06 cc/ m2/24 hr/atm, the standard for the current packaging3. This new film was designed to have a decreased permeability by increasing the tortuous path a gas molecule travels to permeate through a film. The increase in the tortuous path of a gas molecule is accomplished by introducing a  2D material additive. The additive chosen for this project was hexagonal boron nitride (h-BN) which exfoliates into boron nitride nanosheets (BNNs). Nylon 6 was chosen as the candidate for the matrix. This project is required to determine the best methods to synthesize the sample film and then to test the OTR of the film to determine if this design was successful. This project examined methods for pulverizing polymer resin pellets, h-BN exfoliation, and film fabrication using a hot press. This report includes the exfoliation and analysis of h-BN, procedural preparations for films, and a modeling study of estimated OTR of the h-BN/Nylon. The work in this report did not yield a high barrier composite film because of the laboratory closure in response to COVID -19 guidance but provides a concise method of the additive preparation and film synthesis. This gives a good starting point for future research in high barrier films by 2D additive composites.
"
Declassified Intelligence Satellite Photography (DISP) Coverage of Antarctica,22.469988,jpeg quality estimate,['Earth Resources and Remote Sensing'],"This report summarizes the results of a nine-week summer project examining all Declassified Intelligence Satellite Photography (DISP) of Antarctica. It was discovered that the data were collected in three separate missions during 1962 and 1963. The first two missions covered only the coastal areas, while the third mission covered the entire continent. Many of the 1782 frames collected were cloudy. This is especially true of West Antarctica. An optimal set of photographs covering the entire Antarctic coastline is identified along with some examples that show changes in the coastline which have occurred since the early 1960s."
The Solar-B Mission,22.447868,jpeg quality estimate,['Solar Physics'],"Solar-B, the next ISAS mission (with major NASA participation), is designed to address the fundamental question of how magnetic fields interact with plasma to produce solar variability. The mission has a number of unique capabilities that will enable it to answer the outstanding questions of solar magnetism. First, by escaping atmospheric seeing, it will deliver continuous observations of the solar surface with unprecedented spatial resolution. Second, Solar-B will deliver the first accurate measurements of all three components of the photospheric magnetic field. Solar-B will measure both the magnetic energy driving the photosphere and simultaneously its effects in the corona. Solar-B offers unique programmatic opportunities to NASA. It will continue an effective collaboration with our most reliable international partner. It will deliver images and data that will have strong public outreach potential. Finally, the science of Solar-B is clearly related to the themes of origins and plasma astrophysics, and contributes directly to the national space weather and global change programs."
Optical Diagnostic System for Solar Sails: Phase 1 Final Report,22.389656,jpeg quality estimate,['Structural Mechanics'],"NASA's In-Space Propulsion program recently selected AEC-ABLE Engineering and L'Garde, Inc. to develop scale-model solar sail hardware and demonstrate its functionality on the ground. Both are square sail designs with lightweight diagonal booms (<100 g/m) and ultra-thin membranes (<10 g/sq m). To support this technology, the authors are developing an integrated diagnostics instrumentation package for monitoring solar sail structures such as these in a near-term flight experiment. We refer to this activity as the ""Optical Diagnostic System (ODS) for Solar Sails"" project. The approach uses lightweight optics and photogrammetric techniques to measure solar sail membrane and boom shape and dynamics, thermography to map temperature, and non-optical sensors including MEMS accelerometers and load cells. The diagnostics package must measure key structural characteristics including deployment dynamics, sail support tension, boom and sail deflection, boom and sail natural frequencies, sail temperature, and sail integrity. This report summarizes work in the initial 6-month Phase I period (conceptual design phase) and complements the final presentation given in Huntsville, AL on January 14, 2004."
Locally adaptive vector quantization: Data compression with feature preservation,22.330233,jpeg quality estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
Issues in the Imprecise Computation Approach to Fault Tolerance,22.300869,jpeg quality estimate,['Computer Programming and Software'],"The imprecise computation technique can be used in a natural way to enhance fault tolerance. By providing a usable, approximate result whenever a failure or overload prevent the system from producing the desired, precise result, we can increase the availability of data and services, reduce the need for error-recovery operations, and minimize the costs in replication. This paper describes the domain-specific fault tolerance mechanisms that are needed to support the provision and correct usage of imprecise results for several representative application domains. The elements of an application-domain-independent architecture that can effectively integrate these domain-specific mechanisms are also described."
Aircraft Conceptual Design Using Vehicle Sketch Pad,22.239597,jpeg quality estimate,['Aerodynamics'],"Vehicle Sketch Pad (VSP) is a parametric geometry modeling tool that is intended for use in the conceptual design of aircraft. The intent of this software is to rapidly model aircraft configurations without expending the expertise and time that is typically required for modeling with traditional Computer Aided Design (CAD) packages. VSP accomplishes this by using parametrically defined components, such as a wing that is defined by span, area, sweep, taper ratio, thickness to cord, and so on. During this phase of frequent design builds, changes to the model can be rapidly visualized along with the internal volumetric layout. Using this geometry-based approach, parameters such as wetted areas and cord lengths can be easily extracted for rapid external performance analyses, such as a parasite drag buildup. At the completion of the conceptual design phase, VSP can export its geometry to higher fidelity tools. This geometry tool was developed by NASA and is freely available to U.S. companies and universities. It has become integral to conceptual design in the Aeronautics Systems Analysis Branch (ASAB) here at NASA Langley Research Center and is currently being used at over 100 universities, aerospace companies, and other government agencies. This paper focuses on the use of VSP in recent NASA conceptual design studies to facilitate geometry-centered design methodology. Such a process is shown to promote greater levels of creativity, more rapid assessment of critical design issues, and improved ability to quickly interact with higher order analyses. A number of VSP vehicle model examples are compared to CAD-based conceptual design, from a designer perspective; comparisons are also made of the time and expertise required to build the geometry representations as well."
The Geophysical Fluid Flow Cell Experiment,22.204098,jpeg quality estimate,['Fluid Mechanics and Heat Transfer'],"The Geophysical Fluid Flow Cell (GFFC) experiment performed visualizations of thermal convection in a rotating differentially heated spherical shell of fluid. In these experiments dielectric polarization forces are used to generate a radially directed buoyancy force. This enables the laboratory simulation of a number of geophysically and astrophysically important situations in which sphericity and rotation both impose strong constraints on global scale fluid motions. During USML-2 a large set of experiments with spherically symmetric heating were carried out. These enabled the determination of critical points for the transition to various forms of nonaxisymmetric convection and, for highly turbulent flows, the transition latitudes separating the different modes of motion. This paper presents a first analysis of these experiments as well as data on the general performance of the instrument during the USML-2 flight."
Buckets: Smart Objects for Digital Libraries,22.19194,jpeg quality estimate,['Documentation and Information Science'],"Current discussion of digital libraries (DLs) is often dominated by the merits of the respective storage, search and retrieval functionality of archives, repositories, search engines, search interfaces and database systems. While these technologies are necessary for information management, the information content is more important than the systems used for its storage and retrieval. Digital information should have the same long-term survivability prospects as traditional hardcopy information and should be protected to the extent possible from evolving search engine technologies and vendor vagaries in database management systems. Information content and information retrieval systems should progress on independent paths and make limited assumptions about the status or capabilities of the other. Digital information can achieve independence from archives and DL systems through the use of buckets. Buckets are an aggregative, intelligent construct for publishing in DLs. Buckets allow the decoupling of information content from information storage and retrieval. Buckets exist within the Smart Objects and Dumb Archives model for DLs in that many of the functionalities and responsibilities traditionally associated with archives are pushed down (making the archives dumber) into the buckets (making them smarter). Some of the responsibilities imbued to buckets are the enforcement of their terms and conditions, and maintenance and display of their contents."
Photogrammetry Methodology Development for Gossamer Spacecraft Structures,21.983662,jpeg quality estimate,['Structural Mechanics'],"Photogrammetry--the science of calculating 3D object coordinates from images--is a flexible and robust approach for measuring the static and dynamic characteristics of future ultra-lightweight and inflatable space structures (a.k.a., Gossamer structures), such as large membrane reflectors, solar sails, and thin-film solar arrays. Shape and dynamic measurements are required to validate new structural modeling techniques and corresponding analytical models for these unconventional systems. This paper summarizes experiences at NASA Langley Research Center over the past three years to develop or adapt photogrammetry methods for the specific problem of measuring Gossamer space structures. Turnkey industrial photogrammetry systems were not considered a cost-effective choice for this basic research effort because of their high purchase and maintenance costs. Instead, this research uses mainly off-the-shelf digital-camera and software technologies that are affordable to most organizations and provide acceptable accuracy."
The Sensor Test for Orion RelNav Risk Mitigation (STORRM) Development Test Objective,21.716352,jpeg quality estimate,"['Spacecraft Design, Testing and Performance']","The Sensor Test for Orion Relative-Navigation Risk Mitigation (STORRM) Development Test Objective (DTO) flew aboard the Space Shuttle Endeavour on STS-134 in May- June 2011, and was designed to characterize the performance of the flash LIDAR and docking camera being developed for the Orion Multi-Purpose Crew Vehicle. The flash LIDAR, called the Vision Navigation Sensor (VNS), will be the primary navigation instrument used by the Orion vehicle during rendezvous, proximity operations, and docking. The DC will be used by the Orion crew for piloting cues during docking. This paper provides an overview of the STORRM test objectives and the concept of operations. It continues with a description of STORRM's major hardware components, which include the VNS, docking camera, and supporting avionics. Next, an overview of crew and analyst training activities will describe how the STORRM team prepared for flight. Then an overview of in-flight data collection and analysis is presented. Key findings and results from this project are summarized. Finally, the paper concludes with lessons learned from the STORRM DTO."
The Little Photometer That Could: Technical Challenges and Science Results from the Kepler Mission,21.660503,jpeg quality estimate,['Astronomy'],"The Kepler spacecraft launched on March 7, 2009, initiating NASA's first search for Earth-size planets orbiting Sun-like stars. Since launch, Kepler has announced the discovery of 17 exoplanets, including a system of six transiting a Sun-like star, Kepler-11, and the first confirmed rocky planet, Kepler-10b, with a radius of 1.4 that of Earth. Kepler is proving to be a cornucopia of discoveries: it has identified over 1200 candidate planets based on the first 120 days of observations, including 54 that are in or near the habitable zone of their stars, and 68 that are 1.2 Earth radii or smaller. An astounding 408 of these planetary candidates are found in 170 multiple systems, demonstrating the compactness and flatness of planetary systems composed of small planets. Never before has there been a photometer capable of reaching a precision near 20 ppm in 6.5 hours and capable of conducting nearly continuous and uninterrupted observations for months to years. In addition to exoplanets, Kepler is providing a wealth of astrophysics, and is revolutionizing the field of asteroseismology. Designing and building the Kepler photometer and the software systems that process and analyze the resulting data to make the discoveries presented a daunting set of challenges, including how to manage the large data volume. The challenges continue into flight operations, as the photometer is sensitive to its thermal environment, complicating the task of detecting 84 ppm drops in brightness corresponding to Earth-size planets transiting Sun-like stars."
A Decade of Neural Networks: Practical Applications and Prospects,21.53086,jpeg quality estimate,['CYBERNETICS'],"The Jet Propulsion Laboratory Neural Network Workshop, sponsored by NASA and DOD, brings together sponsoring agencies, active researchers, and the user community to formulate a vision for the next decade of neural network research and application prospects. While the speed and computing power of microprocessors continue to grow at an ever-increasing pace, the demand to intelligently and adaptively deal with the complex, fuzzy, and often ill-defined world around us remains to a large extent unaddressed. Powerful, highly parallel computing paradigms such as neural networks promise to have a major impact in addressing these needs. Papers in the workshop proceedings highlight benefits of neural networks in real-world applications compared to conventional computing techniques. Topics include fault diagnosis, pattern recognition, and multiparameter optimization."
"NASA University Research Centers Technical Advances in Education, Aeronautics, Space, Autonomy, Earth and Environment",21.412037,jpeg quality estimate,['Social and Information Sciences (General)'],"This first volume of the Autonomous Control Engineering (ACE) Center Press Series on NASA University Research Center's (URC's) Advanced Technologies on Space Exploration and National Service constitute a report on the research papers and presentations delivered by NASA Installations and industry and Report of the NASA's fourteen URC's held at the First National Conference in Albuquerque, New Mexico from February 16-19, 1997."
Technology Directions for the 21st Century,21.298685,jpeg quality estimate,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
NASA/ASEE Faculty Fellowship Program: 2003 Research Reports,21.122793,jpeg quality estimate,['Engineering (General)'],"This document is a collection of technical reports on research conducted by the participants in the 2003 NASA/ASEE Faculty Fellowship Program at the John F. Kennedy Space Center (KSC). This was the nineteenth year that a NASA/ASEE program has been conducted at KSC. The 2003 program was administered by the University of Central Florida (UCF) in cooperation with KSC. The program was operated under the auspices of the American Society for Engineering Education (ASEE) and the Education Division, NASA Headquarters, Washington, D.C. The KSC program was one of nine such Aeronautics and Space Research Programs funded by NASA Headquarters in 2003. The basic common objectives of the NASA/ASEE Faculty Fellowship Program are: A) To further the professional knowledge of qualified engineering and science faculty members; B) To stimulate an exchange of ideas between teaching participants and employees of NASA; C) To enrich and refresh the research and teaching activities of participants institutions; D) To contribute to the research objectives of the NASA center. The KSC Faculty Fellows spent ten weeks (May 19 through July 25, 2003) working with NASA scientists and engineers on research of mutual interest to the university faculty member and the NASA colleague. The editors of this document were responsible for selecting appropriately qualified faculty to address some of the many research areas of current interest to NASA/KSC. A separate document reports on the administrative aspects of the 2003 program. The NASA/ASEE program is intended to be a two-year program to allow in-depth research by the university faculty member. In many cases a faculty member has developed a close working relationship with a particular NASA group that had provided funding beyond the two-year limit."
Geometric assessment of image quality using digital image registration techniques,121.39738,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Image registration techniques were developed to perform a geometric quality assessment of multispectral and multitemporal image pairs. Based upon LANDSAT tapes, accuracies to a small fraction of a pixel were demonstrated. Because it is insensitive to the choice of registration areas, the technique is well suited to performance in an automatic system. It may be implemented at megapixel-per-second rates using a commercial minicomputer in combination with a special purpose digital preprocessor."
A conceptual study of automatic and semi-automatic quality assurance techniques for round image processing,121.084755,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"This report summarizes the results of a study conducted by Engineering and Economics Research (EER), Inc. under NASA Contract Number NAS5-27513. The study involved the development of preliminary concepts for automatic and semiautomatic quality assurance (QA) techniques for ground image processing. A distinction is made between quality assessment and the more comprehensive quality assurance which includes decision making and system feedback control in response to quality assessment."
Image processing system performance prediction and product quality evaluation,119.86839,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. A new technique for image processing system performance prediction and product quality evaluation was developed. It was entirely objective, quantitative, and general, and should prove useful in system design and quality control. The technique and its application to determination of quality control procedures for the Earth Resources Technology Satellite NASA Data Processing Facility are described."
LANDSAT-4 image data quality analysis,112.96908,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Work done on evaluating the geometric and radiometric quality of early LANDSAT-4 sensor data is described. Band to band and channel to channel registration evaluations were carried out using a line correlator. Visual blink comparisons were run on an image display to observe band to band registration over 512 x 512 pixel blocks. The results indicate a .5 pixel line misregistration between the 1.55 to 1.75, 2.08 to 2.35 micrometer bands and the first four bands. Also a four 30M line and column misregistration of the thermal IR band was observed. Radiometric evaluation included mean and variance analysis of individual detectors and principal components analysis. Results indicate that detector bias for all bands is very close or within tolerance. Bright spots were observed in the thermal IR band on an 18 line by 128 pixel grid. No explanation for this was pursued. The general overall quality of the TM was judged to be very high."
Image Registration Workshop Proceedings,111.94222,image quality calculation,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Improving biomedical image quality with computers,105.80355,image quality calculation,['BIOSCIENCES'],Computerized image enhancement techniques used on biomedical radiographs and photomicrographs
Evaluation of image quality in a Cassegrain-type telescope with an oscillating secondary mirror,105.0143,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"A ray-trace analysis is described of aberrations and extreme rays of a Cassegrain-type telescope with a tilted secondary mirror. The work was motivated by the need to understand the factors limiting image quality and to assist in the design of secondary mirrors for three telescopes with oscillating secondary mirrors (OSM) used at Ames Research Center for high altitude infrared astronomy. The telescopes are a 31-cm-diameter Dall-Kirkham (elliptical primary, spherical secondary) flown aboard a Lear jet, a 71-cm balloon-borne Dall-Kirkham flown on the AIROscope gondola, and a 91-cm true Cassegrain (parabolic primary, hyperbolic secondary) flown aboard a C-141 jet transport. The optics for these telescopes were not designed specifically for OSM operation, but all have OSM's and all must be used with various detector configurations; therefore, a facility that evaluates the performance of a telescope for a given configuration is useful. The analytical expressions are summarized and results for the above systems are discussed. Details of the calculation and a discussion of the computer program are given in the appendices."
Applications notice for participation in the LANDSAT-D image data quality analysis program,102.92627,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"The applications notice for the LANDSAT 4 image data quality analysis program is presented. The objectives of the program are to qualify LANDSAT 4 sensor and systems performance from a user applications point of view, and to identify any malfunctions that may impact data applications. Guidelines for preparing proposals and background information are provided."
Image-adapted visually weighted quantization matrices for digital image compression,97.47199,image quality calculation,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
WELDSMART: A vision-based expert system for quality control,96.22215,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"This work was aimed at exploring means for utilizing computer technology in quality inspection and evaluation. Inspection of metallic welds was selected as the main application for this development and primary emphasis was placed on visual inspection, as opposed to other inspection methods, such as radiographic techniques. Emphasis was placed on methodologies with the potential for use in real-time quality control systems. Because quality evaluation is somewhat subjective, despite various efforts to classify discontinuities and standardize inspection methods, the task of using a computer for both inspection and evaluation was not trivial. The work started out with a review of the various inspection techniques that are used for quality control in welding. Among other observations from this review was the finding that most weld defects result in abnormalities that may be seen by visual inspection. This supports the approach of emphasizing visual inspection for this work. Quality control consists of two phases: (1) identification of weld discontinuities (some of which may be severe enough to be classified as defects), and (2) assessment or evaluation of the weld based on the observed discontinuities. Usually the latter phase results in a pass/fail judgement for the inspected piece. It is the conclusion of this work that the first of the above tasks, identification of discontinuities, is the most challenging one. It calls for sophisticated image processing and image analysis techniques, and frequently ad hoc methods have to be developed to identify specific features in the weld image. The difficulty of this task is generally not due to limited computing power. In most cases it was found that a modest personal computer or workstation could carry out most computations in a reasonably short time period. Rather, the algorithms and methods necessary for identifying weld discontinuities were in some cases limited. The fact that specific techniques were finally developed and successfully demosntrated to work illustrates that the general approach taken here appears to be promising for commercial development of computerized quality inspection systems. Inspection based on these techniques may be used to supplement or substitute more elaborate inspection methods, such as x-ray inspections."
Image Quality of the Helioseismic and Magnetic Imager (HMI) Onboard the Solar Dynamics Observatory (SDO),96.01386,image quality calculation,['Solar Physics'],"We describe the imaging quality of the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO) as measured during the ground calibration of the instrument. We describe the calibration techniques and report our results for the final configuration of HMI. We present the distortion, modulation transfer function, stray light,image shifts introduced by moving parts of the instrument, best focus, field curvature, and the relative alignment of the two cameras. We investigate the gain and linearity of the cameras, and present the measured flat field."
Software to model AXAF-I image quality,95.54814,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"A modular user-friendly computer program for the modeling of grazing-incidence type x-ray optical systems has been developed. This comprehensive computer software GRAZTRACE covers the manipulation of input data, ray tracing with reflectivity and surface deformation effects, convolution with x-ray source shape, and x-ray scattering. The program also includes the capabilities for image analysis, detector scan modeling, and graphical presentation of the results. A number of utilities have been developed to interface the predicted Advanced X-ray Astrophysics Facility-Imaging (AXAF-I) mirror structural and thermal distortions with the ray-trace. This software is written in FORTRAN 77 and runs on a SUN/SPARC station. An interactive command mode version and a batch mode version of the software have been developed."
COUPLING FOR MULTISTAGE IMAGE INTENSIFIERS,93.306816,image quality calculation,['NAVIGATION'],Fiber optics coupling for multistage image intensifiers
Overview on METEOSAT geometrical image data processing,91.663376,image quality calculation,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Digital Images acquired from the geostationary METEOSAT satellites are processed and disseminated at ESA's European Space Operations Centre in Darmstadt, Germany. Their scientific value is mainly dependent on their radiometric quality and geometric stability. This paper will give an overview on the image processing activities performed at ESOC, concentrating on the geometrical restoration and quality evaluation. The performance of the rectification process for the various satellites over the past years will be presented and the impacts of external events as for instance the Pinatubo eruption in 1991 will be explained. Special developments both in hard and software, necessary to cope with demanding tasks as new image resampling or to correct for spacecraft anomalies, are presented as well. The rotating lens of MET-5 causing severe geometrical image distortions is an example for the latter."
Image data compression having minimum perceptual error,90.15659,image quality calculation,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Texture functions in image analysis:  A computationally efficient solution,88.21764,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"A computationally efficient means for calculating texture measurements from digital images by use of the co-occurrence technique is presented. The calculation of the statistical descriptors of image texture and a solution that circumvents the need for calculating and storing a co-occurrence matrix are discussed. The results show that existing efficient algorithms for calculating sums, sums of squares, and cross products can be used to compute complex co-occurrence relationships directly from the digital image input."
Quality and Control of Water Vapor Winds,87.125916,image quality calculation,['Meteorology and Climatology'],"Water vapor imagery from the geostationary satellites such as GOES, Meteosat, and GMS provides synoptic views of dynamical events on a continual basis. Because the imagery represents a non-linear combination of mid- and upper-tropospheric thermodynamic parameters (three-dimensional variations in temperature and humidity), video loops of these image products provide enlightening views of regional flow fields, the movement of tropical and extratropical storm systems, the transfer of moisture between hemispheres and from the tropics to the mid- latitudes, and the dominance of high pressure systems over particular regions of the Earth. Despite the obvious larger scale features, the water vapor imagery contains significant image variability down to the single 8 km GOES pixel. These features can be quantitatively identified and tracked from one time to the next using various image processing techniques. Merrill et al. (1991), Hayden and Schmidt (1992), and Laurent (1993) have documented the operational procedures and capabilities of NOAA and ESOC to produce cloud and water vapor winds. These techniques employ standard correlation and template matching approaches to wind tracking and use qualitative and quantitative procedures to eliminate bad wind vectors from the wind data set. Techniques have also been developed to improve the quality of the operational winds though robust editing procedures (Hayden and Veldon 1991). These quality and control approaches have limitations, are often subjective, and constrain wind variability to be consistent with model derived wind fields. This paper describes research focused on the refinement of objective quality and control parameters for water vapor wind vector data sets. New quality and control measures are developed and employed to provide a more robust wind data set for climate analysis, data assimilation studies, as well as operational weather forecasting. The parameters are applicable to cloud-tracked winds as well with minor modifications. The improvement in winds through use of these new quality and control parameters is measured without the use of rawinsonde or modeled wind field data and compared with other approaches."
PCB Quality Metrics that Drive Reliability (PD 18),86.4788,image quality calculation,['Quality Assurance and Reliability'],"Risk based technology infusion is a deliberate and systematic process which defines the analysis and communication methodology by which new technology is applied and integrated into existing and new designs, identifies technology development needs based on trends analysis and facilitates the identification of shortfalls against performance objectives. This presentation at IPC Works Asia Aerospace 2019 Events provides the audience a snapshot of quality variations in printed wiring board quality, as assessed, using experiences in processing and risk analysis of PWB structural integrity coupons. The presentation will focus on printed wiring board quality metrics used, the relative type and number of non-conformances observed and trend analysis using statistical methods. Trend analysis shows the top five non-conformances observed across PWB suppliers, the root cause(s) behind these non-conformance and suggestions of mitigation plans. The trends will then be matched with the current state of the PWB supplier base and its challenges and opportunities. The presentation further discusses the risk based SMA approaches and methods being applied at GSFC for evaluating candidate printed wiring board technologies which promote the adoption of higher throughput and faster processing technology for GSFC missions.

"
Image Sensor Technology,86.36521,image quality calculation,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites - image sensor systems
Image Calibration,84.433876,image quality calculation,['Man/System Technology and Life Support'],"Calibrate_Image calibrates images obtained from focal plane arrays so that the output image more accurately represents the observed scene. The function takes as input a degraded image along with a flat field image and a dark frame image produced by the focal plane array and outputs a corrected image. The three most prominent sources of image degradation are corrected for: dark current accumulation, gain non-uniformity across the focal plane array, and hot and/or dead pixels in the array. In the corrected output image the dark current is subtracted, the gain variation is equalized, and values for hot and dead pixels are estimated, using bicubic interpolation techniques."
The UNO Aviation Monograph Series: The Airline Quality Rating 1998,84.20043,image quality calculation,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple factors important to consumers. Development history and calculation details for the AQR rating system are detailed in The Airline Quality Rating 1991 issued in April, 1991, by the National Institute for Aviation Research at Wichita State University. This current report, Airline Quality Rating 1998, contains monthly Airline Quality Rating scores for 1997. Additional copies are available by contacting Wichita State University or University of Nebraska at Omaha. The Airline Quality Rating 1998 is a summary of month-by-month quality ratings for the ten major U.S. airlines operating during 1997. Using the Airline Quality Rating system and monthly performance data for each airline for the calendar year of 1997, individual and comparative ratings are reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major airlines domestic operations for the 12 month period of 1997, and industry average results. Also, comparative Airline Quality Rating data for 1991 through 1996 are included to provide a longer term view of quality in the industry."
"Image Segmentation, Registration, Compression, and Matching",84.069466,image quality calculation,['Man/System Technology and Life Support'],"A novel computational framework was developed of a 2D affine invariant matching exploiting a parameter space. Named as affine invariant parameter space (AIPS), the technique can be applied to many image-processing and computer-vision problems, including image registration, template matching, and object tracking from image sequence. The AIPS is formed by the parameters in an affine combination of a set of feature points in the image plane. In cases where the entire image can be assumed to have undergone a single affine transformation, the new AIPS match metric and matching framework becomes very effective (compared with the state-of-the-art methods at the time of this reporting). No knowledge about scaling or any other transformation parameters need to be known a priori to apply the AIPS framework. An automated suite of software tools has been created to provide accurate image segmentation (for data cleaning) and high-quality 2D image and 3D surface registration (for fusing multi-resolution terrain, image, and map data). These tools are capable of supporting existing GIS toolkits already in the marketplace, and will also be usable in a stand-alone fashion. The toolkit applies novel algorithmic approaches for image segmentation, feature extraction, and registration of 2D imagery and 3D surface data, which supports first-pass, batched, fully automatic feature extraction (for segmentation), and registration. A hierarchical and adaptive approach is taken for achieving automatic feature extraction, segmentation, and registration. Surface registration is the process of aligning two (or more) data sets to a common coordinate system, during which the transformation between their different coordinate systems is determined. Also developed here are a novel, volumetric surface modeling and compression technique that provide both quality-guaranteed mesh surface approximations and compaction of the model sizes by efficiently coding the geometry and connectivity/topology components of the generated models. The highly efficient triangular mesh compression compacts the connectivity information at the rate of 1.5-4 bits per vertex (on average for triangle meshes), while reducing the 3D geometry by 40-50 percent. Finally, taking into consideration the characteristics of 3D terrain data, and using the innovative, regularized binary decomposition mesh modeling, a multistage, pattern-drive modeling, and compression technique has been developed to provide an effective framework for compressing digital elevation model (DEM) surfaces, high-resolution aerial imagery, and other types of NASA data."
Physical correction filter for improving the optical quality of an image,83.97487,image quality calculation,['OPTICS'],"A family of physical correction filters is described. Each filter is designed to correct image content of a photographed scene of limited resolution and includes a first filter element with a pinhole through which light passes to a differential amplifier. A second filter element through which light passes through one or more openings, whose geometric configuration is a function of the cause of the resolution loss included. The light, passing through the second filter element, is also supplied to the differential amplifier whose output is used to activate an optical display or recorder to reproduce a photograph or display of the scene in the original photograph or display of the scene in the original photograph with resolution which is significantly greater than that characterizing the original photograph."
Diffraction image formation in optical systems with polarization aberrations. II - Amplitude response matrices for rotationally symmetric systems,83.2984,image quality calculation,['OPTICS'],"In the previous paper in this series (McGuire and Chipman, 1990), a formulation was established for the calculation and analysis of diffraction image quality in polarizing optical systems illuminated with partially polarized, partially coherent light. In the present paper, the effect of second- and fourth-order polarization aberrations on the image plane diffraction patterns are examined. The amplitude response matrix is calculated for optical systems with small numerical apertures. Numerical results are presented for optical systems with circular apertures for three of the aberration types."
The UNO Aviation Monograph Series: The Airline Quality Rating 1997,83.18318,image quality calculation,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple factors important to consumers. Development history and calculation details for the AQR rating system are detailed in The Airline Quality Rating 1991 issued in April, 1991, by the National Institute for Aviation Research at Wichita State University. This current report, Airline Rating 1997, contains monthly Airline Quality Rating scores for 1996. Additional copies are available by contacting Wichita State University or the University of Nebraska at Omaha. The Airline Quality Rating (AQR) 1997 is a summary of a month-by-month quality ratings for the nine major domestic U.S. airlines operating during 1996. Using the Airline Quality Rating system and monthly performance data for each airline for the calendar year of 1996, individual and comparative ratings are reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major domestic airlines across the 12 month period of 1996, and industry average results. Also comparative Airline Quality Rating data for 1991 through 1995 are included to provide a longer term view of quality in the industry."
High compression image and image sequence coding,83.163414,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"The digital representation of an image requires a very large number of bits. This number is even larger for an image sequence. The goal of image coding is to reduce this number, as much as possible, and reconstruct a faithful duplicate of the original picture or image sequence. Early efforts in image coding, solely guided by information theory, led to a plethora of methods. The compression ratio reached a plateau around 10:1 a couple of years ago. Recent progress in the study of the brain mechanism of vision and scene analysis has opened new vistas in picture coding. Directional sensitivity of the neurones in the visual pathway combined with the separate processing of contours and textures has led to a new class of coding methods capable of achieving compression ratios as high as 100:1 for images and around 300:1 for image sequences. Recent progress on some of the main avenues of object-based methods is presented. These second generation techniques make use of contour-texture modeling, new results in neurophysiology and psychophysics and scene analysis."
"Proceedings of the Image Intensifier Symposium, October 24-26, 1961, Fort Belvoir, VA",82.60352,image quality calculation,['NAVIGATION'],Proceedings of second image intensifier symposium
Simulation of Image Performance Characteristics of the Landsat Data Continuity Mission (LDCM) Thermal Infrared Sensor (TIRS),82.37353,image quality calculation,"['Instrumentation and Photography', 'Earth Resources and Remote Sensing']","The next Landsat satellite, which is scheduled for launch in early 2013, will carry two instruments: the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). Significant design changes over previous Landsat instruments have been made to these sensors to potentially enhance the quality of Landsat image data. TIRS, which is the focus of this study, is a dual-band instrument that uses a push-broom style architecture to collect data. To help understand the impact of design trades during instrument build, an effort was initiated to model TIRS imagery. The Digital Imaging and Remote Sensing Image Generation (DIRSIG) tool was used to produce synthetic ""on-orbit"" TIRS data with detailed radiometric, geometric, and digital image characteristics. This work presents several studies that used DIRSIG simulated TIRS data to test the impact of engineering performance data on image quality in an effort to determine if the image data meet specifications or, in the event that they do not, to determine if the resulting image data are still acceptable. "
On-Orbit MTF Measurement and Product Quality Monitoring for Commercial Remote Sensing Systems,82.34566,image quality calculation,['Earth Resources and Remote Sensing'],"Initialization and opportunistic targets are chosen that represent the MTF on the spatial domain. Ideal targets have simple mathematical relationships. Determine the MTF of an on-orbit satellite using in-scene targets: Slant-Edge, Line Source, point Source, and Radial Target. Attempt to facilitate the MTF calculation by automatically locating targets of opportunity. Incorporate MTF results into a product quality monitoring architecture."
Software to model AXAF image quality,80.895485,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"This draft final report describes the work performed under this delivery order from May 1992 through June 1993. The purpose of this contract was to enhance and develop an integrated optical performance modeling software for complex x-ray optical systems such as AXAF. The GRAZTRACE program developed by the MSFC Optical Systems Branch for modeling VETA-I was used as the starting baseline program. The original program was a large single file program and, therefore, could not be modified very efficiently. The original source code has been reorganized, and a 'Make Utility' has been written to update the original program. The new version of the source code consists of 36 small source files to make it easier for the code developer to manage and modify the program. A user library has also been built and a 'Makelib' utility has been furnished to update the library. With the user library, the users can easily access the GRAZTRACE source files and build a custom library. A user manual for the new version of GRAZTRACE has been compiled. The plotting capability for the 3-D point spread functions and contour plots has been provided in the GRAZTRACE using the graphics package DISPLAY. The Graphics emulator over the network has been set up for programming the graphics routine. The point spread function and the contour plot routines have also been modified to display the plot centroid, and to allow the user to specify the plot range, and the viewing angle options. A Command Mode version of GRAZTRACE has also been developed. More than 60 commands have been implemented in a Code-V like format. The functions covered in this version include data manipulation, performance evaluation, and inquiry and setting of internal parameters. The user manual for these commands has been formatted as in Code-V, showing the command syntax, synopsis, and options. An interactive on-line help system for the command mode has also been accomplished to allow the user to find valid commands, command syntax, and command function. A translation program has been written to convert FEA output from structural analysis to GRAZTRACE surface deformation file (.dfm file). The program can accept standard output files and list files from COSMOS/M and NASTRAN finite analysis programs. Some interactive options are also provided, such as Cartesian or cylindrical coordinate transformation, coordinate shift and scale, and axial length change. A computerized database for technical documents relating to the AXAF project has been established. Over 5000 technical documents have been entered into the master database. A user can now rapidly retrieve the desired documents relating to the AXAF project. The summary of the work performed under this contract is shown."
Introduction to computer image processing,80.32277,image quality calculation,['COMPUTERS'],"Theoretical backgrounds and digital techniques for a class of image processing problems are presented. Image formation in the context of linear system theory, image evaluation, noise characteristics, mathematical operations on image and their implementation are discussed. Various techniques for image restoration and image enhancement are presented. Methods for object extraction and the problem of pictorial pattern recognition and classification are discussed."
Fractal image compression,80.094604,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"Fractals are geometric or data structures which do not simplify under magnification. Fractal Image Compression is a technique which associates a fractal to an image. On the one hand, the fractal can be described in terms of a few succinct rules, while on the other, the fractal contains much or all of the image information. Since the rules are described with less bits of data than the image, compression results. Data compression with fractals is an approach to reach high compression ratios for large data streams related to images. The high compression ratios are attained at a cost of large amounts of computation. Both lossless and lossy modes are supported by the technique. The technique is stable in that small errors in codes lead to small errors in image data. Applications to the NASA mission are discussed."
NASA total quality management 1990 accomplishments report,79.18282,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"NASA's efforts in Total Quality Management are based on continuous improvement and serve as a foundation for NASA's present and future endeavors. Given here are numerous examples of quality strategies that have proven effective and efficient in a time when cost reduction is critical. These accomplishment benefit our Agency and help to achieve our primary goal, keeping American in the forefront of the aerospace industry."
A procedure for testing the quality of LANDSAT atmospheric correction algorithms,78.97655,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"There are two basic methods for testing the quality of an algorithm to minimize atmospheric effects on LANDSAT imagery: (1) test the results a posteriori, using ground truth or control points; (2) use a method based on image data plus estimation of additional ground and/or atmospheric parameters. A procedure based on the second method is described. In order to select the parameters, initially the image contrast is examined for a series of parameter combinations. The contrast improves for better corrections. In addition the correlation coefficient between two subimages, taken at different times, of the same scene is used for parameter's selection. The regions to be correlated should not have changed considerably in time. A few examples using this proposed procedure are presented."
Solar radiation at the Earth's surface:  Its calculation and inference from satellite imagery,78.34935,image quality calculation,['SPACE RADIATION'],"Physical and empirical models for calculation of insolation on a horizontal surface are described. Calculation of the spectral components of insolation and the calculation of the integrated values using wavelength-averaged values, ignoring aerosol effects, are discussed. Empirical models for determining insolation from meteorological data under clear and cloudy skies are described. The influence of hourly and daily global solar radiation from GOES satellite images is illustrated. Data acquisition methods for the Great Plains experiment, the determination of cloud free brightness levels, and determination of cloud parameters and target brightness are considered. The use of two and seven satellite images per day resulted in insolation determinations having a standard error of less then 10% of the mean. Use of only one image per day resulted in a standard error of about 20% of the mean."
The vectorization of a ray tracing program for image generation,77.26613,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"Ray tracing is a widely used method for producing realistic computer generated images. Ray tracing involves firing an imaginary ray from a view point, through a point on an image plane, into a three dimensional scene. The intersections of the ray with the objects in the scene determines what is visible at the point on the image plane. This process must be repeated many times, once for each point (commonly called a pixel) in the image plane. A typical image contains more than a million pixels making this process computationally expensive. A traditional ray tracing program processes one ray at a time. In such a serial approach, as much as ninety percent of the execution time is spent computing the intersection of a ray with the surface in the scene. With the CYBER 205, many rays can be intersected with all the bodies im the scene with a single series of vector operations. Vectorization of this intersection process results in large decreases in computation time. The CADLAB's interest in ray tracing stems from the need to produce realistic images of mechanical parts. A high quality image of a part during the design process can increase the productivity of the designer by helping him visualize the results of his work. To be useful in the design process, these images must be produced in a reasonable amount of time. This discussion will explain how the ray tracing process was vectorized and gives examples of the images obtained."
SAO/NASA joint investigation of astronomical viewing quality at Mount Hopkins Observatory:  1969-1971,76.73078,image quality calculation,['SPACE SCIENCES'],"Quantitative measurements of the astronomical seeing conditions have been made with a stellar-image monitor system at the Mt. Hopkins Observatory in Arizona. The results of this joint SAO-NASA experiment indicate that for a 15-cm-diameter telescope, image motion is typically 1 arcsec or less and that intensity fluctuations due to scintillation have a coefficient of irradiance variance of less than 0.12 on the average. Correlations between seeing quality and local meteorological conditions were investigated. Local temperature fluctuations and temperature gradients were found to be indicators of image-motion conditions, while high-altitude-wind conditions were shown to be somewhat correlated with scintillation-spectrum bandwidth. The theoretical basis for the relationship of atmospheric turbulence to optical effects is discussed in some detail, along with a description of the equipment used in the experiment. General site-testing comments and applications of the seeing-test results are also included."
Engineering design criteria for an image intensifier/image converter camera,76.59578,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"The design, display, and evaluation of an image intensifier/image converter camera which can be utilized in various requirements of spaceshuttle experiments are described. An image intensifier tube was utilized in combination with two brassboards as power supply and used for evaluation of night photography in the field. Pictures were obtained showing field details which would have been undistinguishable to the naked eye or to an ordinary camera."
Quality Control of Wind Data from 50-MHz Doppler Radar Wind Profiler,76.57806,image quality calculation,"['Launch Vehicles and Launch Operations', 'Quality Assurance and Reliability']","Upper-level wind profiles obtained from a 50-MHz Doppler Radar Wind Profiler (DRWP) instrument at Kennedy Space Center are incorporated in space launch vehicle design and day-of-launch operations to assess wind effects on the vehicle during ascent. Automated and manual quality control (QC) techniques are implemented to remove spurious data in the upper-level wind profiles caused from atmospheric and non-atmospheric artifacts over the 2010-2012 period of record (POR). By adding the new quality controlled profiles with older profiles from 1997-2009, a robust database will be constructed of upper-level wind characteristics. Statistical analysis will determine the maximum, minimum, and 95th percentile of the wind components from the DRWP profiles over recent POR and compare against the older database. Additionally, this study identifies specific QC flags triggered during the QC process to understand how much data is retained and removed from the profiles."
"The eighth NASA total quality management accomplishments report, 1990",76.13373,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"The eighth annual accomplishments report provides numerous examples of quality strategies that have proven effective and efficient in a time when cost reduction is critical. NASA's continuous improvement efforts can provide insight for others to succeed in their own endeavors. The report covers: top management leadership and support, strategic planning, focus on the customer, employee training and recognition, employee empowerment and teamwork, measurement and analysis, and quality assurance."
The Airline Quality Rating 1999,75.61634,image quality calculation,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple criteria. This current report, Airline Quality Rating 1999, reflects an updated approach to calculating monthly Airline Quality Rating scores for 1998. AQR scores for the calendar year 1998 are based on 15 elements that focus on airline performance areas important to air travel consumers. The Airline Quality Rating is a summary of month-by-month quality ratings for the ten major U.S. airlines operating during 1998. Using the Airline Quality Rating system of weighted averages and monthly performance data in the areas of on-time arrivals, involuntary denied boardings, mishandled baggage, and a combination of 12 customer complaint categories, major airlines comparative performance for the calendar year 1998 is reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major airlines domestic operations for the 12 month period of 1998, and industry average results. Also, comparative Airline Quality Rating data for 1997, using the updated criteria, are included to provide a reference point regarding quality in the industry."
The Eighth Annual NASA/Contractors Conference and 1991 National Symposium on Quality and Productivity: Extending the boundaries of total quality management,75.58726,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"The Eighth Annual NASA/Contractors Conference and 1991 National Symposium on Quality and Productivity provided a forum to exchange knowledge and experiences in these areas of continuous improvement. The more than 1,100 attendees from government, industry, academia, community groups, and the international arena had a chance to learn about methods, tools, and strategies for excellence and to discuss continuous improvement strategies, successes, and failures. This event, linked via satellite to concurrent conferences hosted by the NASA Goddard Space Flight Center in Greenbelt, Maryland, and Martin Marietta Astronautics Group in Denver, Colorado, also explored extending the boundaries of Total Quality Management to include partnerships for quality within communities and encouraged examination, evaluation, and change to incorporate the principles of continuous improvement."
APQ-102 imaging radar digital image quality study,75.15711,image quality calculation,['COMMUNICATIONS AND RADAR'],A modified APQ-102 sidelooking radar collected synthetic aperture radar (SAR) data which was digitized and recorded on wideband magnetic tape. These tapes were then ground processed into computer compatible tapes (CCT's). The CCT's may then be processed into high resolution radar images by software on the CYBER computer.
Proceedings of the NASA Workshop on Image Analysis,74.61089,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Three major topics of image analysis are addressed: segmentation, shape and texture analysis, and structural analysis."
Considerations of digital image-processing,74.47824,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],System techniques and parameter boundaries for digital image processing
Astronomical use of television-type image sensors,74.252075,image quality calculation,['SPACE SCIENCES'],Conference on using TV type image sensors in astronomical photometry
Application of imaging and ultrasound to the quality grading of beef,74.00054,image quality calculation,['ACOUSTICS'],"The results of a study conducted to assist the Department of Agriculture in the task of considering innovative methods for the grading of carcass beef for human consumption is presented. The processing of photographic, television and ultrasound images of the longissimus dorsi muscle at the 12/13th rib cut was undertaken. The results showed that a correlation could be developed between the quality grade of the carcass as determined by a professional grader, and the fat to area ratio of the muscle as determined by image processing techniques. In addition, the use of ultrasound shows the potential for grading of an unsliced carcass or a live animal."
Image data processing of earth resources management,73.822624,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Various image processing and information extraction systems are described along with the design and operation of an interactive multispectral information system, IMAGE 100. Analyses of ERTS data, using IMAGE 100, over a number of U.S. sites are presented. The following analyses are included: investigations of crop inventory and management using remote sensing; and (2) land cover classification for environmental impact assessments. Results show that useful information is provided by IMAGE 100 analyses of ERTS data in digital form."
Spacecraft camera image registration,73.588554,image quality calculation,['Instrumentation and Photography'],"A system for achieving spacecraft camera (1, 2) image registration comprises a portion external to the spacecraft and an image motion compensation system (IMCS) portion onboard the spacecraft. Within the IMCS, a computer (38) calculates an image registration compensation signal (60) which is sent to the scan control loops (84, 88, 94, 98) of the onboard cameras (1, 2). At the location external to the spacecraft, the long-term orbital and attitude perturbations on the spacecraft are modeled. Coefficients (K, A) from this model are periodically sent to the onboard computer (38) by means of a command unit (39). The coefficients (K, A) take into account observations of stars and landmarks made by the spacecraft cameras (1, 2) themselves. The computer (38) takes as inputs the updated coefficients (K, A) plus synchronization information indicating the mirror position (AZ, EL) of each of the spacecraft cameras (1, 2), operating mode, and starting and stopping status of the scan lines generated by these cameras (1, 2), and generates in response thereto the image registration compensation signal (60). The sources of periodic thermal errors on the spacecraft are discussed. The system is checked by calculating measurement residuals, the difference between the landmark and star locations predicted at the external location and the landmark and star locations as measured by the spacecraft cameras (1, 2)."
Dictionary Approaches to Image Compression and Reconstruction,73.29102,image quality calculation,['Computer Programming and Software'],"This paper proposes using a collection of parameterized waveforms, known as a dictionary, for the purpose of medical image compression. These waveforms, denoted as lambda, are discrete time signals, where y represents the dictionary index. A dictionary with a collection of these waveforms Is typically complete or over complete. Given such a dictionary, the goal is to obtain a representation Image based on the dictionary. We examine the effectiveness of applying Basis Pursuit (BP), Best Orthogonal Basis (BOB), Matching Pursuits (MP), and the Method of Frames (MOF) methods for the compression of digitized radiological images with a wavelet-packet dictionary. The performance of these algorithms is studied for medical images with and without additive noise."
TM digital image products for applications,73.23488,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"The image characteristics of digital data generated by LANDSAT 4 thematic mapper (TM) are discussed. Digital data from the TM resides in tape files at various stages of image processing. Within each image data file, the image lines are blocked by a factor of either 5 for a computer compatible tape CCT-BT, or 4 for a CCT-AT and CCT-PT; in each format, the image file has a different format. Nominal geometric corrections which provide proper geodetic relationships between different parts of the image are available only for the CCT-PT. It is concluded that detector 3 of band 5 on the TM does not respond; this channel of data needs replacement. The empty bin phenomenon in CCT-AT images results from integer truncations of mixed-mode arithmetric operations."
"Theory, Image Simulation, and Data Analysis of Chemical Release Experiments",73.20979,image quality calculation,['SPACE RADIATION'],"The final phase of Grant NAG6-1 involved analysis of physics of chemical releases in the upper atmosphere and analysis of data obtained on previous NASA sponsored chemical release rocket experiments. Several lines of investigation of past chemical release experiments and computer simulations have been proceeding in parallel. This report summarizes the work performed and the resulting publications. The following topics are addressed: analysis of the 1987 Greenland rocket experiments; calculation of emission rates for barium, strontium, and calcium; the CRIT 1 and 2 experiments (Collisional Ionization Cross Section experiments); image calibration using background stars; rapid ray motions in ionospheric plasma clouds; and the NOONCUSP rocket experiments."
Dictionary Approaches to Image Compression and Reconstruction,73.162125,image quality calculation,['Computer Programming and Software'],"This paper proposes using a collection of parameterized waveforms, known as a dictionary, for the purpose of medical image compression. These waveforms, denoted as phi(sub gamma), are discrete time signals, where gamma represents the dictionary index. A dictionary with a collection of these waveforms is typically complete or overcomplete. Given such a dictionary, the goal is to obtain a representation image based on the dictionary. We examine the effectiveness of applying Basis Pursuit (BP), Best Orthogonal Basis (BOB), Matching Pursuits (MP), and the Method of Frames (MOF) methods for the compression of digitized radiological images with a wavelet-packet dictionary. The performance of these algorithms is studied for medical images with and without additive noise."
The Effect of Experimental Variables on Industrial X-Ray Micro-Computed Sensitivity,72.69149,image quality calculation,['Quality Assurance and Reliability'],"A study was performed on the effect of experimental variables on radiographic sensitivity (image quality) in x-ray micro-computed tomography images for a high density thin wall metallic cylinder containing micro-EDM holes. Image quality was evaluated in terms of signal-to-noise ratio, flaw detectability, and feature sharpness. The variables included: day-to-day reproducibility, current, integration time, voltage, filtering, number of frame averages, number of projection views, beam width, effective object radius, binning, orientation of sample, acquisition angle range (180deg to 360deg), and directional versus transmission tube. "
Quality Assessment of Landsat Surface Reflectance Products Using MODIS Data,72.34439,image quality calculation,['Earth Resources and Remote Sensing'],"Surface reflectance adjusted for atmospheric effects is a primary input for land cover change detection and for developing many higher level surface geophysical parameters. With the development of automated atmospheric correction algorithms, it is now feasible to produce large quantities of surface reflectance products using Landsat images. Validation of these products requires in situ measurements, which either do not exist or are difficult to obtain for most Landsat images. The surface reflectance products derived using data acquired by the Moderate Resolution Imaging Spectroradiometer (MODIS), however, have been validated more comprehensively. Because the MODIS on the Terra platform and the Landsat 7 are only half an hour apart following the same orbit, and each of the 6 Landsat spectral bands overlaps with a MODIS band, good agreements between MODIS and Landsat surface reflectance values can be considered indicators of the reliability of the Landsat products, while disagreements may suggest potential quality problems that need to be further investigated. Here we develop a system called Landsat-MODIS Consistency Checking System (LMCCS). This system automatically matches Landsat data with MODIS observations acquired on the same date over the same locations and uses them to calculate a set of agreement metrics. To maximize its portability, Java and open-source libraries were used in developing this system, and object-oriented programming (OOP) principles were followed to make it more flexible for future expansion. As a highly automated system designed to run as a stand-alone package or as a component of other Landsat data processing systems, this system can be used to assess the quality of essentially every Landsat surface reflectance image where spatially and temporally matching MODIS data are available. The effectiveness of this system was demonstrated using it to assess preliminary surface reflectance products derived using the Global Land Survey (GLS) Landsat images for the 2000 epoch. As surface reflectance likely will be a standard product for future Landsat missions, the approach developed in this study can be adapted as an operational quality assessment system for those missions."
Image selection system:  Operator's procedures manual,72.22973,image quality calculation,['DOCUMENTATION AND INFORMATION SCIENCE'],Essential retrieval parameter tables on the Image Selection System were consolidated from two separate operations manuals to provide on example of a typical retrieval session.
Particle Image Velocimetry Measurements to Evaluate the Effectiveness of Deck-Edge Columnar Vortex Generators on Aircraft Carriers,72.195145,image quality calculation,['Fluid Mechanics and Thermodynamics'],"Candidate passive flow control devices were chosen from a NASA flow visualization study to investigate their effectiveness at improving flow quality over a flat-top carrier model. Flow over the deck was analyzed using a particle image velocimeter and a 1/120th scaled carrier model in a low-speed wind tunnel. Baseline (no devices) flow quality was compared to flow quality from combinations of bow and deck-edge devices at both zero and 20 degrees yaw. Devices included plain flaps and spiral cross-section columnar vortex generators attached in various combinations to the front and sides of the deck. Centerline and cross plane measurements were made with velocity and average turbulence measurements reported. Results show that the bow/deck-edge flap and bow/deck-edge columnar vortex generator pairs reduce flight deck turbulence both at zero yaw and at 20 degrees yaw by a factor of approximately 20. Of the devices tested, the most effective bow-only device appears to be the plain flap."
Proceedings of the Conference on Parallel Image Processing for Earth Observation Systems,72.02989,image quality calculation,"['PHYSICS, GENERAL']",Conference on parallel image processing for earth observation systems
A quality monitor and monitoring technique employing optically stimulated electron emission,71.973206,image quality calculation,['QUALITY ASSURANCE AND RELIABILITY'],"A light source directs ultraviolet light onto a test surface and a detector detects a current of photoelectrons generated by the light. The detector includes a collector which is positively biased with respect to the test surface. Quality is indicated based on the photoelectron current. The collector is then negatively biased to replace charges removed by the measurement of a nonconducting substrate to permit subsequent measurements. Also, the intensity of the ultraviolet light at a particular wavelength is monitored and the voltage of the light source varied to maintain the light a constant desired intensity. The light source is also cooled via a gas circulation system. If the test surface is an insulator, the surface is bombarded with ultraviolet light in the presence of an electron field to remove the majority of negative charges from the surface. The test surface is then exposed to an ion field until it possesses no net charge. The technique described above is then performed to assess quality."
Monitoring water quality from LANDSAT,71.563576,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Water quality monitoring possibilities from LANDSAT were demonstrated both for direct readings of reflectances from the water and indirect monitoring of changes in use of land surrounding Swift Creek Reservoir in a joint project with the Virginia State Water Control Board and NASA. Film products were shown to have insufficient resolution and all work was done by digitally processing computer compatible tapes. Land cover maps of the 18,000 hectare Swift Creek Reservoir watershed, prepared for two dates in 1974, are shown. A significant decrease in the pine cover was observed in a 740 hectare construction site within the watershed. A measure of the accuracy of classification was obtained by comparing the LANDSAT results with visual classification at five sites on a U-2 photograph. Such changes in land cover can alert personnel to watch for potential changes in water quality."
Foundations for Measuring Volume Rendering Quality,71.50374,image quality calculation,['Computer Programming and Software'],"The goal of this paper is to provide a foundation for objectively comparing volume rendered images. The key elements of the foundation are: (1) a rigorous specification of all the parameters that need to be specified to define the conditions under which a volume rendered image is generated; (2) a methodology for difference classification, including a suite of functions or metrics to quantify and classify the difference between two volume rendered images that will support an analysis of the relative importance of particular differences. The results of this method can be used to study the changes caused by modifying particular parameter values, to compare and quantify changes between images of similar data sets rendered in the same way, and even to detect errors in the design, implementation or modification of a volume rendering system. If one has a benchmark image, for example one created by a high accuracy volume rendering system, the method can be used to evaluate the accuracy of a given image."
An Investigation Into HPLC Data Quality Problems,71.34816,image quality calculation,['Quality Assurance and Reliability'],"This report summarizes the analyses and results produced by a five-member investigative team of Government, university, and industry experts, established by NASA HQ. The team examined data quality problems associated with high performance liquid chromatography (HPLC) analyses of pigment concentrations in seawater samples produced by the San Diego State University (SDSU) Center for Hydro-Optics and Remote Sensing (CHORS). This report shows CHORS did not validate the methods used before placing them into service to analyze field samples for NASA principal investigators (PIs), even though the HPLC literature contained easily accessible method validation procedures, and the importance of implementing them, more than a decade ago. In addition, there were so many sources of significant variance in the CHORS methodologies, that the HPLC system rarely operated within performance criteria capable of producing the requisite data quality. It is the recommendation of the investigative team to a) not correct the data, b) make all the data that was temporarily sequestered available for scientific use, and c) label the affected data with an appropriate warning, e.g., ""These data are not validated and should not be used as the sole basis for a scientific result, conclusion, or hypothesis--independent corroborating evidence is required."""
Application of ESE Data and Tools to Air Quality Management: Services for Helping the Air Quality Community use ESE Data (SHAirED),70.96332,image quality calculation,['Earth Resources and Remote Sensing'],"The goal of this REASoN applications and technology project is to deliver and use Earth Science Enterprise (ESE) data and tools in support of air quality management. Its scope falls within the domain of air quality management and aims to develop a federated air quality information sharing network that includes data from NASA, EPA, US States and others. Project goals were achieved through a access of satellite and ground observation data, web services information technology, interoperability standards, and air quality community collaboration. In contributing to a network of NASA ESE data in support of particulate air quality management, the project will develop access to distributed data, build Web infrastructure, and create tools for data processing and analysis. The key technologies used in the project include emerging web services for developing self describing and modular data access and processing tools, and service oriented architecture for chaining web services together to assemble customized air quality management applications. The technology and tools required for this project were developed within DataFed.net, a shared infrastructure that supports collaborative atmospheric data sharing and processing web services. Much of the collaboration was facilitated through community interactions through the Federation of Earth Science Information Partners (ESIP) Air Quality Workgroup. The main activities during the project that successfully advanced DataFed, enabled air quality applications and established community-oriented infrastructures were: develop access to distributed data (surface and satellite), build Web infrastructure to support data access, processing and analysis create tools for data processing and analysis foster air quality community collaboration and interoperability."
Image enhancement by non-linear extrapolation in frequency space,70.76153,image quality calculation,['Instrumentation and Photography'],"An input image is enhanced to include spatial frequency components having frequencies higher than those in an input image. To this end, an edge map is generated from the input image using a high band pass filtering technique. An enhancing map is subsequently generated from the edge map, with the enhanced map having spatial frequencies exceeding an initial maximum spatial frequency of the input image. The enhanced map is generated by applying a non-linear operator to the edge map in a manner which preserves the phase transitions of the edges of the input image. The enhanced map is added to the input image to achieve a resulting image having spatial frequencies greater than those in the input image. Simplicity of computations and ease of implementation allow for image sharpening after enlargement and for real-time applications such as videophones, advanced definition television, zooming, and restoration of old motion pictures."
"Urban air quality estimation study, phase 1",70.666954,image quality calculation,['ENVIRONMENT POLLUTION'],"Possibilities are explored for applying estimation theory to the analysis, interpretation, and use of air quality measurements in conjunction with simulation models to provide a cost effective method of obtaining reliable air quality estimates for wide urban areas. The physical phenomenology of real atmospheric plumes from elevated localized sources is discussed. A fluctuating plume dispersion model is derived. Individual plume parameter formulations are developed along with associated a priori information. Individual measurement models are developed."
Data analysis for GOPEX image frames,70.49371,image quality calculation,['COMMUNICATIONS AND RADAR'],"The data analysis based on the image frames received at the Solid State Imaging (SSI) camera of the Galileo Optical Experiment (GOPEX) demonstration conducted between 9-16 Dec. 1992 is described. Laser uplink was successfully established between the ground and the Galileo spacecraft during its second Earth-gravity-assist phase in December 1992. SSI camera frames were acquired which contained images of detected laser pulses transmitted from the Table Mountain Facility (TMF), Wrightwood, California, and the Starfire Optical Range (SOR), Albuquerque, New Mexico. Laser pulse data were processed using standard image-processing techniques at the Multimission Image Processing Laboratory (MIPL) for preliminary pulse identification and to produce public release images. Subsequent image analysis corrected for background noise to measure received pulse intensities. Data were plotted to obtain histograms on a daily basis and were then compared with theoretical results derived from applicable weak-turbulence and strong-turbulence considerations. Processing steps are described and the theories are compared with the experimental results. Quantitative agreement was found in both turbulence regimes, and better agreement would have been found, given more received laser pulses. Future experiments should consider methods to reliably measure low-intensity pulses, and through experimental planning to geometrically locate pulse positions with greater certainty."
Image Navigation and Registration Performance Assessment Tool Set for the GOES-R Advanced Baseline Imager and Geostationary Lightning Mapper,70.47027,image quality calculation,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']","The GOES-R Flight Project has developed an Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for measuring Advanced Baseline Imager (ABI) and Geostationary Lightning Mapper (GLM) INR performance metrics in the post-launch period for performance evaluation and long term monitoring. For ABI, these metrics are the 3-sigma errors in navigation (NAV), channel-to-channel registration (CCR), frame-to-frame registration (FFR), swath-to-swath registration (SSR), and within frame registration (WIFR) for the Level 1B image products. For GLM, the single metric of interest is the 3-sigma error in the navigation of background images (GLM NAV) used by the system to navigate lightning strikes. 3-sigma errors are estimates of the 99.73rd percentile of the errors accumulated over a 24-hour data collection period. IPATS utilizes a modular algorithmic design to allow user selection of data processing sequences optimized for generation of each INR metric. This novel modular approach minimizes duplication of common processing elements, thereby maximizing code efficiency and speed. Fast processing is essential given the large number of sub-image registrations required to generate INR metrics for the many images produced over a 24-hour evaluation period. Another aspect of the IPATS design that vastly reduces execution time is the off-line propagation of Landsat based truth images to the fixed grid coordinates system for each of the three GOES-R satellite locations, operational East and West and initial checkout locations. This paper describes the algorithmic design and implementation of IPATS and provides preliminary test results."
Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for the GOES-R Advanced Baseline Imager and Geostationary Lightning Mapper,70.369484,image quality calculation,"['Instrumentation and Photography', 'Meteorology and Climatology']","The GOES-R Flight Project has developed an Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for measuring Advanced Baseline Imager (ABI) and Geostationary Lightning Mapper (GLM) INR performance metrics in the post-launch period for performance evaluation and long term monitoring. For ABI, these metrics are the 3-sigma errors in navigation (NAV), channel-to-channel registration (CCR), frame-to-frame registration (FFR), swath-to-swath registration (SSR), and within frame registration (WIFR) for the Level 1B image products. For GLM, the single metric of interest is the 3-sigma error in the navigation of background images (GLM NAV) used by the system to navigate lightning strikes. 3-sigma errors are estimates of the 99.73rd percentile of the errors accumulated over a 24 hour data collection period. IPATS utilizes a modular algorithmic design to allow user selection of data processing sequences optimized for generation of each INR metric. This novel modular approach minimizes duplication of common processing elements, thereby maximizing code efficiency and speed. Fast processing is essential given the large number of sub-image registrations required to generate INR metrics for the many images produced over a 24 hour evaluation period. Another aspect of the IPATS design that vastly reduces execution time is the off-line propagation of Landsat based truth images to the fixed grid coordinates system for each of the three GOES-R satellite locations, operational East and West and initial checkout locations. This paper describes the algorithmic design and implementation of IPATS and provides preliminary test results."
Single Transducer Ultrasonic Imaging Method that Eliminates the Effect of Plate Thickness Variation in the Image,70.32366,image quality calculation,['Quality Assurance and Reliability'],"This article describes a single transducer ultrasonic imaging method that eliminates the effect of plate thickness variation in the image. The method thus isolates ultrasonic variations due to material microstructure. The use of this method can result in significant cost savings because the ultrasonic image can be interpreted correctly without the need for machining to achieve precise thickness uniformity during nondestructive evaluations of material development. The method is based on measurement of ultrasonic velocity. Images obtained using the thickness-independent methodology are compared with conventional velocity and c-scan echo peak amplitude images for monolithic ceramic (silicon nitride), metal matrix composite and polymer matrix composite materials. It was found that the thickness-independent ultrasonic images reveal and quantify correctly areas of global microstructural (pore and fiber volume fraction) variation due to the elimination of thickness effects. The thickness-independent ultrasonic imaging method described in this article is currently being commercialized under a cooperative agreement between NASA Lewis Research Center and Sonix, Inc."
The New CCSDS Image Compression Recommendation,70.29498,image quality calculation,['Instrumentation and Photography'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-Earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An Application-Specific Integrated Circuit (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images."
Washington Health & Air Quality: Quantifying Air Quality Parameters and Validating Air Pollution Sources Impacting the Health of Puget Sound Residents Through the Use of NASA and ESA Remote Sensing Data,70.278336,image quality calculation,['Earth Resources and Remote Sensing'],"In the Puget Sound region of Washington, high levels of air pollutants put residents health at risk by increasing their likelihood of developing critical respiratory conditions. This project used remotely-sensed data to investigate aerosol optical depth (AOD) from NASA satellite sensors including the Terra and Aqua MODerate Resolution Imaging Spectroradiometer (MODIS) and European Space Agency Copernicus Sentinel-5 Precursor TROPOspheric Monitoring Instrument (TROPOMI). The team visualized the most recent data in Google Earth Engine (GEE) API to display air pollution trends in Washington State, which will support the Puget Sound Clean Air Agencys (PSCAA) decision-making processes. The team performed linear regressions using the Multi-Angle Implementation of Atmospheric Correction (MAIAC) algorithm to form a relationship between ground-level microscopic particles (PM2.5) and AOD in the Puget Sound region, validating the relationship using concentration readings taken from Environmental Protection Agency (EPA) air quality monitors. The team utilized estimated PM2.5 and other satellite data to produce a web-based tool and to evaluate the effectiveness of using such a tool for near real-time air quality monitoring within a particular region. The team found that the tool provides useful supplementary data that fills in the gaps of the PSCAAs air monitoring network."
Visual simulation image generation using a flying-spot scanner,70.08104,image quality calculation,"['FACILITIES, RESEARCH, AND SUPPORT']",Visual simulation image generation using flying spot scanner
Information-Adaptive Image Encoding and Restoration,70.07871,image quality calculation,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) has shown itself to be a very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition. A number of algorithms exist that provide one or more of these features, but not all. In this paper we compare the performance of the MSRCR with techniques that are widely used for image enhancement. Specifically, we compare the MSRCR with color adjustment methods such as gamma correction and gain/offset application, histogram modification techniques such as histogram equalization and manual histogram adjustment, and other more powerful techniques such as homomorphic filtering and 'burning and dodging'. The comparison is carried out by testing the suite of image enhancement methods on a set of diverse images. We find that though some of these techniques work well for some of these images, only the MSRCR performs universally well oil the test set."
Image dissector camera system study,70.05022,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"Various aspects of a rendezvous and docking system using an image dissector detector as compared to a GaAs detector were discussed. Investigation into a gimbled scanning system is also covered and the measured video response curves from the image dissector camera are presented. Rendezvous will occur at ranges greater than 100 meters. The maximum range considered was 1000 meters. During docking, the range, range-rate, angle, and angle-rate to each reflector on the satellite must be measured. Docking range will be from 3 to 100 meters. The system consists of a CW laser diode transmitter and an image dissector receiver. The transmitter beam is amplitude modulated with three sine wave tones for ranging. The beam is coaxially combined with the receiver beam. Mechanical deflection of the transmitter beam, + or - 10 degrees in both X and Y, can be accomplished before or after it is combined with the receiver beam. The receiver will have a field-of-view (FOV) of 20 degrees and an instantaneous field-of-view (IFOV) of two milliradians (mrad) and will be electronically scanned in the image dissector. The increase in performance obtained from the GaAs photocathode is not needed to meet the present performance requirements."
Retinex Preprocessing for Improved Multi-Spectral Image Classification,69.7786,image quality calculation,['Computer Programming and Software'],"The goal of multi-image classification is to identify and label ""similar regions"" within a scene. The ability to correctly classify a remotely sensed multi-image of a scene is affected by the ability of the classification process to adequately compensate for the effects of atmospheric variations and sensor anomalies. Better classification may be obtained if the multi-image is preprocessed before classification, so as to reduce the adverse effects of image formation. In this paper, we discuss the overall impact on multi-spectral image classification when the retinex image enhancement algorithm is used to preprocess multi-spectral images. The retinex is a multi-purpose image enhancement algorithm that performs dynamic range compression, reduces the dependence on lighting conditions, and generally enhances apparent spatial resolution. The retinex has been successfully applied to the enhancement of many different types of grayscale and color images. We show in this paper that retinex preprocessing improves the spatial structure of multi-spectral images and thus provides better within-class variations than would otherwise be obtained without the preprocessing. For a series of multi-spectral images obtained with diffuse and direct lighting, we show that without retinex preprocessing the class spectral signatures vary substantially with the lighting conditions. Whereas multi-dimensional clustering without preprocessing produced one-class homogeneous regions, the classification on the preprocessed images produced multi-class non-homogeneous regions. This lack of homogeneity is explained by the interaction between different agronomic treatments applied to the regions: the preprocessed images are closer to ground truth. The principle advantage that the retinex offers is that for different lighting conditions classifications derived from the retinex preprocessed images look remarkably ""similar"", and thus more consistent, whereas classifications derived from the original images, without preprocessing, are much less similar."
Planetary image conversion task,69.74196,image quality calculation,['SPACE SCIENCES (GENERAL)'],"The Planetary Image Conversion Task group processed 12,500 magnetic tapes containing raw imaging data from JPL planetary missions and produced an image data base in consistent format on 1200 fully packed 6250-bpi tapes. The output tapes will remain at JPL. A copy of the entire tape set was delivered to US Geological Survey, Flagstaff, Ariz. A secondary task converted computer datalogs, which had been stored in project specific MARK IV File Management System data types and structures, to flat-file, text format that is processable on any modern computer system. The conversion processing took place at JPL's Image Processing Laboratory on an IBM 370-158 with existing software modified slightly to meet the needs of the conversion task. More than 99% of the original digital image data was successfully recovered by the conversion task. However, processing data tapes recorded before 1975 was destructive. This discovery is of critical importance to facilities responsible for maintaining digital archives since normal periodic random sampling techniques would be unlikely to detect this phenomenon, and entire data sets could be wiped out in the act of generating seemingly positive sampling results. Reccomended follow-on activities are also included."
Image 100 procedures manual development: Applications system library definition and Image 100 software definition,69.59287,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],An outline for an Image 100 procedures manual for Earth Resources Program image analysis was developed which sets forth guidelines that provide a basis for the preparation and updating of an Image 100 Procedures Manual. The scope of the outline was limited to definition of general features of a procedures manual together with special features of an interactive system. Computer programs were identified which should be implemented as part of an applications oriented library for the system.
Image data processing system requirements study. Volume 1:  Analysis,69.42568,image quality calculation,['GEOPHYSICS'],"Digital image processing, image recorders, high-density digital data recorders, and data system element processing for use in an Earth Resources Survey image data processing system are studied. Loading to various ERS systems is also estimated by simulation."
Analyses of requirements for computer control and data processing experiment subsystems.  Volume 1: ATM experiment S-056 image data processing system techniques development,69.36048,image quality calculation,['COMPUTERS'],"The solar imaging X-ray telescope experiment (designated the S-056 experiment) is described. It will photograph the sun in the far ultraviolet or soft X-ray region. Because of the imaging characteristics of this telescope and the necessity of using special techniques for capturing images on film at these wave lengths, methods were developed for computer processing of the photographs. The problems of image restoration were addressed to develop and test digital computer techniques for applying a deconvolution process to restore overall S-056 image quality. Additional techniques for reducing or eliminating the effects of noise and nonlinearity in S-056 photographs were developed."
Model based estimation of image depth and displacement,69.13018,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"Passive depth and displacement map determinations have become an important part of computer vision processing. Applications that make use of this type of information include autonomous navigation, robotic assembly, image sequence compression, structure identification, and 3-D motion estimation. With the reliance of such systems on visual image characteristics, a need to overcome image degradations, such as random image-capture noise, motion, and quantization effects, is clearly necessary. Many depth and displacement estimation algorithms also introduce additional distortions due to the gradient operations performed on the noisy intensity images. These degradations can limit the accuracy and reliability of the displacement or depth information extracted from such sequences. Recognizing the previously stated conditions, a new method to model and estimate a restored depth or displacement field is presented. Once a model has been established, the field can be filtered using currently established multidimensional algorithms. In particular, the reduced order model Kalman filter (ROMKF), which has been shown to be an effective tool in the reduction of image intensity distortions, was applied to the computed displacement fields. Results of the application of this model show significant improvements on the restored field. Previous attempts at restoring the depth or displacement fields assumed homogeneous characteristics which resulted in the smoothing of discontinuities. In these situations, edges were lost. An adaptive model parameter selection method is provided that maintains sharp edge boundaries in the restored field. This has been successfully applied to images representative of robotic scenarios. In order to accommodate image sequences, the standard 2-D ROMKF model is extended into 3-D by the incorporation of a deterministic component based on previously restored fields. The inclusion of past depth and displacement fields allows a means of incorporating the temporal information into the restoration process. A summary on the conditions that indicate which type of filtering should be applied to a field is provided."
Gray-level transformations for interactive image enhancement,68.55573,image quality calculation,['CYBERNETICS'],A gray-level transformation method suitable for interactive image enhancement was presented. It is shown that the well-known histogram equalization approach is a special case of this method. A technique for improving the uniformity of a histogram is also developed. Experimental results which illustrate the capabilities of both algorithms are described. Two proposals for implementing gray-level transformations in a real-time interactive image enhancement system are also presented.
The Extended-Image Tracking Technique Based on the Maximum Likelihood Estimation,68.39496,image quality calculation,['Communications and Radar'],"This paper describes an extended-image tracking technique based on the maximum likelihood estimation. The target image is assume to have a known profile covering more than one element of a focal plane detector array. It is assumed that the relative position between the imager and the target is changing with time and the received target image has each of its pixels disturbed by an independent additive white Gaussian noise. When a rotation-invariant movement between imager and target is considered, the maximum likelihood based image tracking technique described in this paper is a closed-loop structure capable of providing iterative update of the movement estimate by calculating the loop feedback signals from a weighted correlation between the currently received target image and the previously estimated reference image in the transform domain. The movement estimate is then used to direct the imager to closely follow the moving target. This image tracking technique has many potential applications, including free-space optical communications and astronomy where accurate and stabilized optical pointing is essential."
Computer image processing:  Geologic applications,68.369675,image quality calculation,['GEOPHYSICS'],"Computer image processing of digital data was performed to support several geological studies. The specific goals were to: (1) relate the mineral content to the spectral reflectance of certain geologic materials, (2) determine the influence of environmental factors, such as atmosphere and vegetation, and (3) improve image processing techniques. For detection of spectral differences related to mineralogy, the technique of band ratioing was found to be the most useful. The influence of atmospheric scattering and methods to correct for the scattering were also studied. Two techniques were used to correct for atmospheric effects: (1) dark object subtraction, (2) normalization of use of ground spectral measurements. Of the two, the first technique proved to be the most successful for removing the effects of atmospheric scattering. A digital mosaic was produced from two side-lapping LANDSAT frames. The advantages were that the same enhancement algorithm can be applied to both frames, and there is no seam where the two images are joined."
Simulating the X-Ray Image Contrast to Set-Up Techniques with Desired Flaw Detectability,67.82349,image quality calculation,"['Instrumentation and Photography', 'Quality Assurance and Reliability']","The paper provides simulation data of previous work by the author in developing a model for estimating detectability of crack-like flaws in radiography. The methodology is being developed to help in implementation of NASA Special x-ray radiography qualification, but is generically applicable to radiography. The paper describes a method for characterizing X-ray detector resolution for crack detection. Applicability of ASTM E 2737 resolution requirements to the model are also discussed. The paper describes a model for simulating the detector resolution. A computer calculator application, discussed here, also performs predicted contrast and signal-to-noise ratio calculations. Results of various simulation runs in calculating x-ray flaw size parameter and image contrast for varying input parameters such as crack depth, crack width, part thickness, x-ray angle, part-to-detector distance, part-to-source distance, source sizes, and detector sensitivity and resolution are given as 3D surfaces. These results demonstrate effect of the input parameters on the flaw size parameter and the simulated image contrast of the crack. These simulations demonstrate utility of the flaw size parameter model in setting up x-ray techniques that provide desired flaw detectability in radiography. The method is applicable to film radiography, computed radiography, and digital radiography."
Real-time optical image processing techniques,67.761925,image quality calculation,['OPTICS'],"Nonlinear real-time optical processing on spatial pulse frequency modulation has been pursued through the analysis, design, and fabrication of pulse frequency modulated halftone screens and the modification of micro-channel spatial light modulators (MSLMs). Micro-channel spatial light modulators are modified via the Fabry-Perot method to achieve the high gamma operation required for non-linear operation. Real-time nonlinear processing was performed using the halftone screen and MSLM. The experiments showed the effectiveness of the thresholding and also showed the needs of higher SBP for image processing. The Hughes LCLV has been characterized and found to yield high gamma (about 1.7) when operated in low frequency and low bias mode. Cascading of two LCLVs should also provide enough gamma for nonlinear processing. In this case, the SBP of the LCLV is sufficient but the uniformity of the LCLV needs improvement. These include image correlation, computer generation of holograms, pseudo-color image encoding for image enhancement, and associative-retrieval in neural processing. The discovery of the only known optical method for dynamic range compression of an input image in real-time by using GaAs photorefractive crystals is reported. Finally, a new architecture for non-linear multiple sensory, neural processing has been suggested."
Range and Intensity Image-Based Terrain and Vehicle Relative Pose Estimation System,67.57616,image quality calculation,['Earth Resources and Remote Sensing'],"A navigation system includes an image acquisition device for acquiring a range image of a target vehicle, at least one processor, a memory including a target vehicle model and computer readable program code, where the processor and the computer readable program code are configured to cause the navigation system to convert the range image to a point cloud having three dimensions, compute a transform from the target vehicle model to the point cloud, and use the transform to estimate the target vehicle's attitude and position for capturing the target vehicle."
The 3-D image recognition based on fuzzy neural network technology,67.481346,image quality calculation,['CYBERNETICS'],"Three dimensional stereoscopic image recognition system based on fuzzy-neural network technology was developed. The system consists of three parts; preprocessing part, feature extraction part, and matching part. Two CCD color camera image are fed to the preprocessing part, where several operations including RGB-HSV transformation are done. A multi-layer perception is used for the line detection in the feature extraction part. Then fuzzy matching technique is introduced in the matching part. The system is realized on SUN spark station and special image input hardware system. An experimental result on bottle images is also presented."
Always-Convergent Iterative Noise Removal and Deconvolution for Image Data,67.468155,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"Linear filtering techniques currently used for the restoration of noisy, blurred or otherwise degraded image data are discussed and new techniques related to the iterative techniques of Morrison and van Cittert are developed and implemented. Programs written for the implementation are discussed in the appendices. It is shown that the new techniques are convergent for any system response function, and they are applied to the task of restoring a severely blurred image."
A database system to support image algorithm evaluation,67.36529,image quality calculation,['COMPUTER PROGRAMMING AND SOFTWARE'],"The design is given of an interactive image database system IMDB, which allows the user to create, retrieve, store, display, and manipulate images through the facility of a high-level, interactive image query (IQ) language. The query language IQ permits the user to define false color functions, pixel value transformations, overlay functions, zoom functions, and windows. The user manipulates the images through generic functions. The user can direct images to display devices for visual and qualitative analysis. Image histograms and pixel value distributions can also be computed to obtain a quantitative analysis of images."
On the resolution and image intensity of the field-ion microscope,67.16903,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],High resolution and image intensity of field ion microscope at low temperatures
Guide to Magellan image interpretation,67.03359,image quality calculation,['LUNAR AND PLANETARY EXPLORATION'],"An overview of Magellan Mission requirements, radar system characteristics, and methods of data collection is followed by a description of the image data, mosaic formats, areal coverage, resolution, and pixel DN-to-dB conversion. The availability and sources of image data are outlined. Applications of the altimeter data to estimate relief, Fresnel reflectivity, and surface slope, and the radiometer data to derive microwave emissivity are summarized and illustrated in conjunction with corresponding SAR image data. Same-side and opposite-side stereo images provide examples of parallax differences from which to measure relief with a lateral resolution many times greater than that of the altimeter. Basic radar interactions with geologic surfaces are discussed with respect to radar-imaging geometry, surface roughness, backscatter modeling, and dielectric constant. Techniques are described for interpreting the geomorphology and surface properties of surficial features, impact craters, tectonically deformed terrain, and volcanic landforms. The morphologic characteristics that distinguish impact craters from volcanic craters are defined. Criteria for discriminating extensional and compressional origins of tectonic features are discussed. Volcanic edifices, constructs, and lava channels are readily identified from their radar outlines in images. Geologic map units are identified on the basis of surface texture, image brightness, pattern, and morphology. Superposition, cross-cutting relations, and areal distribution of the units serve to elucidate the geologic history."
Study of Efficient Transmission and Reception of Image-type Data Using Millimeter Waves Interim Engineering Report,66.913994,image quality calculation,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites
Systematic monitoring and evaluation of M7 scanner performance and data quality,66.661095,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"An investigation was conducted to provide the information required to maintain data quality of the Michigan M7 Multispectral scanner by systematic checks on specific system performance characteristics. Data processing techniques which use calibration data gathered routinely every mission have been developed to assess current data quality. Significant changes from past data quality are thus identified and attempts made to discover their causes. Procedures for systematic monitoring of scanner data quality are discussed. In the solar reflective region, calculations of Noise Equivalent Change in Radiance on a permission basis are compared to theoretical tape-recorder limits to provide an estimate of overall scanner performance. M7 signal/noise characteristics are examined."
Literature relevant to remote sensing of water quality,66.54647,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"References relevant to remote sensing of water quality were compiled, organized, and cross-referenced. The following general categories were included: (1) optical properties and measurement of water characteristics; (2) interpretation of water characteristics by remote sensing, including color, transparency, suspended or dissolved inorganic matter, biological materials, and temperature; (3) application of remote sensing for water quality monitoring; (4) application of remote sensing according to water body type; and (5) manipulation, processing and interpretation of remote sensing digital water data."
EO-1 Data Quality and Sensor Stability with Changing Orbital Precession at the End of a 16 Year Mission,66.46553,image quality calculation,['Earth Resources and Remote Sensing'],"The Earth Observing One (EO-1) satellite has completed 16 years of Earth observations in early 2017. What started as a technology mission to test various new advancements turned into a science and application mission that extended many years beyond the satellites planned life expectancy. EO-1s primary instruments are spectral imagers: Hyperion, the only civilian full spectrum spectrometer (430-2400 nm) in orbit; and the Advanced Land Imager (ALI), the prototype for Landsat-8s pushbroom imaging technology. Both Hyperion and ALI instruments have continued to perform well, but in February 2011 the satellite ran out of the fuel necessary to maintain orbit, which initiated a change in precession rate that led to increasingly earlier equatorial crossing times during its last five years. The change from EO-1s original orbit, when it was formation flying with Landsat-7 at a 10:01am equatorial overpass time, to earlier overpass times results in image acquisitions with increasing solar zenith angles (SZAs). In this study, we take several approaches to characterize data quality as SZAs increased. Our results show that for both EO-1 sensors, atmospherically corrected reflectance products are within 5 to 10 of mean pre-drift products. No marked trend in decreasing quality in ALI or Hyperion is apparent through 2016, and these data remain a high quality resource through the end of the mission."
"Denoising with Three Dimensional Fourier Transform for Three Dimensional Images, Including Image Sequences",66.23197,image quality calculation,['Instrumentation and Photography'],"A method of mitigating noise in source image data representing pixels of a 3-D image. The ""3-D image"" may be any type of 3-D image, regardless of whether the third dimension is spatial, temporal, or some other parameter. The 3-D image is divided into three-dimensional chunks of pixels. These chunks are apodized and a three-dimensional Fourier transform is performed on each chunk, thereby producing a three-dimensional spectrum of each chunk. The transformed chunks are processed to estimate a noise floor based on spectral values of the pixels within each chunk. A noise threshold is then determined, and the spectrum of each chunk is filtered with a denoising filter based on the noise threshold. The chunks are then inverse transformed, and recombined into a denoised 3-D image."
Digital image correlation techniques applied to LANDSAT multispectral imagery,66.0246,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Automatic image registration and resampling techniques applied to LANDSAT data achieved accuracies, resulting in mean radial displacement errors of less than 0.2 pixel. The process method utilized recursive computational techniques and line-by-line updating on the basis of feedback error signals. Goodness of local feature matching was evaluated through the implementation of a correlation algorithm. An automatic restart allowed the system to derive control point coordinates over a portion of the image and to restart the process, utilizing this new control point information as initial estimates."
Proceedings of the Second Annual Symposium on Mathematical Pattern Recognition and Image Analysis Program,65.92945,image quality calculation,['EARTH RESOURCES AND REMOTE SENSING'],"Several papers addressing image analysis and pattern recognition techniques for satellite imagery are presented. Texture classification, image rectification and registration, spatial parameter estimation, and surface fitting are discussed."
Quantitative evaluation of water quality in the coastal zone by remote sensing,65.67082,image quality calculation,['GEOPHYSICS'],"Remote sensing as a tool in a waste management program is discussed. By monitoring both the pollution sources and the environmental quality, the interaction between the components of the exturaine system was observed. The need for in situ sampling is reduced with the development of improved calibrated, multichannel sensors. Remote sensing is used for: (1) pollution source determination, (2) mapping the influence zone of the waste source on water quality parameters, and (3) estimating the magnitude of the water quality parameters. Diffusion coefficients and circulation patterns can also be determined by remote sensing, along with subtle changes in vegetative patterns and density."
Optical Signal Processing: Poisson Image Restoration and Shearing Interferometry,65.48303,image quality calculation,['INSTRUMENTATION AND PHOTOGRAPHY'],"Optical signal processing can be performed in either digital or analog systems. Digital computers and coherent optical systems are discussed as they are used in optical signal processing. Topics include: image restoration; phase-object visualization; image contrast reversal; optical computation; image multiplexing; and fabrication of spatial filters. Digital optical data processing deals with restoration of images degraded by signal-dependent noise. When the input data of an image restoration system are the numbers of photoelectrons received from various areas of a photosensitive surface, the data are Poisson distributed with mean values proportional to the illuminance of the incoherently radiating object and background light. Optical signal processing using coherent optical systems is also discussed. Following a brief review of the pertinent details of Ronchi's diffraction grating interferometer, moire effect, carrier-frequency photography, and achromatic holography, two new shearing interferometers based on them are presented. Both interferometers can produce variable shear."
A system to monitor stellar image quality,132.02164,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],Stellar image quality monitoring system
A conceptual study of automatic and semi-automatic quality assurance techniques for round image processing,121.03129,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"This report summarizes the results of a study conducted by Engineering and Economics Research (EER), Inc. under NASA Contract Number NAS5-27513. The study involved the development of preliminary concepts for automatic and semiautomatic quality assurance (QA) techniques for ground image processing. A distinction is made between quality assessment and the more comprehensive quality assurance which includes decision making and system feedback control in response to quality assessment."
Image processing system performance prediction and product quality evaluation,120.95711,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. A new technique for image processing system performance prediction and product quality evaluation was developed. It was entirely objective, quantitative, and general, and should prove useful in system design and quality control. The technique and its application to determination of quality control procedures for the Earth Resources Technology Satellite NASA Data Processing Facility are described."
Percentage cloud cover from Tiros photographs,116.577644,image quality percentage,['METEOROLOGY'],Percentage cloud cover from Tiros photographs analyzed by digital computer
A survey of quality measures for gray-scale image compression,115.3146,image quality percentage,['DOCUMENTATION AND INFORMATION SCIENCE'],"Although a variety of techniques are available today for gray-scale image compression, a complete evaluation of these techniques cannot be made as there is no single reliable objective criterion for measuring the error in compressed images. The traditional subjective criteria are burdensome, and usually inaccurate or inconsistent. On the other hand, being the most common objective criterion, the mean square error (MSE) does not have a good correlation with the viewer's response. It is now understood that in order to have a reliable quality measure, a representative model of the complex human visual system is required. In this paper, we survey and give a classification of the criteria for the evaluation of monochrome image quality."
Image Registration Workshop Proceedings,111.975525,image quality percentage,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Image processing developments and applications for water quality monitoring and trophic state determination,111.74132,image quality percentage,['ENVIRONMENT POLLUTION'],"Remote sensing data analysis of water quality monitoring is evaluated. Data anaysis and image processing techniques are applied to LANDSAT remote sensing data to produce an effective operational tool for lake water quality surveying and monitoring. Digital image processing and analysis techniques were designed, developed, tested, and applied to LANDSAT multispectral scanner (MSS) data and conventional surface acquired data. Utilization of these techniques facilitates the surveying and monitoring of large numbers of lakes in an operational manner. Supervised multispectral classification, when used in conjunction with surface acquired water quality indicators, is used to characterize water body trophic status. Unsupervised multispectral classification, when interpreted by lake scientists familiar with a specific water body, yields classifications of equal validity with supervised methods and in a more cost effective manner. Image data base technology is used to great advantage in characterizing other contributing effects to water quality. These effects include drainage basin configuration, terrain slope, soil, precipitation and land cover characteristics."
Software to model AXAF-I image quality,96.45913,image quality percentage,['COMPUTER PROGRAMMING AND SOFTWARE'],"A modular user-friendly computer program for the modeling of grazing-incidence type x-ray optical systems has been developed. This comprehensive computer software GRAZTRACE covers the manipulation of input data, ray tracing with reflectivity and surface deformation effects, convolution with x-ray source shape, and x-ray scattering. The program also includes the capabilities for image analysis, detector scan modeling, and graphical presentation of the results. A number of utilities have been developed to interface the predicted Advanced X-ray Astrophysics Facility-Imaging (AXAF-I) mirror structural and thermal distortions with the ray-trace. This software is written in FORTRAN 77 and runs on a SUN/SPARC station. An interactive command mode version and a batch mode version of the software have been developed."
Measuring Image Navigation and Registration Performance at the 3-Sigma Level Using Platinum Quality Landmarks,94.99114,image quality percentage,"['Spacecraft Design, Testing and Performance']","Geostationary Operational Environmental Satellite (GOES) Image Navigation and Registration (INR) performance is specified at the 3- level, meaning that 99.7% of a collection of individual measurements must comply with specification thresholds. Landmarks are measured by the Replacement Product Monitor (RPM), part of the operational GOES ground system, to assess INR performance and to close the INR loop. The RPM automatically discriminates between valid and invalid measurements enabling it to run without human supervision. In general, this screening is reliable, but a small population of invalid measurements will be falsely identified as valid. Even a small population of invalid measurements can create problems when assessing performance at the 3-sigma level. This paper describes an additional layer of quality control whereby landmarks of the highest quality (""platinum"") are identified by their self-consistency. The platinum screening criteria are not simple statistical outlier tests against sigma values in populations of INR errors. In-orbit INR performance metrics for GOES-12 and GOES-13 are presented using the platinum landmark methodology."
Analysis of the quality of image data acquired by the LANDSAT-4 thematic mapper and multispectral scanners,87.90041,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"A seven step procedure developed for evaluating the geometric properties of MSS and TM film produces is being implemented. Some 476 control points were selected of which 238 are being tested and edited for digitization and scaling errors. Tables show statistics established for assessing the spectral characteristics and variability, as well as the spatial resolution and radiometric sensitivity of TM data for a forest environment in an effort to determine the extent to which major forest cover type can be detected and identified on TM digital and image products. Results thus far show that the high quality obtained are more than sufficient for meeting most of the inventory objectives of the renewable resource specialist. The TM data should be extremely valuable for: (1) estimating forest cover types; (2) updating land use survey maps; and (3) determining the size and shape and location of individual forest clearings and water resources."
Image Sensor Technology,87.48786,image quality percentage,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites - image sensor systems
Television image compression and small animal remote monitoring,85.34561,image quality percentage,['COMMUNICATIONS AND RADAR'],"It was shown that a subject can reliably discriminate a difference in video image quality (using a specific commercial product) for image compression levels ranging from 384 kbits per second to 1536 kbits per second. However, their discriminations are significantly influenced by whether or not the TV camera is stable or moving and whether or not the animals are quiescent or active, which is correlated with illumination level (daylight versus night illumination, respectively). The highest video rate used here was 1.54 megabits per second, which is about 18 percent of the so-called normal TV resolution of 8.4MHz. Since this video rate was judged to be acceptable by 27 of the 34 subjects (79 percent), for monitoring the general health and status of small animals within their illuminated (lights on) cages (regardless of whether the camera was stable or moved), it suggests that an immediate Space Station Freedom to ground bandwidth reduction of about 80 percent can be tolerated without a significant loss in general monitoring capability. Another general conclusion is that the present methodology appears to be effective in quantifying visual judgments of video image quality."
Detection of Obstacles in Monocular Image Sequences,83.40061,image quality percentage,['Optics'],"The ability to detect and locate runways/taxiways and obstacles in images captured using on-board sensors is an essential first step in the automation of low-altitude flight, landing, takeoff, and taxiing phase of aircraft navigation. Automation of these functions under different weather and lighting situations, can be facilitated by using sensors of different modalities. An aircraft-based Synthetic Vision System (SVS), with sensors of different modalities mounted on-board, complements the current ground-based systems in functions such as detection and prevention of potential runway collisions, airport surface navigation, and landing and takeoff in all weather conditions. In this report, we address the problem of detection of objects in monocular image sequences obtained from two types of sensors, a Passive Millimeter Wave (PMMW) sensor and a video camera mounted on-board a landing aircraft. Since the sensors differ in their spatial resolution, and the quality of the images obtained using these sensors is not the same, different approaches are used for detecting obstacles depending on the sensor type. These approaches are described separately in two parts of this report. The goal of the first part of the report is to develop a method for detecting runways/taxiways and objects on the runway in a sequence of images obtained from a moving PMMW sensor. Since the sensor resolution is low and the image quality is very poor, we propose a model-based approach for detecting runways/taxiways. We use the approximate runway model and the position information of the camera provided by the Global Positioning System (GPS) to define regions of interest in the image plane to search for the image features corresponding to the runway markers. Once the runway region is identified, we use histogram-based thresholding to detect obstacles on the runway and regions outside the runway. This algorithm is tested using image sequences simulated from a single real PMMW image."
"Proceedings of the Image Intensifier Symposium, October 24-26, 1961, Fort Belvoir, VA",82.90995,image quality percentage,['NAVIGATION'],Proceedings of second image intensifier symposium
NASA total quality management 1989 accomplishments report,82.884865,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"Described here are the accomplishments of NASA as a result of the use of Total Quality Management (TQM). The principles in practice which led to these process refinements are important cultural elements to any organization's productivity and quality efforts. The categories of TQM discussed here are top management leadership and support, strategic planning, focus on the customer, employee training and recognition, employee empowerment and teamwork, measurement and analysis, and quality assurance."
NASA total quality management 1989 accomplishments report,81.288765,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"NASA and contractor employees achieved many notable improvements in 1989. The highlights of those improvements, described in this seventh annual Accomplishments Report, demonstrate that the people who support NASA's activities are getting more involved in quality and continuous improvement efforts. Their gains solidly support NASA's and this Nation's goal to remain a leader in space exploration and in world-wide market competition, and, when communicated to others through avenues such as this report, foster improvement efforts across government and industry. The principles in practice which led to these process refinements are important cultural elements to any organization's productivity and quality efforts. The categories in this report reflect NASA principles set forth in the 1980's and are more commonly known today as Total Quality Management (TQM): top management leadership and support; strategic planning; focus on the customer; employee training and recognition; employee empowerment and teamwork; measurement and analysis; and quality assurance."
Context dependent prediction and category encoding for DPCM image compression,79.4756,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],"Efficient compression of image data requires the understanding of the noise characteristics of sensors as well as the redundancy expected in imagery. Herein, the techniques of Differential Pulse Code Modulation (DPCM) are reviewed and modified for information-preserving data compression. The modifications include: mapping from intensity to an equal variance space; context dependent one and two dimensional predictors; rationale for nonlinear DPCM encoding based upon an image quality model; context dependent variable length encoding of 2x2 data blocks; and feedback control for constant output rate systems. Examples are presented at compression rates between 1.3 and 2.8 bits per pixel. The need for larger block sizes, 2D context dependent predictors, and the hope for sub-bits-per-pixel compression which maintains spacial resolution (information preserving) are discussed."
The Airline Quality Rating 1999,79.37776,image quality percentage,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple criteria. This current report, Airline Quality Rating 1999, reflects an updated approach to calculating monthly Airline Quality Rating scores for 1998. AQR scores for the calendar year 1998 are based on 15 elements that focus on airline performance areas important to air travel consumers. The Airline Quality Rating is a summary of month-by-month quality ratings for the ten major U.S. airlines operating during 1998. Using the Airline Quality Rating system of weighted averages and monthly performance data in the areas of on-time arrivals, involuntary denied boardings, mishandled baggage, and a combination of 12 customer complaint categories, major airlines comparative performance for the calendar year 1998 is reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major airlines domestic operations for the 12 month period of 1998, and industry average results. Also, comparative Airline Quality Rating data for 1997, using the updated criteria, are included to provide a reference point regarding quality in the industry."
Quality Control of Wind Data from 50-MHz Doppler Radar Wind Profiler,79.37737,image quality percentage,"['Launch Vehicles and Launch Operations', 'Quality Assurance and Reliability']","Upper-level wind profiles obtained from a 50-MHz Doppler Radar Wind Profiler (DRWP) instrument at Kennedy Space Center are incorporated in space launch vehicle design and day-of-launch operations to assess wind effects on the vehicle during ascent. Automated and manual quality control (QC) techniques are implemented to remove spurious data in the upper-level wind profiles caused from atmospheric and non-atmospheric artifacts over the 2010-2012 period of record (POR). By adding the new quality controlled profiles with older profiles from 1997-2009, a robust database will be constructed of upper-level wind characteristics. Statistical analysis will determine the maximum, minimum, and 95th percentile of the wind components from the DRWP profiles over recent POR and compare against the older database. Additionally, this study identifies specific QC flags triggered during the QC process to understand how much data is retained and removed from the profiles."
The Eighth Annual NASA/Contractors Conference and 1991 National Symposium on Quality and Productivity: Extending the boundaries of total quality management,79.225685,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"The Eighth Annual NASA/Contractors Conference and 1991 National Symposium on Quality and Productivity provided a forum to exchange knowledge and experiences in these areas of continuous improvement. The more than 1,100 attendees from government, industry, academia, community groups, and the international arena had a chance to learn about methods, tools, and strategies for excellence and to discuss continuous improvement strategies, successes, and failures. This event, linked via satellite to concurrent conferences hosted by the NASA Goddard Space Flight Center in Greenbelt, Maryland, and Martin Marietta Astronautics Group in Denver, Colorado, also explored extending the boundaries of Total Quality Management to include partnerships for quality within communities and encouraged examination, evaluation, and change to incorporate the principles of continuous improvement."
Iterative edge- and wavelet-based image registration of AVHRR and GOES satellite imagery,78.3719,image quality percentage,['Computer Programming and Software'],"Most automatic registration methods are either correlation-based, feature-based, or a combination of both. Examples of features which can be utilized for automatic image registration are edges, regions, corners, or wavelet-extracted features. In this paper, we describe two proposed approaches, based on edge or edge-like features, which are very appropriate to highlight regions of interest such as coastlines. The two iterative methods utilize the Normalized Cross-Correlation of edge and wavelet features and are applied to such problems as image-to-map registration, landmarking, and channel-to-channel co-registration, utilizing test data, AVHRR data, as well as GOES image data."
Application of imaging and ultrasound to the quality grading of beef,78.30023,image quality percentage,['ACOUSTICS'],"The results of a study conducted to assist the Department of Agriculture in the task of considering innovative methods for the grading of carcass beef for human consumption is presented. The processing of photographic, television and ultrasound images of the longissimus dorsi muscle at the 12/13th rib cut was undertaken. The results showed that a correlation could be developed between the quality grade of the carcass as determined by a professional grader, and the fat to area ratio of the muscle as determined by image processing techniques. In addition, the use of ultrasound shows the potential for grading of an unsliced carcass or a live animal."
NASA total quality management 1990 accomplishments report,78.248985,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"NASA's efforts in Total Quality Management are based on continuous improvement and serve as a foundation for NASA's present and future endeavors. Given here are numerous examples of quality strategies that have proven effective and efficient in a time when cost reduction is critical. These accomplishment benefit our Agency and help to achieve our primary goal, keeping American in the forefront of the aerospace industry."
Software to model AXAF image quality,77.01463,image quality percentage,['COMPUTER PROGRAMMING AND SOFTWARE'],"This draft final report describes the work performed under this delivery order from May 1992 through June 1993. The purpose of this contract was to enhance and develop an integrated optical performance modeling software for complex x-ray optical systems such as AXAF. The GRAZTRACE program developed by the MSFC Optical Systems Branch for modeling VETA-I was used as the starting baseline program. The original program was a large single file program and, therefore, could not be modified very efficiently. The original source code has been reorganized, and a 'Make Utility' has been written to update the original program. The new version of the source code consists of 36 small source files to make it easier for the code developer to manage and modify the program. A user library has also been built and a 'Makelib' utility has been furnished to update the library. With the user library, the users can easily access the GRAZTRACE source files and build a custom library. A user manual for the new version of GRAZTRACE has been compiled. The plotting capability for the 3-D point spread functions and contour plots has been provided in the GRAZTRACE using the graphics package DISPLAY. The Graphics emulator over the network has been set up for programming the graphics routine. The point spread function and the contour plot routines have also been modified to display the plot centroid, and to allow the user to specify the plot range, and the viewing angle options. A Command Mode version of GRAZTRACE has also been developed. More than 60 commands have been implemented in a Code-V like format. The functions covered in this version include data manipulation, performance evaluation, and inquiry and setting of internal parameters. The user manual for these commands has been formatted as in Code-V, showing the command syntax, synopsis, and options. An interactive on-line help system for the command mode has also been accomplished to allow the user to find valid commands, command syntax, and command function. A translation program has been written to convert FEA output from structural analysis to GRAZTRACE surface deformation file (.dfm file). The program can accept standard output files and list files from COSMOS/M and NASTRAN finite analysis programs. Some interactive options are also provided, such as Cartesian or cylindrical coordinate transformation, coordinate shift and scale, and axial length change. A computerized database for technical documents relating to the AXAF project has been established. Over 5000 technical documents have been entered into the master database. A user can now rapidly retrieve the desired documents relating to the AXAF project. The summary of the work performed under this contract is shown."
Transform image coding  Final report,76.927155,image quality percentage,['COMPUTERS'],Digital image coding by mathematical transforms
Astronomical use of television-type image sensors,76.353745,image quality percentage,['SPACE SCIENCES'],Conference on using TV type image sensors in astronomical photometry
Solid state image sensor research,76.30372,image quality percentage,['ELECTRONICS'],Solid state image sensing devices developed for meteorological satellite applications
NASA Excellence Award for Quality and Productivity 1989 highlights. The 1989 recipient: Lockheed Engineering and Sciences Company,76.29654,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"The NASA Excellence Award for Productivity and Quality is the result of NASA's desire to encourage superior quality and the continuous improvement philosophy in the aerospace industry. It is awarded to NASA contractors, subcontractors, and suppliers who have demonstrated sustained excellence, customer orientation, and outstanding achievements in a total quality management (TQM) environment. The 'highlights' booklet is intended to transfer successful techniques demonstrated by the performance and quality of major NASA contractors."
SAO/NASA joint investigation of astronomical viewing quality at Mount Hopkins Observatory:  1969-1971,76.10393,image quality percentage,['SPACE SCIENCES'],"Quantitative measurements of the astronomical seeing conditions have been made with a stellar-image monitor system at the Mt. Hopkins Observatory in Arizona. The results of this joint SAO-NASA experiment indicate that for a 15-cm-diameter telescope, image motion is typically 1 arcsec or less and that intensity fluctuations due to scintillation have a coefficient of irradiance variance of less than 0.12 on the average. Correlations between seeing quality and local meteorological conditions were investigated. Local temperature fluctuations and temperature gradients were found to be indicators of image-motion conditions, while high-altitude-wind conditions were shown to be somewhat correlated with scintillation-spectrum bandwidth. The theoretical basis for the relationship of atmospheric turbulence to optical effects is discussed in some detail, along with a description of the equipment used in the experiment. General site-testing comments and applications of the seeing-test results are also included."
APQ-102 imaging radar digital image quality study,75.91153,image quality percentage,['COMMUNICATIONS AND RADAR'],A modified APQ-102 sidelooking radar collected synthetic aperture radar (SAR) data which was digitized and recorded on wideband magnetic tape. These tapes were then ground processed into computer compatible tapes (CCT's). The CCT's may then be processed into high resolution radar images by software on the CYBER computer.
"The eighth NASA total quality management accomplishments report, 1990",75.19658,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"The eighth annual accomplishments report provides numerous examples of quality strategies that have proven effective and efficient in a time when cost reduction is critical. NASA's continuous improvement efforts can provide insight for others to succeed in their own endeavors. The report covers: top management leadership and support, strategic planning, focus on the customer, employee training and recognition, employee empowerment and teamwork, measurement and analysis, and quality assurance."
Research relative to automated multisensor image registration,75.19194,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],The basic aproaches to image registration are surveyed. Three image models are presented as models of the subpixel problem. A variety of approaches to the analysis of subpixel analysis are presented using these models.
Particle Image Velocimetry Measurements to Evaluate the Effectiveness of Deck-Edge Columnar Vortex Generators on Aircraft Carriers,74.994446,image quality percentage,['Fluid Mechanics and Thermodynamics'],"Candidate passive flow control devices were chosen from a NASA flow visualization study to investigate their effectiveness at improving flow quality over a flat-top carrier model. Flow over the deck was analyzed using a particle image velocimeter and a 1/120th scaled carrier model in a low-speed wind tunnel. Baseline (no devices) flow quality was compared to flow quality from combinations of bow and deck-edge devices at both zero and 20 degrees yaw. Devices included plain flaps and spiral cross-section columnar vortex generators attached in various combinations to the front and sides of the deck. Centerline and cross plane measurements were made with velocity and average turbulence measurements reported. Results show that the bow/deck-edge flap and bow/deck-edge columnar vortex generator pairs reduce flight deck turbulence both at zero yaw and at 20 degrees yaw by a factor of approximately 20. Of the devices tested, the most effective bow-only device appears to be the plain flap."
STRIPE: Remote Driving Using Limited Image Data,74.87206,image quality percentage,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
Estimating the quality of pasturage in the municipality of Paragominas (PA) by means of automatic analysis of LANDSAT data,74.70154,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The use of LANDSAT data to evaluate pasture quality in the Amazon region is demonstrated. Pasture degradation in deforested areas of a traditional tropical forest cattle-raising region was estimated. Automatic analysis using interactive multispectral analysis (IMAGE-100) shows that 24% of the deforested areas were occupied by natural vegetation regrowth, 24% by exposed soil, 15% by degraded pastures, and 46% was suitable grazing land."
The UNO Aviation Monograph Series: The Airline Quality Rating 1998,74.225945,image quality percentage,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple factors important to consumers. Development history and calculation details for the AQR rating system are detailed in The Airline Quality Rating 1991 issued in April, 1991, by the National Institute for Aviation Research at Wichita State University. This current report, Airline Quality Rating 1998, contains monthly Airline Quality Rating scores for 1997. Additional copies are available by contacting Wichita State University or University of Nebraska at Omaha. The Airline Quality Rating 1998 is a summary of month-by-month quality ratings for the ten major U.S. airlines operating during 1997. Using the Airline Quality Rating system and monthly performance data for each airline for the calendar year of 1997, individual and comparative ratings are reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major airlines domestic operations for the 12 month period of 1997, and industry average results. Also, comparative Airline Quality Rating data for 1991 through 1996 are included to provide a longer term view of quality in the industry."
Monitoring water quality from LANDSAT,73.92112,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"Water quality monitoring possibilities from LANDSAT were demonstrated both for direct readings of reflectances from the water and indirect monitoring of changes in use of land surrounding Swift Creek Reservoir in a joint project with the Virginia State Water Control Board and NASA. Film products were shown to have insufficient resolution and all work was done by digitally processing computer compatible tapes. Land cover maps of the 18,000 hectare Swift Creek Reservoir watershed, prepared for two dates in 1974, are shown. A significant decrease in the pine cover was observed in a 740 hectare construction site within the watershed. A measure of the accuracy of classification was obtained by comparing the LANDSAT results with visual classification at five sites on a U-2 photograph. Such changes in land cover can alert personnel to watch for potential changes in water quality."
Image data processing system requirements study. Volume 1:  Analysis,73.6625,image quality percentage,['GEOPHYSICS'],"Digital image processing, image recorders, high-density digital data recorders, and data system element processing for use in an Earth Resources Survey image data processing system are studied. Loading to various ERS systems is also estimated by simulation."
The effect of lossy image compression on image classification,73.412415,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
ATM experiment S-056 image processing requirements definition,73.32868,image quality percentage,['COMPUTERS'],"A plan is presented for satisfying the image data processing needs of the S-056 Apollo Telescope Mount experiment. The report is based on information gathered from related technical publications, consultation with numerous image processing experts, and on the experience that was in working on related image processing tasks over a two-year period."
The UNO Aviation Monograph Series: The Airline Quality Rating 1997,73.31867,image quality percentage,['Air Transportation and Safety'],"The Airline Quality Rating (AQR) was developed and first announced in early 1991 as an objective method of comparing airline performance on combined multiple factors important to consumers. Development history and calculation details for the AQR rating system are detailed in The Airline Quality Rating 1991 issued in April, 1991, by the National Institute for Aviation Research at Wichita State University. This current report, Airline Rating 1997, contains monthly Airline Quality Rating scores for 1996. Additional copies are available by contacting Wichita State University or the University of Nebraska at Omaha. The Airline Quality Rating (AQR) 1997 is a summary of a month-by-month quality ratings for the nine major domestic U.S. airlines operating during 1996. Using the Airline Quality Rating system and monthly performance data for each airline for the calendar year of 1996, individual and comparative ratings are reported. This research monograph contains a brief summary of the AQR methodology, detailed data and charts that track comparative quality for major domestic airlines across the 12 month period of 1996, and industry average results. Also comparative Airline Quality Rating data for 1991 through 1995 are included to provide a longer term view of quality in the industry."
Foundations for Measuring Volume Rendering Quality,72.639015,image quality percentage,['Computer Programming and Software'],"The goal of this paper is to provide a foundation for objectively comparing volume rendered images. The key elements of the foundation are: (1) a rigorous specification of all the parameters that need to be specified to define the conditions under which a volume rendered image is generated; (2) a methodology for difference classification, including a suite of functions or metrics to quantify and classify the difference between two volume rendered images that will support an analysis of the relative importance of particular differences. The results of this method can be used to study the changes caused by modifying particular parameter values, to compare and quantify changes between images of similar data sets rendered in the same way, and even to detect errors in the design, implementation or modification of a volume rendering system. If one has a benchmark image, for example one created by a high accuracy volume rendering system, the method can be used to evaluate the accuracy of a given image."
Proceedings of the Conference on Parallel Image Processing for Earth Observation Systems,72.589455,image quality percentage,"['PHYSICS, GENERAL']",Conference on parallel image processing for earth observation systems
Downscaling a Global Climate Model to Simulate Climate Change Impacts on U.S. Regional and Urban Air Quality,72.54288,image quality percentage,['Meteorology and Climatology'],"Climate change can exacerbate future regional air pollution events by making conditions more favorable to form high levels of ozone. In this study, we use spectral nudging with WRF to downscale NASA earth system GISS modelE2 results during the years 2006 to 2010 and 2048 to 2052 over the continental United States in order to compare the resulting meteorological fields from the air quality perspective during the four seasons of five-year historic and future climatological periods. GISS results are used as initial and boundary conditions by the WRF RCM to produce hourly meteorological fields. The downscaling technique and choice of physics parameterizations used are evaluated by comparing them with in situ observations. This study investigates changes of similar regional climate conditions down to a 12km by 12km resolution, as well as the effect of evolving climate conditions on the air quality at major U.S. cities. The high resolution simulations produce somewhat different results than the coarse resolution simulations in some regions. Also, through the analysis of the meteorological variables that most strongly influence air quality, we find consistent changes in regional climate that would enhance ozone levels in four regions of the U.S. during fall (Western U.S., Texas, Northeastern, and Southeastern U.S), one region during summer (Texas), and one region where changes potentially would lead to better air quality during spring (Northeast). We also find that daily peak temperatures tend to increase in most major cities in the U.S. which would increase the risk of health problems associated with heat stress. Future work will address a more comprehensive assessment of emissions and chemistry involved in the formation and removal of air pollutants."
"Research achievements review, volume 4.  Quality and reliability assurance research at MSFC",72.49254,image quality percentage,['MACHINE ELEMENTS AND PROCESSES'],Advanced methods of reliability engineering and quality assurance for aerospace application
"Design and development of solid state image converter for space vehicles  final report, 1 jun. 1963 - 30 apr. 1965",72.257515,image quality percentage,['ELECTRONIC EQUIPMENT'],Solid state image converter for space vehicles
Automatic Image Registration of Multimodal Remotely Sensed Data with Global Shearlet Features,71.289055,image quality percentage,"['Earth Resources and Remote Sensing', 'Instrumentation and Photography']","Automatic image registration is the process of aligning two or more images of approximately the same scene with minimal human assistance. Wavelet-based automatic registration methods are standard, but sometimes are not robust to the choice of initial conditions. That is, if the images to be registered are too far apart relative to the initial guess of the algorithm, the registration algorithm does not converge or has poor accuracy, and is thus not robust. These problems occur because wavelet techniques primarily identify isotropic textural features and are less effective at identifying linear and curvilinear edge features. We integrate the recently developed mathematical construction of shearlets, which is more effective at identifying sparse anisotropic edges, with an existing automatic wavelet-based registration algorithm. Our shearlet features algorithm produces more distinct features than wavelet features algorithms; the separation of edges from textures is even stronger than with wavelets. Our algorithm computes shearlet and wavelet features for the images to be registered, then performs least squares minimization on these features to compute a registration transformation. Our algorithm is two-staged and multiresolution in nature. First, a cascade of shearlet features is used to provide a robust, though approximate, registration. This is then refined by registering with a cascade of wavelet features. Experiments across a variety of image classes show an improved robustness to initial conditions, when compared to wavelet features alone."
Purkinje image eyetracking:  A market survey,71.273544,image quality percentage,['AEROSPACE MEDICINE'],"The Purkinje image eyetracking system was analyzed to determine the marketability of the system. The eyetracking system is a synthesis of two separate instruments, the optometer that measures the refractive power of the eye and the dual Purkinje image eyetracker that measures the direction of the visual axis."
Retinex Preprocessing for Improved Multi-Spectral Image Classification,71.093765,image quality percentage,['Computer Programming and Software'],"The goal of multi-image classification is to identify and label ""similar regions"" within a scene. The ability to correctly classify a remotely sensed multi-image of a scene is affected by the ability of the classification process to adequately compensate for the effects of atmospheric variations and sensor anomalies. Better classification may be obtained if the multi-image is preprocessed before classification, so as to reduce the adverse effects of image formation. In this paper, we discuss the overall impact on multi-spectral image classification when the retinex image enhancement algorithm is used to preprocess multi-spectral images. The retinex is a multi-purpose image enhancement algorithm that performs dynamic range compression, reduces the dependence on lighting conditions, and generally enhances apparent spatial resolution. The retinex has been successfully applied to the enhancement of many different types of grayscale and color images. We show in this paper that retinex preprocessing improves the spatial structure of multi-spectral images and thus provides better within-class variations than would otherwise be obtained without the preprocessing. For a series of multi-spectral images obtained with diffuse and direct lighting, we show that without retinex preprocessing the class spectral signatures vary substantially with the lighting conditions. Whereas multi-dimensional clustering without preprocessing produced one-class homogeneous regions, the classification on the preprocessed images produced multi-class non-homogeneous regions. This lack of homogeneity is explained by the interaction between different agronomic treatments applied to the regions: the preprocessed images are closer to ground truth. The principle advantage that the retinex offers is that for different lighting conditions classifications derived from the retinex preprocessed images look remarkably ""similar"", and thus more consistent, whereas classifications derived from the original images, without preprocessing, are much less similar."
Single Transducer Ultrasonic Imaging Method that Eliminates the Effect of Plate Thickness Variation in the Image,70.96433,image quality percentage,['Quality Assurance and Reliability'],"This article describes a single transducer ultrasonic imaging method that eliminates the effect of plate thickness variation in the image. The method thus isolates ultrasonic variations due to material microstructure. The use of this method can result in significant cost savings because the ultrasonic image can be interpreted correctly without the need for machining to achieve precise thickness uniformity during nondestructive evaluations of material development. The method is based on measurement of ultrasonic velocity. Images obtained using the thickness-independent methodology are compared with conventional velocity and c-scan echo peak amplitude images for monolithic ceramic (silicon nitride), metal matrix composite and polymer matrix composite materials. It was found that the thickness-independent ultrasonic images reveal and quantify correctly areas of global microstructural (pore and fiber volume fraction) variation due to the elimination of thickness effects. The thickness-independent ultrasonic imaging method described in this article is currently being commercialized under a cooperative agreement between NASA Lewis Research Center and Sonix, Inc."
Quality Assessment of Landsat Surface Reflectance Products Using MODIS Data,70.9425,image quality percentage,['Earth Resources and Remote Sensing'],"Surface reflectance adjusted for atmospheric effects is a primary input for land cover change detection and for developing many higher level surface geophysical parameters. With the development of automated atmospheric correction algorithms, it is now feasible to produce large quantities of surface reflectance products using Landsat images. Validation of these products requires in situ measurements, which either do not exist or are difficult to obtain for most Landsat images. The surface reflectance products derived using data acquired by the Moderate Resolution Imaging Spectroradiometer (MODIS), however, have been validated more comprehensively. Because the MODIS on the Terra platform and the Landsat 7 are only half an hour apart following the same orbit, and each of the 6 Landsat spectral bands overlaps with a MODIS band, good agreements between MODIS and Landsat surface reflectance values can be considered indicators of the reliability of the Landsat products, while disagreements may suggest potential quality problems that need to be further investigated. Here we develop a system called Landsat-MODIS Consistency Checking System (LMCCS). This system automatically matches Landsat data with MODIS observations acquired on the same date over the same locations and uses them to calculate a set of agreement metrics. To maximize its portability, Java and open-source libraries were used in developing this system, and object-oriented programming (OOP) principles were followed to make it more flexible for future expansion. As a highly automated system designed to run as a stand-alone package or as a component of other Landsat data processing systems, this system can be used to assess the quality of essentially every Landsat surface reflectance image where spatially and temporally matching MODIS data are available. The effectiveness of this system was demonstrated using it to assess preliminary surface reflectance products derived using the Global Land Survey (GLS) Landsat images for the 2000 epoch. As surface reflectance likely will be a standard product for future Landsat missions, the approach developed in this study can be adapted as an operational quality assessment system for those missions."
Gray-level transformations for interactive image enhancement,70.91491,image quality percentage,['CYBERNETICS'],A gray-level transformation method suitable for interactive image enhancement was presented. It is shown that the well-known histogram equalization approach is a special case of this method. A technique for improving the uniformity of a histogram is also developed. Experimental results which illustrate the capabilities of both algorithms are described. Two proposals for implementing gray-level transformations in a real-time interactive image enhancement system are also presented.
Analyses of requirements for computer control and data processing experiment subsystems.  Volume 1: ATM experiment S-056 image data processing system techniques development,70.780235,image quality percentage,['COMPUTERS'],"The solar imaging X-ray telescope experiment (designated the S-056 experiment) is described. It will photograph the sun in the far ultraviolet or soft X-ray region. Because of the imaging characteristics of this telescope and the necessity of using special techniques for capturing images on film at these wave lengths, methods were developed for computer processing of the photographs. The problems of image restoration were addressed to develop and test digital computer techniques for applying a deconvolution process to restore overall S-056 image quality. Additional techniques for reducing or eliminating the effects of noise and nonlinearity in S-056 photographs were developed."
Optical Signal Processing: Poisson Image Restoration and Shearing Interferometry,70.25316,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],"Optical signal processing can be performed in either digital or analog systems. Digital computers and coherent optical systems are discussed as they are used in optical signal processing. Topics include: image restoration; phase-object visualization; image contrast reversal; optical computation; image multiplexing; and fabrication of spatial filters. Digital optical data processing deals with restoration of images degraded by signal-dependent noise. When the input data of an image restoration system are the numbers of photoelectrons received from various areas of a photosensitive surface, the data are Poisson distributed with mean values proportional to the illuminance of the incoherently radiating object and background light. Optical signal processing using coherent optical systems is also discussed. Following a brief review of the pertinent details of Ronchi's diffraction grating interferometer, moire effect, carrier-frequency photography, and achromatic holography, two new shearing interferometers based on them are presented. Both interferometers can produce variable shear."
Image 100 procedures manual development: Applications system library definition and Image 100 software definition,70.177666,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],An outline for an Image 100 procedures manual for Earth Resources Program image analysis was developed which sets forth guidelines that provide a basis for the preparation and updating of an Image 100 Procedures Manual. The scope of the outline was limited to definition of general features of a procedures manual together with special features of an interactive system. Computer programs were identified which should be implemented as part of an applications oriented library for the system.
Ninth Annual NASA/Contractors Conference on Quality and Productivity. World Class Excellence: The Journey Continues. Conference presentations,70.174286,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],The topics covered include the following: The George M. Low Trophy; total quality assessment and measurement; using award criteria to improve organizational effectiveness; results--keeping an eye on the bottom line; capturing customer satisfaction; moving from management to leadership; leadership versus management; transforming the management team; leadership success stories; success stories in the quest for excellence; small business successes; education success stories; government success stories; tools and techniques for total quality management (TQM) integration; planning and organizing for TQM integration; successful stories for implementing system level TQM/CI tools; assessing TQM results; establishing an environment for continuous improvement at NASA; empowerment; synergism of partnering; and partnerships in education.
Visual simulation image generation using a flying-spot scanner,70.128944,image quality percentage,"['FACILITIES, RESEARCH, AND SUPPORT']",Visual simulation image generation using flying spot scanner
Systems and Methods for Automated Composite Layup Quality Assurance,70.07004,image quality percentage,['Composite Materials'],"Systems, methods, apparatus, and articles of manufacture for automated composite layup quality assurance."
Real-time optical image processing techniques,69.897354,image quality percentage,['OPTICS'],"Nonlinear real-time optical processing on spatial pulse frequency modulation has been pursued through the analysis, design, and fabrication of pulse frequency modulated halftone screens and the modification of micro-channel spatial light modulators (MSLMs). Micro-channel spatial light modulators are modified via the Fabry-Perot method to achieve the high gamma operation required for non-linear operation. Real-time nonlinear processing was performed using the halftone screen and MSLM. The experiments showed the effectiveness of the thresholding and also showed the needs of higher SBP for image processing. The Hughes LCLV has been characterized and found to yield high gamma (about 1.7) when operated in low frequency and low bias mode. Cascading of two LCLVs should also provide enough gamma for nonlinear processing. In this case, the SBP of the LCLV is sufficient but the uniformity of the LCLV needs improvement. These include image correlation, computer generation of holograms, pseudo-color image encoding for image enhancement, and associative-retrieval in neural processing. The discovery of the only known optical method for dynamic range compression of an input image in real-time by using GaAs photorefractive crystals is reported. Finally, a new architecture for non-linear multiple sensory, neural processing has been suggested."
SLAR image interpretation keys for geographic analysis,69.48829,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"A means for side-looking airborne radar (SLAR) imagery to become a more widely used data source in geoscience and agriculture is suggested by providing interpretation keys as an easily implemented interpretation model. Interpretation problems faced by the researcher wishing to employ SLAR are specifically described, and the use of various types of image interpretation keys to overcome these problems is suggested. With examples drawn from agriculture and vegetation mapping, direct and associate dichotomous image interpretation keys are discussed and methods of constructing keys are outlined. Initial testing of the keys, key-based automated decision rules, and the role of the keys in an information system for agriculture are developed."
Image processing system to analyze droplet distributions in sprays,69.14941,image quality percentage,['FLUID MECHANICS AND HEAT TRANSFER'],An image processing system was developed which automatically analyzes the size distributions in fuel spray video images. Images are generated by using pulsed laser light to freeze droplet motion in the spray sample volume under study. This coherent illumination source produces images which contain droplet diffraction patterns representing the droplets degree of focus. The analysis is performed by extracting feature data describing droplet diffraction patterns in the images. This allows the system to select droplets from image anomalies and measure only those droplets considered in focus. Unique features of the system are the totally automated analysis and droplet feature measurement from the grayscale image. The feature extraction and image restoration algorithms used in the system are described. Preliminary performance data is also given for two experiments. One experiment gives a comparison between a synthesized distribution measured manually and automatically. The second experiment compares a real spray distribution measured using current methods against the automatic system.
The Quality Control Algorithms Used in the Process of Creating the NASA Kennedy Space Center Lightning Protection System Towers Meteorological Database,68.92581,image quality percentage,"['Launch Vehicles and Launch Operations', 'Meteorology and Climatology', 'Quality Assurance and Reliability']","The methodology and the results of the quality control (QC) process of the meteorological data from the Lightning Protection System (LPS) towers located at Kennedy Space Center (KSC) launch complex 39B (LC-39B) are documented in this paper. Meteorological data are used to design a launch vehicle, determine operational constraints, and to apply defined constraints on day-of-launch (DOL). In order to properly accomplish these tasks, a representative climatological database of meteorological records is needed because the database needs to represent the climate the vehicle will encounter. Numerous meteorological measurement towers exist at KSC; however, the engineering tasks need measurements at specific heights, some of which can only be provided by a few towers. Other than the LPS towers, Tower 313 is the only tower that provides observations up to 150 m. This tower is located approximately 3.5 km from LC-39B. In addition, data need to be QC'ed to remove erroneous reports that could pollute the results of an engineering analysis, mislead the development of operational constraints, or provide a false image of the atmosphere at the tower's location."
A synoptic description of coal basins via image processing,68.661476,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"An existing image processing system is adapted to describe the geologic attributes of a regional coal basin. This scheme handles a map as if it were a matrix, in contrast to more conventional approaches which represent map information in terms of linked polygons. The utility of the image processing approach is demonstrated by a multiattribute analysis of the Herrin No. 6 coal seam in Illinois. Findings include the location of a resource and estimation of tonnage corresponding to constraints on seam thickness, overburden, and Btu value, which are illustrative of the need for new mining technology."
Analysis and design of a refractive virtual image system,68.28532,image quality percentage,['OPTICS'],"The optical performance of a virtual image display system is evaluated. Observation of a two-element (unachromatized doublet) refractive system led to the conclusion that the major source of image degradation was lateral chromatic aberration. This conclusion was verified by computer analysis of the system. The lateral chromatic aberration is given in terms of the resolution of the phosphor dots on a standard shadow mask color cathode ray tube. Single wavelength considerations include:  astigmatism, apparent image distance from the observer, binocular disparities and differences of angular magnification of the images presented to each of the observer's eyes. Where practical, these results are related to the performance of the human eye. All these techniques are applied to the previously mentioned doublet and a triplet refractive system. The triplet provides a 50-percent reduction in lateral chromatic aberration which was the design goal. Distortion was also reduced to a minimum over the field of view. The methods used in the design of the triplet are presented along with a method of relating classical aberration curves to image distance and binocular disparity."
CRT image recording evaluation,68.14996,image quality percentage,['ELECTRONIC EQUIPMENT'],"Performance capabilities and limitations of a fiber optic coupled line scan CRT image recording system were investigated. The test program evaluated the following components: (1). P31 phosphor CRT with EMA faceplate; (2). P31 phosphor CRT with clear clad faceplate; (3). Type 7743 semi-gloss dry process positive print paper; (4). Type 777 flat finish dry process positive print paper; (5). Type 7842 dry process positive film; and (6). Type 1971 semi-gloss wet process positive print paper. Detailed test procedures used in each test are provided along with a description of each test, the test data, and an analysis of the results."
EO-1 Data Quality and Sensor Stability with Changing Orbital Precession at the End of a 16 Year Mission,67.7109,image quality percentage,['Earth Resources and Remote Sensing'],"The Earth Observing One (EO-1) satellite has completed 16 years of Earth observations in early 2017. What started as a technology mission to test various new advancements turned into a science and application mission that extended many years beyond the satellites planned life expectancy. EO-1s primary instruments are spectral imagers: Hyperion, the only civilian full spectrum spectrometer (430-2400 nm) in orbit; and the Advanced Land Imager (ALI), the prototype for Landsat-8s pushbroom imaging technology. Both Hyperion and ALI instruments have continued to perform well, but in February 2011 the satellite ran out of the fuel necessary to maintain orbit, which initiated a change in precession rate that led to increasingly earlier equatorial crossing times during its last five years. The change from EO-1s original orbit, when it was formation flying with Landsat-7 at a 10:01am equatorial overpass time, to earlier overpass times results in image acquisitions with increasing solar zenith angles (SZAs). In this study, we take several approaches to characterize data quality as SZAs increased. Our results show that for both EO-1 sensors, atmospherically corrected reflectance products are within 5 to 10 of mean pre-drift products. No marked trend in decreasing quality in ALI or Hyperion is apparent through 2016, and these data remain a high quality resource through the end of the mission."
Quality-Controlled Wind Data from the Kennedy Space Center 915 Megahertz Doppler Radar Wind Profiler Network,66.80847,image quality percentage,['Meteorology and Climatology'],"The National Aeronautics and Space Administration s (NASA) Kennedy Space Center (KSC) has installed a five-instrument 915-Megahertz (MHz) Doppler Radar Wind Profiler (DRWP) system that records atmospheric wind profile properties. The purpose of these profilers is to fill data gaps between the top of the KSC wind tower network and the lowest measurement altitude of the KSC 50-MHz DRWP. The 915-MHz DRWP system has the capability to generate three-dimensional wind data outputs from approximately 150 meters (m) to 6,000 m at roughly 15-minute (min) intervals. NASA s long-term objective is to combine the 915-MHz and 50-MHz DRWP systems to create complete vertical wind profiles up to 18,300 m to be used in trajectory and loads analyses of space vehicles and by forecasters on day-of-launch (DOL). This analysis utilizes automated and manual quality control (QC) processes to remove erroneous and unrealistic wind data returned by the 915-MHz DRWP system. The percentage of data affected by each individual QC check in the period of record (POR) (i.e., January to April 2006) was computed, demonstrating the variability in the amount of data affected by the QC processes. The number of complete wind profiles available at given altitude thresholds for each profiler in the POR was calculated and outputted graphically, followed by an assessment of the number of complete wind profiles available for any profiler in the POR. A case study is also provided to demonstrate the QC process on a day of a known weather event."
A Study of Lessons and Experiences of NASA Centers in the Use of Commercial Off the Shelf (COTS) Electronics,66.65668,image quality percentage,"['Electronics and Electrical Engineering', 'Quality Assurance and Reliability']","The NASA Engineering and Safety Center (NESC) sponsored a Technical Assessment relating to the utilization of commercial-off-the-shelf (COTS) electrical, electronic, and electromechanical (EEE) parts in spaceflight systems at NASA Centers. The assessment had two primary goals. The first was to capture each NASA Centers current practices, best practices, lessons learned and Center-proposed recommendations on the use of COTS EEE parts and assemblies in critical ground support equipment (GSE). The second was to provide recommendations on the use of COTS, including a set of current best practices based on the Centers current and best practices and the NESC teams discussions. One key achievement made by the assessment team was the characterization of the term Industry Leading Parts Manufacturers (ILPMs) as parts manufacturer with high volume automatic production facilities and which can provide documented proof of the technology, process and product qualification, and its implementation of the best practices for zero defects for parts quality, reliability and workmanship. The assessment concluded with numerous findings, takeaways and recommendations that will be discussed during this presentation."
Uniform color space analysis of LACIE image products,66.55984,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Analysis and comparison of image products generated by different algorithms show that the scaling and biasing of data channels for control of PFC primaries lead to loss of information (in a probability-of misclassification sense) by two major processes. In order of importance they are: neglecting the input of one channel of data in any one image, and failing to provide sufficient color resolution of the data. The scaling and biasing approach tends to distort distance relationships in data space and provides less than desirable resolution when the data variation is typical of a developed, nonhazy agricultural scene."
Optimum constrained image restoration filters,66.34367,image quality percentage,['OPTICS'],"The filter was developed in Hilbert space by minimizing the radius of gyration of the overall or composite system point-spread function subject to constraints on the radius of gyration of the restoration filter point-spread function, the total noise power in the restored image, and the shape of the composite system frequency spectrum. An iterative technique is introduced which alters the shape of the optimum composite system point-spread function, producing a suboptimal restoration filter which suppresses undesirable secondary oscillations. Finally this technique is applied to multispectral scanner data obtained from the Earth Resources Technology Satellite to provide resolution enhancement. An experimental approach to the problems involving estimation of the effective scanner aperture and matching the ERTS data to available restoration functions is presented."
Proceedings of the Second Annual Symposium on Mathematical Pattern Recognition and Image Analysis Program,66.11318,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"Several papers addressing image analysis and pattern recognition techniques for satellite imagery are presented. Texture classification, image rectification and registration, spatial parameter estimation, and surface fitting are discussed."
Application of LANDSAT data and digital image processing,65.73534,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Based on LANDSAT 1 and 2 data, applications in the fields of coal mining, lignite exploration, and thematic mapping in geology are demonstrated. The hybrid image processing system, its software, and its utilization for educational purposes is described. A pre-operational European satellite is proposed."
Study of Efficient Transmission and Reception of Image-type Data Using Millimeter Waves Interim Engineering Report,65.57202,image quality percentage,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites
The effect of measurement errors and computational approximations on a perspective ILM radar image,65.562965,image quality percentage,['RESEARCH AND SUPPORT FACILITIES (AIR)'],"The effect was examined of aircraft position and attitude, of measurement errors, and of computational approximations on the size, shape, and position of a perspective radar image of an airport runway as might be displayed by an independent landing monitor in transport aircraft. The effect on runway image geometry was examined for different aircraft attitudes and different aircraft positions relative to a standard three degree glide slope. Measurement errors investigated were errors in radar azimuth angle and range, and errors in those aircraft parameters supplied to the radar for use in converting the radar image into a perspective format (namely pitch, roll, and altitude). Also investigated were the effects of using certain mathematical approximations, such as small angle, in the coordinate transformation which converts the image to a perspective format."
Image Detective 2.0: Engaging Citizen Scientists with NASA Astronaut Photography,65.50986,image quality percentage,['Instrumentation and Photography'],"Image Detective 2.0 engages citizen scientists with NASA astronaut photography of the Earth obtained by crew members on the International Space Station (ISS). Engaged citizen scientists are helping to build a more comprehensive and searchable database by geolocating this imagery and contributing to new imagery collections. Image Detective 2.0 is the newest addition to the suite of citizen scientist projects available through CosmoQuest, an effort led by the Astronomical Society of the Pacific (ASP) and supported through a NASA Science Mission Directorate Cooperative Agreement Notice award. CosmoQuest hosts a number of citizen science projects enabling individuals from around the world to engage in authentic NASA science. Image Detective 2.0, an effort that focuses on imagery acquired by astronauts on the International Space Station, builds on work initiated in 2012 by scientists and education specialists at the NASA Johnson Space Center. Through the many lessons learned, Image Detective 2.0 enhances the original project by offering new and improved options for participation. Existing users, as well as new Image Detective participants joining through the CosmoQuest platform, gain first-hand experience working with astronaut photography and become more engaged with this valuable data being obtained from the International Space Station. Citizens around the world are captivated by astronauts living and working in space. As crew members have a unique vantage point from which to view our Earth, the Crew Earth Observations (CEO) online database, referred to as the Gateway to Astronaut Photography of Earth (https://eol.jsc.nasa.gov/), provides a means for crew members to share their unique views of our home planet from the ISS with the scientific community and the public. Astronaut photography supports multiple uses including scientific investigations, visualizations, education, and outreach. These astronaut images record how the planet is changing over time, from human-made changes like urban growth and agriculture, to natural features and landforms such as tropical cyclones, aurora, coastlines, volcanoes and more. This imagery provides researchers on Earth with data to understand the planet from the perspective of the ISS, and is a useful complement to other remotely sensed datasets collected from robotic satellite platforms."
Spatial vision processes: From the optical image to the symbolic structures of contour information,65.19646,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],"The significance of machine and natural vision is discussed together with the need for a general approach to image acquisition and processing aimed at recognition. An exploratory scheme is proposed which encompasses the definition of spatial primitives, intrinsic image properties and sampling, 2-D edge detection at the smallest scale, the construction of spatial primitives from edges, and the isolation of contour information from textural information. Concepts drawn from or suggested by natural vision at both perceptual and physiological levels are relied upon heavily to guide the development of the overall scheme. The scheme is intended to provide a larger context in which to place the emerging technology of detector array focal-plane processors. The approach differs from many recent efforts in edge detection and image coding by emphasizing smallest scale edge detection as a foundation for multi-scale symbolic processing while diminishing somewhat the importance of image convolutions with multi-scale edge operators. Cursory treatments of information theory illustrate that the direct application of this theory to structural information in images could not be realized."
Onboard utilization of ground control points for image correction.  Volume 2:  Analysis and simulation results,65.13123,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"An approach to remote sensing that meets future mission requirements was investigated. The deterministic acquisition of data and the rapid correction of data for radiometric effects and image distortions are the most critical limitations of remote sensing. The following topics are discussed: onboard image correction systems, GCP navigation system simulation, GCP analysis, and image correction analysis measurement."
"Short wide angle, 1-1/2 inch electrostatic image dissector with parallel plate resistive strip electronic multiplier  Final report, 22 Feb. 1963 - 1 Nov. 1965",64.866356,image quality percentage,['ELECTRONIC EQUIPMENT'],Electron optical imaging system and combination anode-deflection cone for electrostatic image dissector tube - testing and calibration
ERTS image data compression technique evaluation,64.86265,image quality percentage,['COMPUTERS'],"The background and results of an investigation concerning the use of multispectral data compression in the ERTS program are presented. An average compression of greater than 2:1 has been achieved under the constraint of zero distortion in the reconstructed image, and four MSS tapes can be compressed to fill less than one reel of magnetic tape. A preliminary study of the hardware implementation of this processor proves the feasability of compression at input bit rates of over 100Mbs."
Proceedings of the NASA Symposium on Mathematical Pattern Recognition and Image Analysis,64.67049,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The application of mathematical and statistical analyses techniques to imagery obtained by remote sensors is described by Principal Investigators. Scene-to-map registration, geometric rectification, and image matching are among the pattern recognition aspects discussed."
Statistical separability and classification of land use classes using image-100,64.382286,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. The statistical separability of land use classes in the subsets of one to four spectral channels was investigated. Using ground observations and aerial photography, the MSS data of LANDSAT were analyzed with the Image-100. In the subsets of one to three spectral channels, channel 4, channel 4 & 7, and channels 4, 5, & 7 were found to be the best choices (ch.4 - 0.5 to 0.6 microns, ch. 5 - 0.6 to 0.7 microns, ch. 6 - 0.7 to 0.8 microns, and ch. 7 - 0.8 to 1.1 microns). For the single cell option of the Image-100, the errors of omission varied from 5% for the industrial class to 46% for the institutional class. The errors of commission varied from 11% for the commercial class to 39% for the industrial class. On the whole, the sample classifier gave considerably more accurate results compared to the single cell or multicell option."
Generation of housing quality data from multiband aerial photographs,64.356186,image quality percentage,['INSTRUMENTATION AND PHOTOGRAPHY'],Generation of housing quality data from aerial photographs
The Effect of Hole Quality on the Bearing Strength of Carbon Fiber Laminates,64.30224,image quality percentage,"['Composite Materials', 'Electronics and Electrical Engineering']","On programs involving flight hardware for launch vehicles that the author has been involved in over the years, the question always arises as to how to best machine and inspect holes drilled for fasteners. While common sense dictates that well-drilled holes are desired over poorly drilled holes for bolt bearing applications, the effect of hole quality on the bearing strength of carbon fiber laminates has not been extensively studied in the open literature. If this effect is not quantitatively known, the question of what hole quality needs to be required for the flight hardware cannot be answered. In addition, the function of the part with respect to the holes needs to be taken into account. There is no such thing as a defect-free hole since some chip-out of fibers within the hole will occur regardless of drill bit and backing plate pressure. For most holes machined in a carbon fiber laminate, some extent of delamination and fiber breakout on the entrance and exit of the hole is going to occur. Much time and money can be spent on trying to perfect the drilling technique and subsequent inspection of holes in carbon fiber laminates, including adding extra plies of cloth to the surfaces to help prevent fiber breakout; but is this effort really justified if little-to-no increase in bearing strength is realized by good-quality holes over those of lesser quality?"
"Design, implementation and investigation of an image guide-based optical flip-flop array",64.275,image quality percentage,['OPTICS'],Presented is the design for an image guide-based optical flip-flop array created using a Hughes liquid crystal light valve and a flexible image guide in a feedback loop. This design is used to investigate the application of image guides as a communication mechanism in numerical optical computers. It is shown that image guides can be used successfully in this manner but mismatch match between the input and output fiber arrays is extremely limiting.
The Application of Remote Sensing Techniques to Urban Data Acquisition,64.25496,image quality percentage,['GENERAL'],"The application of remote sensing techniques useful in acquiring data concerning housing quality is discussed. Conclusions reached from the investigation were: (1) Use of individuals with a higher degree of training in photointerpretation should significantly increase the percentage of successful classifications. (2) Small area classification of urban housing quality can definitely be accomplished via high resolution aerial photography. Such surveys, at the levels of accuracy demonstrated, can be of major utility in quick look surveys. (3) Survey costs should be significantly reduced."
Proceedings of the Third Annual Symposium on Mathematical Pattern Recognition and Image Analysis,63.944878,image quality percentage,['NUMERICAL ANALYSIS'],Topics addressed include: multivariate spline method; normal mixture analysis applied to remote sensing; image data analysis; classifications in spatially correlated environments; probability density functions; graphical nonparametric methods; subpixel registration analysis; hypothesis integration in image understanding systems; rectification of satellite scanner imagery; spatial variation in remotely sensed images; smooth multidimensional interpolation; and optimal frequency domain textural edge detection filters.
Application of digital image processing techniques to astronomical imagery 1978,63.630714,image quality percentage,['ASTRONOMY'],"Techniques for using image processing in astronomy are identified and developed for the following: (1) geometric and radiometric decalibration of vidicon-acquired spectra, (2) automatic identification and segregation of stars from galaxies; and (3) display of multiband radio maps in compact and meaningful formats. Examples are presented of these techniques applied to a variety of objects."
Digital CODEC for real-time processing of broadcast quality video signals at 1.8 bits/pixel,63.547905,image quality percentage,['COMMUNICATIONS AND RADAR'],"Advances in very large scale integration and recent work in the field of bandwidth efficient digital modulation techniques have combined to make digital video processing technically feasible an potentially cost competitive for broadcast quality television transmission. A hardware implementation was developed for DPCM (differential pulse code midulation)-based digital television bandwidth compression algorithm which processes standard NTSC composite color television signals and produces broadcast quality video in real time at an average of 1.8 bits/pixel. The data compression algorithm and the hardware implementation of the codec are described, and performance results are provided."
Image motion stabilization requirements for dynamic visual tasks in space,63.392914,image quality percentage,['BIOSCIENCES'],Image motion stabilization for dynamic visual task by astronauts on future space missions
AVIRIS data quality for coniferous canopy chemistry,63.148537,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"An assessment of AVIRIS data quality for studying coniferous canopy chemistry was made. Seven flightlines of AVIRIS data were acquired over a transect of coniferous forest sites in central Oregon. Both geometric and radiometric properties of the data were examined including: pixel size, swath width, spectral position and signal-to-noise ratio. A flat-field correction was applied to AVIRIS data from a coniferous forest site. Future work with this data set will exclude data from spectrometers C and D due to low signal-to-noise ratios. Data from spectrometers A and B will be used to examine the relationship between the canopy chemical composition of the forest sites and AVIRIS spectral response."
Application of LANDSAT-2 data to the implementation and enforcement of the Pennsylvania Surface Mining Conservation and Reclamation Act,62.946617,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Evaluation of LANDSAT imagery indicates severe limitations in its utility for surface mine land studies. Image stripping resulting from unequal detector response on satellite degrades the image quality to the extent that images of scales larger than 1:125,000 are of limited value for manual interpretation. Computer processing of LANDSAT data to improve image quality is essential; the removal of scanline stripping and enhancement of mine land reflectance data combined with color composite printing permits useful photographic enlargements to approximately 1:60,000."
Quality Control of Wind Data from 50-MHz Doppler Radar Wind Profiler,62.705948,image quality percentage,"['Meteorology and Climatology', 'Spacecraft Instrumentation and Astrionics', 'Launch Vehicles and Launch Operations']","Upper-level wind profiles obtained from a 50-MHz Doppler Radar Wind Profiler (DRWP) instrument at Kennedy Space Center are incorporated in space launch vehicle design and day-of-launch operations to assess wind effects on the vehicle during ascent. Automated and manual quality control (QC) techniques are implemented to remove spurious data in the upper-level wind profiles caused from atmospheric and non-atmospheric artifacts over the 2010-2012 period of record (POR). By adding the new quality controlled profiles with older profiles from 1997-2009, a robust database will be constructed of upper-level wind characteristics. Statistical analysis will determine the maximum, minimum, and 95th percentile of the wind components from the DRWP profiles over recent POR and compare against the older database. Additionally, this study identifies specific QC flags triggered during the QC process to understand how much data is retained and removed from the profiles."
Anaysis of the quality of image data required by the LANDSAT-4 Thematic Mapper and Multispectral Scanner,62.691143,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The spatial, geometric, and radiometric qualities of LANDSAT 4 thematic mapper (TM) and multispectral scanner (MSS) data were evaluated by interpreting, through visual and computer means, film and digital products for selected agricultural and forest cover types in California. Multispectral analyses employing Bayesian maximum likelihood, discrete relaxation, and unsupervised clustering algorithms were used to compare the usefulness of TM and MSS data for discriminating individual cover types. Some of the significant results are as follows: (1) for maximizing the interpretability of agricultural and forest resources, TM color composites should contain spectral bands in the visible, near-reflectance infrared, and middle-reflectance infrared regions, namely TM 4 and TM % and must contain TM 4 in all cases even at the expense of excluding TM 5; (2) using enlarged TM film products, planimetric accuracy of mapped poins was within 91 meters (RMSE east) and 117 meters (RMSE north); (3) using TM digital products, planimetric accuracy of mapped points was within 12.0 meters (RMSE east) and 13.7 meters (RMSE north); and (4) applying a contextual classification algorithm to TM data provided classification accuracies competitive with Bayesian maximum likelihood."
Methodology for the Elimination of Reflection and System Vibration Effects in Particle Image Velocimetry Data Processing,62.53199,image quality percentage,['Acoustics'],"A methodology to eliminate model reflection and system vibration effects from post processed particle image velocimetry data is presented. Reflection and vibration lead to loss of data, and biased velocity calculations in PIV processing. A series of algorithms were developed to alleviate these problems. Reflections emanating from the model surface caused by the laser light sheet are removed from the PIV images by subtracting an image in which only the reflections are visible from all of the images within a data acquisition set. The result is a set of PIV images where only the seeded particles are apparent. Fiduciary marks painted on the surface of the test model were used as reference points in the images. By locating the centroids of these marks it was possible to shift all of the images to a common reference frame. This image alignment procedure as well as the subtraction of model reflection are performed in a first algorithm. Once the images have been shifted, they are compared with a background image that was recorded under no flow conditions. The second and third algorithms find the coordinates of fiduciary marks in the acquisition set images and the background image and calculate the displacement between these images. The final algorithm shifts all of the images so that fiduciary mark centroids lie in the same location as the background image centroids. This methodology effectively eliminated the effects of vibration so that unbiased data could be used for PIV processing. The PIV data used for this work was generated at the NASA Langley Research Center Quiet Flow Facility. The experiment entailed flow visualization near the flap side edge region of an airfoil model. Commercial PIV software was used for data acquisition and processing. In this paper, the experiment and the PIV acquisition of the data are described. The methodology used to develop the algorithms for reflection and system vibration removal is stated, and the implementation, testing and validation of these algorithms are presented."
Land use change detection with LANDSAT-2 data for monitoring and predicting regional water quality degradation,62.519993,image quality percentage,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Comparison between LANDSAT 1 and 2 imagery of Arkansas provided evidence of significant land use changes during the 1972-75 time period. Analysis of Arkansas historical water quality information has shown conclusively that whereas point source pollution generally can be detected by use of water quality data collected by state and federal agencies, sampling methodologies for nonpoint source contamination attributable to surface runoff are totally inadequate. The expensive undertaking of monitoring all nonpoint sources for numerous watersheds can be lessened by implementing LANDSAT change detection analyses."
The effect of variety and maturity on the quality of freeze-dried carrots.  The effect of microwave blanching on the nutritional and textural quality of freeze-dried spinach,62.43716,image quality percentage,['MAN/SYSTEM TECHNOLOGY AND LIFE SUPPORT'],"Using carrots, the quality of freeze-dried products was studied to determine the optimum varieties and maturation stages for quality attributes such as appearance, flavor, texture, and nutritive value. The quality of freeze-dried carrots is discussed in terms of Gardner color, alcohol insoluble solids, viscosity, and core/cortex ratio. Also, microwave blanching of freeze-dried spinach was studied to determine vitamin interrelationships, anatomical changes, and oxidative deteriorations in terms of preprocessing microwave treatments. Statistical methods were employed in the gathering of data and interpretation of results in both studies."
Measuring Total and Germinable Spore Populations,62.372307,image quality percentage,['Exobiology'],"It has been shown that bacterial endospores can be enumerated using a microscopy based assay that images the luminescent halos from terbium ions bound to dipicolinic acid, a spore specific chemical marker released upon spore germination. Further development of the instrument has simplified it towards automation while at the same time improving image quality. Enumeration of total spore populations has also been developed allowing measurement of the percentage of viable spores in any population by comparing the germinable/culturable spores to the total. Percentage viability will allow a more quantitative comparison of the ability of spores to survive across a wide range of extreme environments."
An evaluation of the total quality management implementation strategy for the advanced solid rocket motor project at NASA's Marshall Space Flight Center,61.998035,image quality percentage,['QUALITY ASSURANCE AND RELIABILITY'],"An evaluation of the NASA's Marshall Space Flight Center (MSFC) strategy to implement Total Quality Management (TQM) in the Advanced Solid Rocket Motor (ASRM) Project is presented. The evaluation of the implementation strategy reflected the Civil Service personnel perspective at the project level. The external and internal environments at MSFC were analyzed for their effects on the ASRM TQM strategy. Organizational forms, cultures, management systems, problem solving techniques, and training were assessed for their influence on the implementation strategy. The influence of ASRM's effort was assessed relative to its impact on mature projects as well as future projects at MSFC."
Thermal Optimization of Growth and Quality in Protein Crystals,61.917805,image quality percentage,['Solid-State Physics'],"Experimental evidence suggests that larger and higher quality crystals can be attained in the microgravity of space; however, the effect of growth rate on protein crystal quality is not well documented. This research is the first step towards providing strategies to grow crystals under constant rates of growth. Controlling growth rates at a constant value allows for direct one-to-one comparison of results obtained in microgravity and on earth. The overall goal of the project was to control supersaturation at a constant value during protein crystal growth by varying temperature in a predetermined manner. Applying appropriate theory requires knowledge of specific physicochemical properties of the protein solution including the effect of supersaturation on growth rates and the effect of temperature on protein solubility. Such measurements typically require gram quantities of protein and many months of data acquisition. A second goal of the project applied microcalorimetry for the rapid determination of these physicochemical properties using a minimum amount of protein. These two goals were successfully implemented on hen egg-white lysozyme. Results of these studies are described in the attached reprints."
"Study of hybrid microcircuit quality standards, inspection criteria, and reliability screening techniques  Final report",61.784668,image quality percentage,['ELECTRONICS'],"Hybrid microcircuit quality standards, inspection criteria, and reliability screening"
Functionalization of Single-Wall Carbon Nanotubes by Photo-Oxidation,103.038284,photo analysis invention,['Man/System Technology and Life Support'],"new technique for carbon nanotube oxidation was developed based upon the photo-oxidation of organic compounds. The resulting method is more benign than conventional oxidation approaches and produces single-wall carbon nanotubes (SWCNTs) with higher levels of oxidation. In this procedure, an oxygen saturated suspension of SWNTs in a suitable solvent containing a singlet oxygen sensitizer, such as Rose Bengal, is irradiated with ultraviolet light. The resulting oxidized tubes are recovered by filtering the suspension, followed by washing to remove any adsorbed solvent and sensitizer, and drying in a vacuum oven. Chemical analysis by FT-infrared and x-ray photoelectron spectroscopy revealed that the oxygen content of the photo-oxidized SWCNT was 11.3 atomic % compared to 6.7 atomic % for SWCNT that had been oxidized by standard treatment in refluxing acid. The photo-oxidized SWCNT produced by this method can be used directly in various polymer matrixes, or can be further modified by chemical reactions at the oxygen functional groups and then used as additives. This method may also be suitable for use in oxidation of multiwall carbon nanotubes and graphenes."
Portable Unit for Metabolic Analysis,92.24292,photo analysis invention,['Man/System Technology and Life Support'],"The Portable Unit for Metabolic Analysis measures human metabolic function. The compact invention attaches to the face of a subject and it is able to record highly time-resolved measurements of air temperature and pressure, flow rates during inhalation and exhalation, and oxygen and carbon dioxide partial pressure. The device is capable of `breath-by-breath` analysis and `within-breath` analysis at high temporal resolution."
Chemometric analysis for extraction of individual fluorescence spectrum and lifetimes from a target mixture,90.05892,photo analysis invention,['Atomic and Molecular Physics'],"The present invention is a system for chemometric analysis for the extraction of the individual component fluorescence spectra and fluorescence lifetimes from a target mixture. The present invention combines a processor with an apparatus for generating an excitation signal to transmit at a target mixture and an apparatus for detecting the emitted signal from the target mixture. The present invention extracts the individual fluorescence spectrum and fluorescence lifetime measurements from the frequency and wavelength data acquired from the emitted signal. The present invention uses an iterative solution that first requires the initialization of several decision variables and the initial approximation determinations of intermediate matrices. The iterative solution compares the decision variables for convergence to see if further approximation determinations are necessary. If the solution converges, the present invention then determines the reduced best fit error for the analysis of the individual fluorescence lifetime and the fluorescence spectrum before extracting the individual fluorescence lifetime and fluorescence spectrum from the emitted signal of the target mixture."
Signal generation and mixing electronics for frequency-domain lifetime and spectral fluorometry,74.96354,photo analysis invention,['Instrumentation and Photography'],"The present invention additionally comprises a method and apparatus for generating and mixing signals for frequency-domain lifetime and spectral fluorometry. The present invention comprises a plurality of signal generators that generate a plurality of signals where the signal generators modulate the amplitude and/or the frequency of the signals. The present invention uses one of these signals to drive an excitation signal that the present invention then directs and transmits at a target mixture, which absorbs the energy from the excitation signal. The property of fluorescence causes the target mixture to emit an emitted signal that the present invention detects with a signal detector. The present invention uses a plurality of mixers to produce a processor reference signal and a data signal. The present invention then uses a processor to compare the processor reference signal with the data signal by analyzing the differences in the phase and the differences in the amplitude between the two signals. The processor then extracts the fluorescence lifetime and fluorescence spectrum of the emitted signal from the phase and amplitude information using a chemometric analysis."
A Method of Poisson's Ration Imaging Within a Material Part,74.77286,photo analysis invention,['Numerical Analysis'],"The present invention is directed to a method of displaying the Poisson's ratio image of a material part. In the present invention, longitudinal data is produced using a longitudinal wave transducer and shear wave data is produced using a shear wave transducer. The respective data is then used to calculate the Poisson's ratio for the entire material part. The Poisson's ratio approximations are then used to display the data."
Method of Poisson's ratio imaging within a material part,74.64705,photo analysis invention,['Numerical Analysis'],The present invention is directed to a method of displaying the Poisson's ratio image of a material part. In the present invention longitudinal data is produced using a longitudinal wave transducer and shear wave data is produced using a shear wave transducer. The respective data is then used to calculate the Poisson's ratio for the entire material part. The Poisson's ratio approximations are then used to displayed the image.
Spectral and Temporal Laser Fluorescence Analysis Such as for Natural Aquatic Environments,72.21328,photo analysis invention,"['Lasers and Masers', 'Oceanography']","An Advanced Laser Fluorometer (ALF) can combine spectrally and temporally resolved measurements of laser-stimulated emission (LSE) for characterization of dissolved and particulate matter, including fluorescence constituents, in liquids. Spectral deconvolution (SDC) analysis of LSE spectral measurements can accurately retrieve information about individual fluorescent bands, such as can be attributed to chlorophyll-a (Chl-a), phycobiliprotein (PBP) pigments, or chromophoric dissolved organic matter (CDOM), among others. Improved physiological assessments of photosynthesizing organisms can use SDC analysis and temporal LSE measurements to assess variable fluorescence corrected for SDC-retrieved background fluorescence. Fluorescence assessments of Chl-a concentration based on LSE spectral measurements can be improved using photo-physiological information from temporal measurements. Quantitative assessments of PBP pigments, CDOM, and other fluorescent constituents, as well as basic structural characterizations of photosynthesizing populations, can be performed using SDC analysis of LSE spectral measurements."
Photoacoustic Chemical Detector,71.55197,photo analysis invention,['Instrumentation and Photography'],"A laser vibrometer for measurement of ambient chemical species includes a laser that produces a beam that is split into a reference readout beam and a signal readout beam. A probe laser beam is tuned to an absorption feature of a molecular transition, and generates acoustic signals when incident on a gaseous species via the photo acoustic effect. The scattered acoustic signals are incident on a thin membrane that vibrates. The readout laser beam reflected from the vibrating membrane is mixed with the reference beam at the surface of a photo-EMF detector. Interferrometric fringes are generated at the surface of the photo-EMF detector. Electric current is generated in the photo-EMF detector when the fringes are in motion due to undulations in the signal readout beam imparted by the vibrating membrane. A highly sensitive photo-EMF detector is capable of detecting picoJoules or less laser energy generated by vibrating processes."
Full Field Photoelastic Stress Analysis,69.908646,photo analysis invention,['Structural Mechanics'],"A structural specimen coated with or constructed of photoelastic material, when illuminated with circularly polarized light will, when stressed: reflect or transmit elliptically polarized light, the direction of the axes of the ellipse and variation of the elliptically light from illuminating circular light will correspond to and indicate the direction and magnitude of the shear stresses for each illuminated point on the specimen. The principles of this invention allow for several embodiments of stress analyzing apparatus, ranging from those involving multiple rotating optical elements, to those which require no moving parts at all. A simple polariscope may be constructed having two polarizing filters with a single one-quarter waveplate placed between the polarizing filters. Light is projected through the first polarizing filter and the one-quarter waveplate and is reflected from a sub-fringe birefringent coating on a structure under load. Reflected light from the structure is analyzed with a polarizing filter. The two polarizing filters and the one-quarter waveplate may be rotated together or the analyzer alone may be rotated. Computer analysis of the variation in light intensity yields shear stress magnitude and direction."
Photoreceptor System for Melatonin Regulation and Phototherapy,67.992134,photo analysis invention,['Instrumentation and Photography'],"The present invention involves a light system for stimulating or regulating neuroendocrine, circadian, and photoneural systems in mammals based upon the discovery of peak sensitivity ranging from 425-505 nm; a light meter system for quantifying light which stimulates or regulates mammalian circadian, photoneural, and neuroendocrine systems. The present invention also relates to translucent and transparent materials, and lamps or other light sources with or without filters capable of stimulating or regulating neuroendocrine, circadian, and photoneural systems in mammals. Additionally, the present invention involves treatment of mammals with a wide variety of disorders or deficits, including light responsive disorders, eating disorders, menstrual cycle disorders, non-specific alerting and performance deficits, hormone-sensitive cancers, and cardiovascular disorders."
Spectroscopic Chemical Analysis Methods and Apparatus,67.71617,photo analysis invention,['Man/System Technology and Life Support'],"This invention relates to non-contact spectroscopic methods and apparatus for performing chemical analysis and the ideal wavelengths and sources needed for this analysis. It employs deep ultraviolet (200- to 300-nm spectral range) electron-beam-pumped wide bandgap semiconductor lasers, incoherent wide bandgap semiconductor lightemitting devices, and hollow cathode metal ion lasers. Three achieved goals for this innovation are to reduce the size (under 20 L), reduce the weight [under 100 lb (.45 kg)], and reduce the power consumption (under 100 W). This method can be used in microscope or macroscope to provide measurement of Raman and/or native fluorescence emission spectra either by point-by-point measurement, or by global imaging of emissions within specific ultraviolet spectral bands. In other embodiments, the method can be used in analytical instruments such as capillary electrophoresis, capillary electro-chromatography, high-performance liquid chromatography, flow cytometry, and related instruments for detection and identification of unknown analytes using a combination of native fluorescence and/or Raman spectroscopic methods. This design provides an electron-beampumped semiconductor radiation-producing method, or source, that can emit at a wavelength (or wavelengths) below 300 nm, e.g. in the deep ultraviolet between about 200 and 300 nm, and more preferably less than 260 nm. In some variations, the method is to produce incoherent radiation, while in other implementations it produces laser radiation. In some variations, this object is achieved by using an AlGaN emission medium, while in other implementations a diamond emission medium may be used. This instrument irradiates a sample with deep UV radiation, and then uses an improved filter for separating wavelengths to be detected. This provides a multi-stage analysis of the sample. To avoid the difficulties related to producing deep UV semiconductor sources, a pumping approach has been developed that uses ballistic electron beam injection directly into the active region of a wide bandgap semiconductor material."
Atmospheric electron x-ray spectrometer,66.00829,photo analysis invention,['Instrumentation and Photography'],"The present invention comprises an apparatus for performing in-situ elemental analyses of surfaces. The invention comprises an atmospheric electron x-ray spectrometer with an electron column which generates, accelerates, and focuses electrons in a column which is isolated from ambient pressure by a:thin, electron transparent membrane. After passing through the membrane, the electrons impinge on the sample in atmosphere to generate characteristic x-rays. An x-ray detector, shaping amplifier, and multi-channel analyzer are used for x-ray detection and signal analysis. By comparing the resultant data to known x-ray spectral signatures, the elemental composition of the surface can be determined."
Ultraviolet and thermally stable polymer compositions,65.842445,photo analysis invention,['PROPELLANTS'],"A class of polymers is provided, namely, poly(diarylsiloxy) arylazines. These polymers have a basic chemical composition which has the property of stabilizing the optical and physical properties of the polymer against the degradative effect of ultraviolet light and high temperatures. This stabilization occurs at wavelengths including those shorter than found on the surface of the earth and in the absence or presence of oxygen, making the polymers of the present invention useful for high performance coating applications in extraterrestrial space as well as similar applications in terrestrial service. The invention also provides aromatic azines which are useful in the preparation of polymers such as those of the present invention."
DNA Photolithography with Cinnamate Crosslinkers,63.089176,photo analysis invention,['Life Sciences (General)'],"The present invention relates generally to cinnamate crosslinkers. Specifically, the present invention relates to gels, biochips, and functionalized surfaces useful as probes, in assays, in gels, and for drug delivery, and methods of making the same using a newly-discovered crosslinking configuration."
Compact near-IR and mid-IR cavity ring down spectroscopy device,62.86706,photo analysis invention,['Communications and Radar'],This invention relates to a compact cavity ring down spectrometer for detection and measurement of trace species in a sample gas using a tunable solid-state continuous-wave mid-infrared PPLN OPO laser or a tunable low-power solid-state continuous wave near-infrared diode laser with an algorithm for reducing the periodic noise in the voltage decay signal which subjects the data to cluster analysis or by averaging of the interquartile range of the data.
Method and system for near-field spectroscopy using targeted deposition of nanoparticles,62.83009,photo analysis invention,['Instrumentation and Photography'],"There is provided in one embodiment of the invention a method for analyzing a sample material using surface enhanced spectroscopy. The method comprises the steps of imaging the sample material with an atomic force microscope (AFM) to select an area of interest for analysis, depositing nanoparticles onto the area of interest with an AFM tip, illuminating the deposited nanoparticles with a spectrometer excitation beam, and disengaging the AFM tip and acquiring a localized surface enhanced spectrum. The method may further comprise the step of using the AFM tip to modulate the spectrometer excitation beam above the deposited nanoparticles to obtain improved sensitivity data and higher spatial resolution data from the sample material. The invention further comprises in one embodiment a system for analyzing a sample material using surface enhanced spectroscopy."
Oxygen sensitive paper,61.814037,photo analysis invention,['MATERIALS'],Paper is impregnated with mixture of methylene blue and ethylenediaminetetraacetic acid. Methylene blue is photo-reduced to leuco-form. Paper is kept isolated from oxygen until ready for use. Paper can be reused by photo-reduction after oxygen exposure.
Methods of Attaching or Grafting Carbon Nanotubes to Silicon Surfaces and Composite Structures Derived Therefrom,61.768467,photo analysis invention,"['Inorganic, Organic and Physical Chemistry']","The present invention is directed toward methods of attaching or grafting carbon nanotubes (CNTs) to silicon surfaces. In some embodiments, such attaching or grafting occurs via functional groups on either or both of the CNTs and silicon surface. In some embodiments, the methods of the present invention include: (1) reacting a silicon surface with a functionalizing agent (such as oligo(phenylene ethynylene)) to form a functionalized silicon surface; (2) dispersing a quantity of CNTs in a solvent to form dispersed CNTs; and (3) reacting the functionalized silicon surface with the dispersed CNTs. The present invention is also directed to the novel compositions produced by such methods."
Computed Tomography Imaging Spectrometer (CTIS) with 2D Reflective Grating for Ultraviolet to Long-Wave Infrared Detection Especially Useful for Surveying Transient Events,60.755318,photo analysis invention,['Instrumentation and Photography'],"The optical system of this invention is an unique type of imaging spectrometer, i.e. an instrument that can determine the spectra of all points in a two-dimensional scene. The general type of imaging spectrometer under which this invention falls has been termed a computed-tomography imaging spectrometer (CTIS). CTIS's have the ability to perform spectral imaging of scenes containing rapidly moving objects or evolving features, hereafter referred to as transient scenes. This invention, a reflective CTIS with an unique two-dimensional reflective grating, can operate in any wavelength band from the ultraviolet through long-wave infrared. Although this spectrometer is especially useful for events it is also for investigation of some slow moving phenomena as in the life sciences."
Computed tomography imaging spectrometer (CTIS) with 2D reflective grating for ultraviolet to long-wave infrared detection especially useful for surveying transient events,60.402718,photo analysis invention,['Instrumentation and Photography'],"The optical system of this invention is an unique type of imaging spectrometer, i.e. an instrument that can determine the spectra of all points in a two-dimensional scene. The general type of imaging spectrometer under which this invention falls has been termed a computed-tomography imaging spectrometer (CTIS). CTIS's have the ability to perform spectral imaging of scenes containing rapidly moving objects or evolving features, hereafter referred to as transient scenes. This invention, a reflective CTIS with an unique two-dimensional reflective grating, can operate in any wavelength band from the ultraviolet through long-wave infrared. Although this spectrometer is especially useful for rapidly occurring events it is also useful for investigation of some slow moving phenomena as in the life sciences."
High precision computing with charge domain devices and a pseudo-spectral method therefor,59.757748,photo analysis invention,['Instrumentation and Photography'],"The present invention enhances the bit resolution of a CCD/CID MVM processor by storing each bit of each matrix element as a separate CCD charge packet. The bits of each input vector are separately multiplied by each bit of each matrix element in massive parallelism and the resulting products are combined appropriately to synthesize the correct product. In another aspect of the invention, such arrays are employed in a pseudo-spectral method of the invention, in which partial differential equations are solved by expressing each derivative analytically as matrices, and the state function is updated at each computation cycle by multiplying it by the matrices. The matrices are treated as synaptic arrays of a neural network and the state function vector elements are treated as neurons. In a further aspect of the invention, moving target detection is performed by driving the soliton equation with a vector of detector outputs. The neural architecture consists of two synaptic arrays corresponding to the two differential terms of the soliton-equation and an adder connected to the output thereof and to the output of the detector array to drive the soliton equation."
"Miniature, low-power X-ray tube using a microchannel electron generator electron source",59.362724,photo analysis invention,['Instrumentation and Photography'],"Embodiments of the invention provide a novel, low-power X-ray tube and X-ray generating system. Embodiments of the invention use a multichannel electron generator as the electron source, thereby increasing reliability and decreasing power consumption of the X-ray tube. Unlike tubes using a conventional filament that must be heated by a current power source, embodiments of the invention require only a voltage power source, use very little current, and have no cooling requirements. The microchannel electron generator comprises one or more microchannel plates (MCPs), Each MCP comprises a honeycomb assembly of a plurality of annular components, which may be stacked to increase electron intensity. The multichannel electron generator used enables directional control of electron flow. In addition, the multichannel electron generator used is more robust than conventional filaments, making the resulting X-ray tube very shock and vibration resistant."
Interaction of microwaves with carbon nanotubes to facilitate modification,59.074436,photo analysis invention,['Composite Materials'],"The present invention is directed toward methods of crosslinking carbon nanotubes to each other using microwave radiation, articles of manufacture produced by such methods, compositions produced by such methods, and applications for such compositions and articles of manufacture. The present invention is also directed toward methods of radiatively modifying composites and/or blends comprising carbon nanotubes with microwaves, and to the compositions produced by such methods. In some embodiments, the modification comprises a crosslinking process, wherein the carbon nanotubes serve as a conduit for thermally and photolytically crosslinking the host matrix with microwave radiation."
Apparatus and Method for Elimination of Polarization-Induced Fading in Fiber-optic Sensor System,58.999535,photo analysis invention,['Instrumentation and Photography'],The invention is an apparatus and method of eliminating polarization-induced fading in interferometric fiber-optic sensor system having a wavelength-swept laser optical signal. The interferometric return signal from the sensor arms are combined and provided to a multi-optical path detector assembly and ultimately to a data acquisition and processing unit by way of a switch that is time synchronized with the laser scan sweep cycle.
Encapsulation system for the immunoisolation of living cells,58.470955,photo analysis invention,['Life Sciences (General)'],"The present invention is drawn to a composition of matter comprising high viscosity sodium alginate, cellulose sulfate and a multi-component polycation. Additionally, the present invention provides methods for making capsules, measuring capsule permeability to immunologically-relevant proteins and treating disease in an animal using encapsulated cells. Over one thousand combinations of polyanions and polycations were examined as polymer candidates suitable for encapsulation of living cells and thirty-three pairs were effective. The combination of sodium alginate, cellulose sulfate, poly(methylene-co-guanidine) hydrochloride, calcium chloride, and sodium chloride produced the most desirable results. Pancreatic islets encapsulated in this multicomponent capsule demonstrated glucose-stimulated insulin secretion in vitro and reversed diabetes without stimulating immune reaction in mice. The capsule formulation and system of the present invention allows independent adjustments of capsule size, wall thickness, mechanical strength and permeability, and offers distinct advantages for immunoisolating cells."
Neutron and Gamma-Ray Detection System,57.94053,photo analysis invention,"['Nuclear Physics', 'Space Radiation', 'Instrumentation and Photography']",The present invention is a radially symmetric imaging detector that measures an incident neutron's or gamma-ray's energy and identifies its source on an event-by-event basis.
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,57.903053,photo analysis invention,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
Puncture-Healing Thermoplastic Resin Carbon-Fiber-Reinforced Composites,57.856026,photo analysis invention,['Composite Materials'],"A composite comprising a combination of a self-healing polymer matrix and a carbon fiber reinforcement is described. In one embodiment, the matrix is a polybutadiene graft copolymer matrix, such as polybutadiene graft copolymer comprising poly(butadiene)-graft-poly(methyl acrylate-co-acrylonitrile). A method of fabricating the composite is also described, comprising the steps of manufacturing a pre-impregnated unidirectional carbon fiber preform by wetting a plurality of carbon fibers with a solution, the solution comprising a self-healing polymer and a solvent, and curing the preform. A method of repairing a structure made from the composite of the invention is described. A novel prepreg material used to manufacture the composite of the invention is described."
Formation and Control of Fluidic Species,57.823753,photo analysis invention,['Instrumentation and Photography'],"This invention generally relates to systems and methods for the formation and/or control of fluidic species, and articles produced by such systems and methods. In some cases, the invention involves unique fluid channels, systems, controls, and/or restrictions, and combinations thereof. In certain embodiments, the invention allows fluidic streams (which can be continuous or discontinuous, i.e., droplets) to be formed and/or combined, at a variety of scales, including microfluidic scales. In one set of embodiments, a fluidic stream may be produced from a channel, where a cross-sectional dimension of the fluidic stream is smaller than that of the channel, for example, through the use of structural elements, other fluids, and/or applied external fields, etc. In some cases, a Taylor cone may be produced. In another set of embodiments, a fluidic stream may be manipulated in some fashion, for example, to create tubes (which may be hollow or solid), droplets, nested tubes or droplets, arrays of tubes or droplets, meshes of tubes, etc. In some cases, droplets produced using certain embodiments of the invention may be charged or substantially charged, which may allow their further manipulation, for instance, using applied external fields. Non-limiting examples of such manipulations include producing charged droplets, coalescing droplets (especially at the microscale), synchronizing droplet formation, aligning molecules within the droplet, etc. In some cases, the droplets and/or the fluidic streams may include colloids, cells, therapeutic agents, and the like. "
Method and apparatus for holographic processing,57.750492,photo analysis invention,['Optics'],"A method and apparatus for holographically processing optical signals in a fiber-optic sensor system. In the present invention, holographic processors are utilized in various combinations with light sources, fiber-optic transmission means, sensors and detectors to provide low cost, compact, sensitive and accurate sensor systems. By means of the holographic processors of the present invention, the aforementioned sensor systems are used to monitor such physical parameters as temperature, pressure, flow-rate, and the like, and to provide output signal indications thereof that are compatible with digital receiving stations and immune to electro-magnetic interference, hazardous atmosphere, and the inimical effects of inadvertent intensity variation due to equipment vibration and the like. In one typical embodiment, a pair of holographic processors are employed in a color multiplex-demultiplex sensor system in which a first holographic processor is employed to color multiplex a sensor signal and a second holographic processor is employed to decode the color-multiplexed signal into a binary pattern that is then transmitted onto a set of photodetectors. In each of the embodiments of the invention disclosed, a unique geometrical orientation of a hologram is utilized to minimize spurious signal interference that would otherwise hamper or totally preclude the holographic processing."
Puncture-Healing Thermoplastic Resin Carbon-Fiber Reinforced Composites,56.447697,photo analysis invention,['Composite Materials'],"A composite comprising a combination of a self-healing polymer matrix and a carbon fiber reinforcement is described. In one embodiment, the matrix is a polybutadiene graft copolymer matrix, such as polybutadiene graft copolymer comprising poly(butadiene)-graft-poly(methyl acrylate-co-acrylonitrile). A method of fabricating the composite is also described, comprising the steps of manufacturing a pre-impregnated unidirectional carbon fiber preform by wetting a plurality of carbon fibers with a solution, the solution comprising a self-healing polymer and a solvent, and curing the preform. A method of repairing a structure made from the composite of the invention is described. A novel prepreg material used to manufacture the composite of the invention is described."
"Smart materials: strain sensing and stress determination by means of nanotube sensing systems, composites, and devices",56.30526,photo analysis invention,['Composite Materials'],"The present invention is directed toward devices comprising carbon nanotubes that are capable of detecting displacement, impact, stress, and/or strain in materials, methods of making such devices, methods for sensing/detecting/monitoring displacement, impact, stress, and/or strain via carbon nanotubes, and various applications for such methods and devices. The devices and methods of the present invention all rely on mechanically-induced electronic perturbations within the carbon nanotubes to detect and quantify such stress/strain. Such detection and quantification can rely on techniques which include, but are not limited to, electrical conductivity/conductance and/or resistivity/resistance detection/measurements, thermal conductivity detection/measurements, electroluminescence detection/measurements, photoluminescence detection/measurements, and combinations thereof. All such techniques rely on an understanding of how such properties change in response to mechanical stress and/or strain."
Particle sensor array,55.667645,photo analysis invention,['Instrumentation and Photography'],"A particle sensor array which in a preferred embodiment comprises a static random access memory having a plurality of ion-sensitive memory cells, each such cell comprising at least one pull-down field effect transistor having a sensitive drain surface area (such as by bloating) and at least one pull-up field effect transistor having a source connected to an offset voltage. The sensitive drain surface area and the offset voltage are selected for memory cell upset by incident ions such as alpha-particles. The static random access memory of the present invention provides a means for selectively biasing the memory cells into the same state in which each of the sensitive drain surface areas is reverse biased and then selectively reducing the reversed bias on these sensitive drain surface areas for increasing the upset sensitivity of the cells to ions. The resulting selectively sensitive memory cells can be used in a number of applications. By way of example, the present invention can be used for measuring the linear energy transfer of ion particles, as well as a device for assessing the resistance of CMOS latches to Cosmic Ray induced single event upsets. The sensor of the present invention can also be used to determine the uniformity of an ion beam."
Particle Capture Devices and Methods of Use Thereof,55.62483,photo analysis invention,"['Instrumentation and Photography', 'Fluid Mechanics and Thermodynamics']","The present invention provides a device and methods of use thereof in microscale particle capturing and particle pairing. This invention provides particle patterning device, which mechanically traps individual particles within first chambers of capture units, transfer the particles to second chambers of opposing capture units, and traps a second type of particle in the same second chamber. The device and methods allow for high yield assaying of trapped cells, high yield fusion of trapped, paired cells, for controlled binding of particles to cells and for specific chemical reactions between particle interfaces and particle contents. The device and method provide means of identification of the particle population and a facile route to particle collection."
Digital Mammography with a Mosaic of CCD-Arrays,55.47521,photo analysis invention,['Aerospace Medicine'],"The present invention relates generally to a mammography device and method and more particularly to a novel digital mammography device and method to detect microcalcifications of precancerous tissue. A digital mammography device uses a mosaic of electronic digital imaging arrays to scan an x-ray image. The mosaic of arrays is repositioned several times to expose different portions of the image, until the entire image is scanned. The data generated by the arrays during each exposure is stored in a computer. After the final exposure, the computer combines data of the several partial images to produce a composite of the original x-ray image. An aperture plate is used to reduce scatter and the overall exposure of the patient to x-rays. The novelty of this invention is that it provides a digital mammography device with large field coverage, high spatial resolution, scatter rejection, excellent contrast characteristics and lesion detectability under clinical conditions. This device also shields the patient from excessive radiation, can detect extremely small calcifications and allows manipulation and storage of the image."
Designing It Smart With SIV,55.266747,photo analysis invention,['Instrumentation and Photography'],"When research staff at NASA s Glenn Research Center developed and patented Stereo Imaging Velocimetry (SIV), the world s first three-dimensional (3-D), full-field quantitative and qualitative analysis tool to investigate flow velocities, experiments that were previously impossible became a reality. Seizing the opportunity to commercialize NASA s breakthrough invention, Digital Interface Systems (DIS), Inc., of North Olmsted, Ohio, acquired an exclusive license to market SIV, which has a range of applications from improving the aerodynamics of aircraft and automobiles to avoiding ""no flow"" regions in artificial hearts."
Streptavidin-binding peptides and uses thereof,55.06184,photo analysis invention,['Life Sciences (General)'],"The invention provides peptides with high affinity for streptavidin. These peptides may be expressed as part of fusion proteins to facilitate the detection, quantitation, and purification of proteins of interest."
Method for Ground-to-Satellite Laser Calibration System,55.056423,photo analysis invention,['Instrumentation and Photography'],"The present invention comprises an approach for calibrating the sensitivity to polarization, optics degradation, spectral and stray light response functions of instruments on orbit. The concept is based on using an accurate ground-based laser system, Ground-to-Space Laser Calibration (GSLC), transmitting laser light to instrument on orbit during nighttime substantially clear-sky conditions. To minimize atmospheric contribution to the calibration uncertainty the calibration cycles should be performed in short time intervals, and all required measurements are designed to be relative. The calibration cycles involve ground operations with laser beam polarization and wavelength changes.



"
Method for Ground-to-Space Laser Calibration System,54.819706,photo analysis invention,['Instrumentation and Photography'],"The present invention comprises an approach for calibrating the sensitivity to polarization, optics degradation, spectral and stray light response functions of instruments on orbit. The concept is based on using an accurate ground-based laser system, Ground-to-Space Laser Calibration (GSLC), transmitting laser light to instrument on orbit during nighttime substantially clear-sky conditions. To minimize atmospheric contribution to the calibration uncertainty the calibration cycles should be performed in short time intervals, and all required measurements are designed to be relative. The calibration cycles involve ground operations with laser beam polarization and wavelength changes."
Streptavidin-binding peptides and uses thereof,54.611393,photo analysis invention,['Life Sciences (General)'],"The invention provides peptides with high affinity for streptavidin. These peptides may be expressed as part of fusion proteins to facilitate the detection, quantitation, and purification of proteins of interest."
Isolated resonator gyroscope with a drive and sense plate,54.44623,photo analysis invention,['Instrumentation and Photography'],"The present invention discloses a resonator gyroscope comprising a vibrationally isolated resonator including a proof mass, a counterbalancing plate having an extensive planar region, and one or more flexures interconnecting the proof mass and counterbalancing plate. A baseplate is affixed to the resonator by the one or more flexures and sense and drive electrodes are affixed to the baseplate proximate to the extensive planar region of the counterbalancing plate for exciting the resonator and sensing movement of the gyroscope. The isolated resonator transfers substantially no net momentum to the baseplate when the resonator is excited."
Wavelength independent interferometer,54.377274,photo analysis invention,['Instrumentation and Photography'],"A polychromatic interferometer utilizing a plurality of parabolic reflective surfaces to properly preserve the fidelity of light wavefronts irrespective of their wavelengths as they pass through the instrument is disclosed. A preferred embodiment of the invention utilizes an optical train which comprises three off-axis parabolas arranged in conjunction with a beam-splitter and a reference mirror to form a Twyman-Green interferometer. An illumination subsystem is provided and comprises a pair of lasers at different preselected wavelengths in the visible spectrum. The output light of the two lasers is coaxially combined by means of a plurality of reflectors and a grating beam combiner to form a single light source at the focal point of the first parabolic reflection surface which acts as a beam collimator for the rest of the optical train. By using visible light having two distinct wavelengths, the present invention provides a long equivalent wavelength interferogram which operates at visible light wherein the effective wavelength is equal to the product of the wavelengths of the two laser sources divided by their difference in wavelength. As a result, the invention provides the advantages of what amounts to long wavelength interferometry but without incurring the disadvantage of the negligible reflection coefficient of the human eye to long wavelength frequencies which would otherwise defeat any attempt to form an interferogram at that low frequency using only one light source."
Method and apparatus for white-light dispersed-fringe interferometric measurement of corneal topography,54.342937,photo analysis invention,['Instrumentation and Photography'],"An novel interferometric apparatus and method for measuring the topography of aspheric surfaces, without requiring any form of scanning or phase shifting. The apparatus and method of the present invention utilize a white-light interferometer, such as a white-light Twyman-Green interferometer, combined with a means for dispersing a polychromatic interference pattern, using a fiber-optic bundle and a disperser such as a prism for determining the monochromatic spectral intensities of the polychromatic interference pattern which intensities uniquely define the optical path differences or OPD between the surface under test and a reference surface such as a reference sphere. Consequently, the present invention comprises a snapshot approach to measuring aspheric surface topographies such as the human cornea, thereby obviating vibration sensitive scanning which would otherwise reduce the accuracy of the measurement. The invention utilizes a polychromatic interference pattern in the pupil image plane, which is dispersed on a point-wise basis, by using a special area-to-line fiber-optic manifold, onto a CCD or other type detector comprising a plurality of columns of pixels. Each such column is dedicated to a single point of the fringe pattern for enabling determination of the spectral content of the pattern. The auto-correlation of the dispersed spectrum of the fringe pattern is uniquely characteristic of a particular optical path difference between the surface under test and a reference surface."
Monitored separation device,54.24116,photo analysis invention,['Optics'],"A device for separating and purifying useful quantities of particles comprises: a. an anolyte reservoir connected to an anode, the anolyte reservoir containing an electrophoresis buffer; b. a catholyte reservoir connected to a cathode, the catholyte reservoir also containing the electrophoresis buffer; c. a power supply connected to the anode and to the cathode; d. a column having a first end inserted into the anolyte reservoir, a second end inserted into the catholyte reservoir, and containing a separation medium; e. a light source; f. a first optical fiber having a first fiber end inserted into the separation medium, and having a second fiber end connected to the light source; g. a photo detector; h. a second optical fiber having a third fiber end inserted into the separation medium, and having a fourth fiber end connected to the photo detector; and i. an ion-exchange membrane in the anolyte reservoir."
Method for Monitored Separation and Collection of Biological Materials,54.162,photo analysis invention,"['Aerospace Medicine', 'Life Sciences (General)', 'Inorganic, Organic and Physical Chemistry']","A device for separating and purifying useful quantities of particles comprises: (a) an anolyte reservoir connected to an anode, the anolyte reservoir containing an electrophoresis buffer; (b) a catholyte reservoir connected to a cathode, the catholyte reservoir also containing the electrophoresis buffer; (c) a power supply connected to the anode and to the cathode; (d) a column having a first end inserted into the anolyte reservoir, a second end inserted into the catholyte reservoir, and containing a separation medium; (e) a light source; (f) a first optical fiber having a first fiber end inserted into the separation medium, and having a second fiber end connected to the light source; (g) a photo detector; (h) a second optical fiber having a third fiber end inserted into the separation medium, and having a fourth fiber end connected to the photo detector; and (i) an ion-exchange membrane in the anolyte reservoir."
Method and Apparatus for a Miniature Bioreactor System for Long-Term Cell Culture,54.077812,photo analysis invention,['Life Sciences (General)'],"A bioreactor and method that permits continuous and simultaneous short, moderate, or long term cell culturing of one or more cell types or tissue in a laminar flow configuration is disclosed, where the bioreactor supports at least two laminar flow zones, which are isolated by laminar flow without the need for physical barriers between the zones. The bioreactors of this invention are ideally suited for studying short, moderate and long term studies of cell cultures and the response of cell cultures to one or more stressors such as pharmaceuticals, hypoxia, pathogens, or any other stressor. The bioreactors of this invention are also ideally suited for short, moderate or long term cell culturing with periodic cell harvesting and/or medium processing for secreted cellular components."
Sensor chip and apparatus for tactile and/or flow sensing,54.052826,photo analysis invention,['Electronics and Electrical Engineering'],"A sensor chip, comprising a flexible, polymer-based substrate, and at least one microfabricated sensor disposed on the substrate and including a conductive element. The at least one sensor comprises at least one of a tactile sensor and a flow sensor. Other embodiments of the present invention include sensors and/or multi-modal sensor nodes."
Sensor chip and apparatus for tactile and/or flow sensing,53.88092,photo analysis invention,['Electronics and Electrical Engineering'],"A sensor chip, comprising a flexible, polymer-based substrate, and at least one microfabricated sensor disposed on the substrate and including a conductive element. The at least one sensor comprises at least one of a tactile sensor and a flow sensor. Other embodiments of the present invention include sensors and/or multi-modal sensor nodes."
Scanning Mode Sensor for Detection of Flow Inhomogeneities,53.775314,photo analysis invention,['Instrumentation and Photography'],"A scanning mode sensor and method is provided for detection of flow inhomogeneities such as shock. The field of use of this invention is ground test control and engine control during supersonic flight. Prior art measuring techniques include interferometry. Schlieren, and shadowgraph techniques. These techniques. however, have problems with light dissipation. The present method and sensor utilizes a pencil beam of energy which is passed through a transparent aperture in a flow inlet in a time-sequential manner so as to alter the energy beam. The altered beam or its effects are processed and can be studied to reveal information about flow through the inlet which can in turn be used for engine control."
Adjustable mount for electro-optic transducers in an evacuated cryogenic system,53.367496,photo analysis invention,['MECHANICAL ENGINEERING'],"The invention is an adjustable mount for positioning an electro-optic transducer in an evacuated cryogenic environment. Electro-optic transducers are used in this manner as high sensitivity detectors of gas emission lines of spectroscopic analysis. The mount is made up of an adjusting mechanism and a transducer mount. The adjusting mechanism provided five degrees of freedom, linear adjustments and angular adjustments. The mount allows the use of an internal lens to focus energy on the transducer element thereby improving the efficiency of the detection device. Further, the transducer mount, although attached to the adjusting mechanism, is isolated thermally such that a cryogenic environment can be maintained at the transducer while the adjusting mechanism remains at room temperature. Radiation shields also are incorporated to further reduce heat flow to the transducer location."
Active pixel sensor with intra-pixel charge transfer,52.928883,photo analysis invention,['Instrumentation and Photography'],"An imaging device formed as a monolithic complementary metal oxide semiconductor integrated circuit in an industry standard complementary metal oxide semiconductor process, the integrated circuit including a focal plane array of pixel cells, each one of the cells including a photogate overlying the substrate for accumulating photo-generated charge in an underlying portion of the substrate, a readout circuit including at least an output field effect transistor formed in the substrate, and a charge coupled device section formed on the substrate adjacent the photogate having a sensing node connected to the output transistor and at least one charge coupled device stage for transferring charge from the underlying portion of the substrate to the sensing node."
Systems and Methods for Automated Vessel Navigation Using Sea State Prediction,52.912704,photo analysis invention,"['Oceanography', 'Instrumentation and Photography', 'Statistics and Probability']","Systems and methods for sea state prediction and autonomous navigation in accordance with embodiments of the invention are disclosed. One embodiment of the invention includes a method of predicting a future sea state including generating a sequence of at least two 3D images of a sea surface using at least two image sensors, detecting peaks and troughs in the 3D images using a processor, identifying at least one wavefront in each 3D image based upon the detected peaks and troughs using the processor, characterizing at least one propagating wave based upon the propagation of wavefronts detected in the sequence of 3D images using the processor, and predicting a future sea state using at least one propagating wave characterizing the propagation of wavefronts in the sequence of 3D images using the processor. Another embodiment includes a method of autonomous vessel navigation based upon a predicted sea state and target location."
Dynamically controlled crystal growth system,52.853653,photo analysis invention,['Life Sciences (General)'],"Crystal growth can be initiated and controlled by dynamically controlled vapor diffusion or temperature change. In one aspect, the present invention uses a precisely controlled vapor diffusion approach to monitor and control protein crystal growth. The system utilizes a humidity sensor and various interfaces under computer control to effect virtually any evaporation rate from a number of different growth solutions simultaneously by means of an evaporative gas flow. A static laser light scattering sensor can be used to detect aggregation events and trigger a change in the evaporation rate for a growth solution. A control/follower configuration can be used to actively monitor one chamber and accurately control replicate chambers relative to the control chamber. In a second aspect, the invention exploits the varying solubility of proteins versus temperature to control the growth of protein crystals. This system contains miniature thermoelectric devices under microcomputer control that change temperature as needed to grow crystals of a given protein. Complex temperature ramps are possible using this approach. A static laser light scattering probe also can be used in this system as a non-invasive probe for detection of aggregation events. The automated dynamic control system provides systematic and predictable responses with regard to crystal size. These systems can be used for microgravity crystallization projects, for example in a space shuttle, and for crystallization work under terrestial conditions. The present invention is particularly useful for macromolecular crystallization, e.g. for proteins, polypeptides, nucleic acids, viruses and virus particles."
Identification Of Cells With A Compact Microscope Imaging System With Intelligent Controls,52.83168,photo analysis invention,['Instrumentation and Photography'],"A Microscope Imaging System (CMIS) with intelligent controls is disclosed that provides techniques for scanning, identifying, detecting and tracking mic?oscopic changes in selected characteristics or features of various surfaces including, but not limited to, cells, spheres, and manufactured products subject to difficult-to-see imperfections. The practice of the present invention provides applications that include colloidal hard spheres experiments, biological cell detection for patch clamping, cell movement and tracking, as well as defect identification in products, such as semiconductor devices, where surface damage can be significant, but difficult to detect. The CMIS system is a machine vision system, which combines intelligent image processing with remote control capabilities and provides the ability to autofocus on a microscope sample, automatically scan an image, and perform machine vision analysis on multiple samples simultaneously."
Tracking of cells with a compact microscope imaging system with intelligent controls,52.673264,photo analysis invention,['Instrumentation and Photography'],"A Microscope Imaging System (CMIS) with intelligent controls is disclosed that provides techniques for scanning, identifying, detecting and tracking microscopic changes in selected characteristics or features of various surfaces including, but not limited to, cells, spheres, and manufactured products subject to difficult-to-see imperfections. The practice of the present invention provides applications that include colloidal hard spheres experiments, biological cell detection for patch clamping, cell movement and tracking, as well as defect identification in products, such as semiconductor devices, where surface damage can be significant, but difficult to detect. The CMIS system is a machine vision system, which combines intelligent image processing with remote control capabilities and provides the ability to auto-focus on a microscope sample, automatically scan an image, and perform machine vision analysis on multiple samples simultaneously."
Surface Imaging Skin Friction Instrument and Method,52.45737,photo analysis invention,['Instrumentation and Photography'],"A surface imaging skin friction instrument allowing 2D resolution of spatial image by a 2D Hilbert transform and 2D inverse thin-oil film solver, providing an innovation over prior art single point approaches. Incoherent, monochromatic light source can be used. The invention provides accurate, easy to use, economical measurement of larger regions of surface shear stress in a single test."
Chelators whose affinity for calcium is decreased by illumination,52.323624,photo analysis invention,"['Inorganic, Organic and Physical Chemistry']","The present invention discloses a group of calcium chelating compounds which have a descreased affinity for calcium following illumination. These new compounds contain a photolabile nitrobenzyl derivative coupled to a tetracarboxylate Ca.sup.2+ chelating parent compound having the octacoordinate chelating groups characteristic of EGTA or BAPTA. In a first form, the new compounds are comprised of a BAPTA-like chelator coupled to a single 2-nitrobenzyl derivative, which in turn is a photochemical precursor of a 2-nitrosobenzophenone. In a second form, the new compounds are comprised of a BAPTA-like chelator coupled to two 2-nitrobenzyl derivatives, themselves photochemical prcursors of the related 2-nitrosobenzophenones. The present invention also discloses a novel method for preparing 1-hydroxy- or 1-alkoxy-1-(2-nitroaryl)-1-aryl methanes. Methanes of this type are critical to the preparation of, or actually constitute, the photolabile Ca.sup.2+ chelating compounds disclosed and claimed herein."
Selective functionalization of carbon nanotube tips allowing fabrication of new classes of nanoscale sensing and manipulation tools,52.311054,photo analysis invention,['Electronics and Electrical Engineering'],"Embodiments in accordance with the present invention relate to techniques for the growth and attachment of single wall carbon nanotubes (SWNT), facilitating their use as robust and well-characterized tools for AFM imaging and other applications. In accordance with one embodiment, SWNTs attached to an AFM tip can function as a structural scaffold for nanoscale device fabrication on a scanning probe. Such a probe can trigger, with nanometer precision, specific biochemical reactions or conformational changes in biological systems. The consequences of such triggering can be observed in real time by single-molecule fluorescence, electrical, and/or AFM sensing. Specific embodiments in accordance with the present invention utilize sensing and manipulation of individual molecules with carbon nanotubes, coupled with single-molecule fluorescence imaging, to allow observation of spectroscopic signals in response to mechanically induced molecular changes. Biological macromolecules such as proteins or DNA can be attached to nanotubes to create highly specific single-molecule probes for investigations of intermolecular dynamics, for assembling hybrid biological and nanoscale materials, or for developing molecular electronics. In one example, electrical wiring of single redox enzymes to carbon nanotube scanning probes allows observation and electrochemical control over single enzymatic reactions by monitoring fluorescence from a redox-active cofactor or the formation of fluorescent products. Enzymes ''nanowired'' to the tips of carbon nanotubes in accordance with embodiments of the present invention, may enable extremely sensitive probing of biological stimulus-response with high spatial resolution, including product-induced signal transduction."
Photochemical preparation of olefin addition catalysts,52.109943,photo analysis invention,['Chemistry and Materials (General)'],"Novel polymer supported catalysts are prepared by photo-irradiation of low valent transition metal compounds such as Co.sub.2 (CO).sub.8, Rh.sub.4 (CO).sub.12 or Ru.sub.3 (CO).sub.12 in the presence of solid polymers containing amine ligands such as polyvinyl pyridine. Hydroformylation of olefins to aldehydes at ambient conditions has been demonstrated."
Paraelectric gas flow accelerator,51.922333,photo analysis invention,['Instrumentation and Photography'],"A substrate is configured with first and second sets of electrodes, where the second set of electrodes is positioned asymmetrically between the first set of electrodes. When a RF voltage is applied to the electrodes sufficient to generate a discharge plasma (e.g., a one-atmosphere uniform glow discharge plasma) in the gas adjacent to the substrate, the asymmetry in the electrode configuration results in force being applied to the active species in the plasma and in turn to the neutral background gas. Depending on the relative orientation of the electrodes to the gas, the present invention can be used to accelerate or decelerate the gas. The present invention has many potential applications, including increasing or decreasing aerodynamic drag or turbulence, and controlling the flow of active and/or neutral species for such uses as flow separation, altering heat flow, plasma cleaning, sterilization, deposition, etching, or alteration in wettability, printability, and/or adhesion."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 16),51.864132,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Citations of patents and patent applications for the period May 1969 through December 1979 are indexed according to subject, invention, source, U. S. patent number, and accession number."
"Image Station Matching, Preprocessing, Spatial Registration and Change Detection with Multi-Temporal Remotely-Sensed Imagery",51.86028,photo analysis invention,['Instrumentation and Photography'],"A method for collecting and processing remotely sensed imagery in order to achieve precise spatial co-registration (e.g., matched alignment) between multi-temporal image sets is presented. Such precise alignment or spatial co-registration of imagery can be used for change detection, image fusion, and temporal analysis/modeling. Further, images collected in this manner may be further processed in such a way that image frames or line arrays from corresponding photo stations are matched, co-aligned and if desired merged into a single image and/or subjected to the same processing sequence. A second methodology for automated detection of moving objects within a scene using a time series of remotely sensed imagery is also presented. Specialized image collection and preprocessing procedures are utilized to obtain precise spatial co-registration (image registration) between multitemporal image frame sets. In addition, specialized change detection techniques are employed in order to automate the detection of moving objects."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 23),51.8145,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for 4000 patent and patent applications citations for the period May 1969 through June 1983 are listed. Subject, invention, source, number, and accession number indexes are included."
Forced ion migration for chalcogenide phase change memory device,51.693676,photo analysis invention,['Computer Programming and Software'],"Non-volatile memory devices with two stacked layers of chalcogenide materials comprising the active memory device have been investigated for their potential as phase-change memories. The devices tested included GeTe/SnTe, Ge.sub.2Se.sub.3/SnTe, and Ge.sub.2Se.sub.3/SnSe stacks. All devices exhibited resistance switching behavior. The polarity of the applied voltage with respect to the SnTe or SnSe layer was critical to the memory switching properties, due to the electric field induced movement of either Sn or Te into the Ge-chalcogenide layer. One embodiment of the invention is a device comprising a stack of chalcogenide-containing layers which exhibit phase-change switching only after a reverse polarity voltage potential is applied across the stack causing ion movement into an adjacent layer and thus ""activating"" the device to act as a phase-change random access memory device or a reconfigurable electronics device when the applied voltage potential is returned to the normal polarity. Another embodiment of the invention is a device that is capable of exhibiting more than two data states."
Organic crystalline films for optical applications and related methods of fabrication,51.64037,photo analysis invention,"['Inorganic, Organic and Physical Chemistry']","The present invention provides organic single crystal films of less than 20 .mu.m, and devices and methods of making such films. The crystal films are useful in electro-optical applications and can be provided as part of an electro-optical device which provides strength, durability, and relative ease of manipulation of the mono-crystalline films during and after crystal growth."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 29),51.3719,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for over 4400 patents and patent applications citations for the period May 1969 through June 1986 are listed. Subject, invention, source, number, and accession number indexes are included."
Electronically conducting polymers with silver grains,51.349514,photo analysis invention,['Electronics and Electrical Engineering'],"The present invention provides electronically conducting polymer films formed from photosensitive formulations of pyrrole and an electron acceptor that have been selectively exposed to UV light, laser light, or electron beams. The formulations may include photoinitiators, flexibilizers, solvents and the like. These solutions can be used in applications including printed circuit boards and through-hole plating and enable direct metallization processes on non-conducting substrates. After forming the conductive polymer patterns, a printed wiring board can be formed by sensitizing the polymer with palladium and electrolytically depositing copper."
Forced Ion Migration for Chalcogenide Phase Change Memory Device,51.296345,photo analysis invention,"['Solid-State Physics', 'Computer Operations and Hardware']","Non-volatile memory devices with two stacked layers of chalcogenide materials comprising the active memory device have been investigated for their potential as phase-change memories. The devices tested included GeTe/SnTe, Ge2Se3/SnTe, and Ge2Se3/SnSe stacks. All devices exhibited resistance switching behavior. The polarity of the applied voltage with respect to the SnTe or SnSe layer was critical to the memory switching properties, due to the electric field induced movement of either Sn or Te into the Ge-chalcogenide layer. One embodiment of the invention is a device comprising a stack of chalcogenide-containing layers which exhibit phase-change switching only after a reverse polarity voltage potential is applied across the stack causing ion movement into an adjacent layer and thus ""activating"" the device to act as a phase-change random access memory device or a reconfigurable electronics device when the applied voltage potential is returned to the normal polarity. Another embodiment of the invention is a device that is capable of exhibiting more than two data states."
Isolated resonator gyroscope with a drive and sense plate,51.287766,photo analysis invention,['Physics (General)'],"The present invention discloses a resonator gyroscope comprising a vibrationally isolated resonator including a proof mass, a counterbalancing plate having an extensive planar region, and one or more flexures interconnecting the proof mass and counterbalancing plate. A baseplate is affixed to the resonator by the one or more flexures and sense and drive electrodes are affixed to the baseplate proximate to the extensive planar region of the counterbalancing plate for exciting the resonator and sensing movement of the gyroscope. The isolated resonator transfers substantially no net momentum to the baseplate when the resonator is excited."
Smart accelerometer,51.242405,photo analysis invention,['INSTRUMENTATION AND PHOTOGRAPHY'],The invention discloses methods and apparatus for detecting vibrations from machines which indicate an impending malfunction for the purpose of preventing additional damage and allowing for an orderly shutdown or a change in mode of operation. The method and apparatus is especially suited for reliable operation in providing thruster control data concerning unstable vibration in an electrical environment which is typically noisy and in which unrecognized ground loops may exist.
Forced ion migration for chalcogenide phase change memory device,51.22648,photo analysis invention,['Electronics and Electrical Engineering'],"Non-volatile memory devices with two stacked layers of chalcogenide materials comprising the active memory device have been investigated for their potential as phase change memories. The devices tested included GeTe/SnTe, Ge.sub.2Se.sub.3/SnTe, and Ge.sub.2Se.sub.3/SnSe stacks. All devices exhibited resistance switching behavior. The polarity of the applied voltage with respect to the SnTe or SnSe layer was critical to the memory switching properties, due to the electric field induced movement of either Sn or Te into the Ge-chalcogenide layer. One embodiment of the invention is a device comprising a stack of chalcogenide-containing layers which exhibit phase change switching only after a reverse polarity voltage potential is applied across the stack causing ion movement into an adjacent layer and thus ""activating"" the device to act as a phase change random access memory device or a reconfigurable electronics device when the applied voltage potential is returned to the normal polarity. Another embodiment of the invention is a device that is capable of exhibiting more that two data states."
Ultraviolet and thermally stable polymer compositions,51.184498,photo analysis invention,['NONMETALLIC MATERIALS'],"A new class of polymers is provided, namely, poly (diarylsiloxy) arylazines. These novel polymers have a basic chemical composition which has the property of stabilizing the optical and physical properties of the polymer against the degradative effect of ultraviolet light and high temperatures. This stabilization occurs at wavelengths including those shorter than found on the surface of the earth and in the absence or presence of oxygen, making the polymers useful for high performance coating applications in extraterrestrial space as well as similar applications in terrestrial service. The invention also provides novel aromatic azines which are useful in the preparation of polymers such as those described."
Digital X-ray camera for quality evaluation three-dimensional topographic reconstruction of single crystals of biological macromolecules,51.100243,photo analysis invention,['Instrumentation and Photography'],"The present invention provides a digital topography imaging system for determining the crystalline structure of a biological macromolecule, wherein the system employs a charge coupled device (CCD) camera with antiblooming circuitry to directly convert x-ray signals to electrical signals without the use of phosphor and measures reflection profiles from the x-ray emitting source after x-rays are passed through a sample. Methods for using said system are also provided."
NASA patent abstracts bibliography: A continuing bibliography. Section 1: Abstracts (supplement 22),51.059082,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for over 4000 patents and patent applications citations for the period May 1969 through December 1982 are listed. Subject, invention, source, number, and accession number indexes are included."
NASA Patent Abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 20),50.913197,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for approximately 4000 citations for the period May 1969 through December 1981 are listed. Subject, invention, source, number, and accession number indexes are included."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 21) Abstracts,50.790325,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for 4000 patents and patent applications citations for the period May 1969 through June 1982 are listed. Subject, invention, source, number and accession number indexes are included."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 19),50.759697,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Citations of approximately 4,000 patents and patent applications for the period May 1969 through July 1981 are indexed according to subject, invention, source, number, and accession number."
NASA patent abstracts bibliography: A continuing bibliography. Section 2: Indexes (supplement 18),50.238007,photo analysis invention,['DOCUMENTATION AND INFORMATION SCIENCE'],"Entries for 3900 patents and patent applications citations for the period May 1980 through December 1980 are listed. Indexes for subject, invention, source, number, and accession number are included."
Streamline-based microfluidic device,50.195213,photo analysis invention,['Life Sciences (General)'],"The present invention provides a streamline-based device and a method for using the device for continuous separation of particles including cells in biological fluids. The device includes a main microchannel and an array of side microchannels disposed on a substrate. The main microchannel has a plurality of stagnation points with a predetermined geometric design, for example, each of the stagnation points has a predetermined distance from the upstream edge of each of the side microchannels. The particles are separated and collected in the side microchannels."
Method of forming electronically conducting polymers on conducting and nonconducting substrates,50.105522,photo analysis invention,['Nonmetallic Materials'],"The present invention provides electronically conducting polymer films formed from photosensitive formulations of pyrrole and an electron acceptor that have been selectively exposed to UV light, laser light, or electron beams. The formulations may include photoinitiators, flexibilizers, solvents and the like. These solutions can be used in applications including printed circuit boards and through-hole plating and enable direct metallization processes on non-conducting substrates. After forming the conductive polymer patterns, a printed wiring board can be formed by sensitizing the polymer with palladium and electrolytically depositing copper."
System and method for confining an object to a region of fluid flow having a stagnation point,50.059612,photo analysis invention,['Instrumentation and Photography'],"A device for confining an object to a region proximate to a fluid flow stagnation point includes one or more inlets for carrying the fluid into the region, one or more outlets for carrying the fluid out of the region, and a controller, in fluidic communication with the inlets and outlets, for adjusting the motion of the fluid to produce a stagnation point in the region, thereby confining the object to the region. Applications include, for example, prolonged observation of the object, manipulation of the object, etc. The device optionally may employ a feedback control mechanism, a sensing apparatus (e.g., for imaging), and a storage medium for storing, and a computer for analyzing and manipulating, data acquired from observing the object. The invention further provides methods of using such a device and system in a number of fields, including biology, chemistry, physics, material science, and medical science."
Improved real-time imaging spectrometer,49.96342,photo analysis invention,['OPTICS'],"An improved AOTF-based imaging spectrometer that offers several advantages over prior art AOTF imaging spectrometers is presented. The ability to electronically set the bandpass wavelength provides observational flexibility. Various improvements in optical architecture provide simplified magnification variability, improved image resolution and light throughput efficiency and reduced sensitivity to ambient light. Two embodiments of the invention are: (1) operation in the visible/near-infrared domain of wavelength range 0.48 to 0.76 microns; and (2) infrared configuration which operates in the wavelength range of 1.2 to 2.5 microns."
Thermomagnetic recording and magnetic-optic playback system,49.6474,photo analysis invention,['INSTRUMENTATION AND PHOTOGRAPHY'],"A magnetic recording and magneto-optic playback system is disclosed wherein thermomagnetic recording is employed. A transparent isotropic film is heated along a continuous path by a focused laser beam. As each successive area of the path is heated locally to the vicinity of its Curie point in the presence of an applied magnetic field, a magneto-optic density is established proportional to the magnetic field and fixed in place as the area cools once the laser beam moves on to an adjacent area. To play back the recorded data, the intensity of the laser beam is reduced to avoid reaching the vicinity of the Curie point of the film as it is scanned by the laser beam in the same manner as for recording. A Faraday effect analyzer and photo detector are employed as a transducer for producing an output signal."
High-efficiency electron ionizer for a mass spectrometer array,49.524372,photo analysis invention,['Instrumentation and Photography'],"The present invention provides an improved electron ionizer for use in a quadrupole mass spectrometer. The improved electron ionizer includes a repeller plate that ejects sample atoms or molecules, an ionizer chamber, a cathode that emits an electron beam into the ionizer chamber, an exit opening for excess electrons to escape, at least one shim plate to collimate said electron beam, extraction apertures, and a plurality of lens elements for focusing the extracted ions onto entrance apertures."
Trace level detection of analytes using artificial olfactometry,49.35106,photo analysis invention,['Instrumentation and Photography'],"The present invention provides a device for detecting the presence of an analyte, such as for example, a lightweight device, including: a sample chamber having a fluid inlet port for the influx of the analyte; a fluid concentrator in flow communication with the sample chamber wherein the fluid concentrator has an absorbent material capable of absorbing the analyte and capable of desorbing a concentrated analyte; and an array of sensors in fluid communication with the concentrated analyte to be released from the fluid concentrator."
State of the art:  Design for noise I,49.306843,photo analysis invention,['ACOUSTICS'],"The engineering management strategy involved in designing helicopters to meet a noise requirement is discussed. A different perspective is given on how designs were accomplished in the past, how a new design would be carried out today, and how knowledge gained through acoustics R&D will have two important effects. It will lead to invention of quieter features to be incorporated, and it will improve the ability to predict accurately the noise levels of new designs before they are built. Each of these effects changes the basic design process."
Slush hydrogen fluid characterization and instrumentation analysis,48.828064,photo analysis invention,['PROPULSION SYSTEMS'],Slush hydrogen fluid characterization and instrumentation analysis
Spatial Standard Observer,48.711483,photo analysis invention,['Instrumentation and Photography'],"The present invention relates to devices and methods for the measurement and/or for the specification of the perceptual intensity of a visual image. or the perceptual distance between a pair of images. Grayscale test and reference images are processed to produce test and reference luminance images. A luminance filter function is convolved with the reference luminance image to produce a local mean luminance reference image . Test and reference contrast images are produced from the local mean luminance reference image and the test and reference luminance images respectively, followed by application of a contrast sensitivity filter. The resulting images are combined according to mathematical prescriptions to produce a Just Noticeable Difference, JND value, indicative of a Spatial Standard Observer. SSO. Some embodiments include masking functions. window functions. special treatment for images lying on or near border and pre-processing of test images."
Multiple order common path spectrometer,48.70133,photo analysis invention,['Physics (General)'],"The present invention relates to a dispersive spectrometer. The spectrometer allows detection of multiple orders of light on a single focal plane array by splitting the orders spatially using a dichroic assembly. A conventional dispersion mechanism such as a defraction grating disperses the light spectrally. As a result, multiple wavelength orders can be imaged on a single focal plane array of limited spectral extent, doubling (or more) the number of spectral channels as compared to a conventional spectrometer. In addition, this is achieved in a common path device."
Method for Reducing the Refresh Rate of Fiber Bragg Grating Sensors,48.54117,photo analysis invention,"['Instrumentation and Photography', 'Electronics and Electrical Engineering']","The invention provides a method of obtaining the FBG data in final form (transforming the raw data into frequency and location data) by taking the raw FBG sensor data and dividing the data into a plurality of segments over time. By transforming the raw data into a plurality of smaller segments, processing time is significantly decreased. Also, by defining the segments over time, only one processing step is required. By employing this method, the refresh rate of FBG sensor systems can be improved from about 1 scan per second to over 20 scans per second."
Scanning Mode Sensor for Detection of Flow Inhomogeneities,48.191326,photo analysis invention,['Fluid Mechanics and Heat Transfer'],"A scanning mode sensor and method is provided for detection of flow inhomogeneities such as shock. The field of use of this invention is ground test control and engine control during supersonic flight. Prior art measuring techniques include interferometry, Schlieren, and shadowgraph techniques. These techniques, however, have problems with light dissipation. The present method and sensor utilizes a pencil beam of energy which is passed through a transparent aperture in a flow inlet in a time-sequential manner so as to alter the energy beam. The altered beam or its effects are processed and can be studied to reveal information about flow through the inlet which can in turn be used for engine control."
Trace level detection of analytes using artificial olfactometry,48.06038,photo analysis invention,['Instrumentation and Photography'],"The present invention provides a device for detecting the presence of an analyte, wherein said analyte is a microorganism marker gas. The device comprises a sample chamber having a fluid inlet port for the influx of the microorganism marker gas; a fluid concentrator in flow communication with the sample chamber, wherein the fluid concentrator has an absorbent material capable of absorbing the microorganism marker gas and thereafter releasing a concentrated microorganism marker gas; and an array of sensors in fluid communication with the concentrated microorganism marker gas. The sensor array detects and identifies the marker gas upon its release from fluid concentrate."
Ask Magazine,47.82703,photo analysis invention,['Space Sciences (General)'],"The topics covered include: The Summer of Hydrogen; Leading Your Leaders; Dawn: Cooperation, not Control; Best Buy: Planning for Disaster The Astronaut Glove Challenge: Big Innovation from a (Very) Small Team; Using the Space Glove to Teach Spatial Thinking; The Power of Story; Interview with Jay O'Callahan; Learning from Space Entrepreneurs; Featured Invention: Laser Scaling Device; Reaching for the APEX at Ames; The Project Manager Who Saved His Country; Choosing and Developing the Right Leadership Styles for Projects; and The Costs of Knowledge."
Active pixel sensor with intra-pixel charge transfer,47.696983,photo analysis invention,['Solid-State Physics'],"An imaging device formed as a monolithic complementary metal oxide semiconductor integrated circuit in an industry standard complementary metal oxide semiconductor process, the integrated circuit including a focal plane array of pixel cells, each one of the cells including a photogate overlying the substrate for accumulating photo-generated charge in an underlying portion of the substrate, a readout circuit including at least an output field effect transistor formed in the substrate, and a charge coupled device section formed on the substrate adjacent the photogate having a sensing node connected to the output transistor and at least one charge coupled device stage for transferring charge from the underlying portion of the substrate to the sensing node."
Method of acquiring an image from an optical structure having pixels with dedicated readout circuits,47.413174,photo analysis invention,['Electronics and Electrical Engineering'],"An imaging device formed as a monolithic complementary metal oxide semiconductor integrated circuit in an industry standard complementary metal oxide semiconductor process, the integrated circuit including a focal plane array of pixel cells, each one of the cells including a photogate overlying the substrate for accumulating photo-generated charge in an underlying portion of the substrate, a readout circuit including at least an output field effect transistor formed in the substrate, and a charge coupled device section formed on the substrate adjacent the photogate having a sensing node connected to the output transistor and at least one charge coupled device stage for transferring charge from the underlying portion of the substrate to the sensing node."
Residual gas analysis in the Test and Evaluation Division at GSFC,47.39989,photo analysis invention,['INSTRUMENTATION AND PHOTOGRAPHY'],Residual gas analysis with mass spectrometer used for testing spacecraft and its components
Method and system for an automated tool for en route traffic controllers,47.332832,photo analysis invention,['Air Transportation and Safety'],"A method and system for a new automation tool for en route air traffic controllers first finds all aircraft flying on inefficient routes, then determines whether it is possible to save time by bypassing some route segments, and finally whether the improved route is free of conflicts with other aircraft. The method displays all direct-to eligible aircraft to an air traffic controller in a list sorted by highest time savings. By allowing the air traffic controller to easily identify and work with the highest pay-off aircraft, the method of the present invention contributes to a significant increase in both air traffic controller and aircraft productivity. A graphical computer interface (GUI) is used to enable the air traffic controller to send the aircraft direct to a waypoint or fix closer to the destination airport by a simple point and click action."
Head Mounted Display with a Roof Mirror Array Fold,47.181763,photo analysis invention,"['Instrumentation and Photography', 'Optics', 'Man/System Technology and Life Support']",The present invention includes a head mounted display (HMD) worn by a user. The HMD includes a display projecting an image through an optical lens. The HMD also includes a one-dimensional retro reflective array receiving the image through the optical lens at a first angle with respect to the display and deflecting the image at a second angle different than the first angle with respect to the display. The one-dimensional retro reflective array reflects the image in order to project the image onto an eye of the user.
Method and System For an Automated Tool for En Route Traffic Controllers,47.119656,photo analysis invention,['Mechanical Engineering'],"A method and system for a new automation tool for en route air traffic controllers first finds all aircraft flying on inefficient routes, then determines whether it is possible to save time by bypassing some route segments, and finally whether the improved route is free of conflicts with other aircraft. The method displays all direct-to eligible aircraft to an air traffic controller in a list sorted by highest time savings. By allowing the air traffic controller to easily identify and work with the highest pay-off aircraft, the method of the present invention contributes to a significant increase in both air traffic controller and aircraft productivity. A graphical computer interface (GUI) is used to enable the air traffic controller to send the aircraft direct to a waypoint or fix closer to the destination airport by a simple point and click action."
"NASA Tech Briefs, April 1995",47.08194,photo analysis invention,['Technology Utilization and Surface Transportation'],"This issue of the NASA Tech Briefs has a special focus section on video and imaging, a feature on the NASA invention of the year, and a resource report on the Dryden Flight Research Center. The issue also contains articles on electronic components and circuits, electronic systems, physical sciences, materials, computer programs, mechanics, machinery, manufacturing/fabrication, mathematics and information sciences and life sciences. In addition to the standard articles in the NASA Tech brief, this contains a supplement entitled ""Laser Tech Briefs"" which features an article on the National Ignition Facility, and other articles on the use of Lasers."
"Modeling of video traffic in packet networks, low rate video compression, and the development of a lossy+lossless image compression algorithm",131.22244,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this reporting period we have worked on three somewhat different problems. These are modeling of video traffic in packet networks, low rate video compression, and the development of a lossy + lossless image compression algorithm, which might have some application in browsing algorithms. The lossy + lossless scheme is an extension of work previously done under this grant. It provides a simple technique for incorporating browsing capability. The low rate coding scheme is also a simple variation on the standard discrete cosine transform (DCT) coding approach. In spite of its simplicity, the approach provides surprisingly high quality reconstructions. The modeling approach is borrowed from the speech recognition literature, and seems to be promising in that it provides a simple way of obtaining an idea about the second order behavior of a particular coding scheme. Details about these are presented."
Algorithm for Compressing Time-Series Data,115.292984,lossy algorithm,['Man/System Technology and Life Support'],"An algorithm based on Chebyshev polynomials effects lossy compression of time-series data or other one-dimensional data streams (e.g., spectral data) that are arranged in blocks for sequential transmission. The algorithm was developed for use in transmitting data from spacecraft scientific instruments to Earth stations. In spite of its lossy nature, the algorithm preserves the information needed for scientific analysis. The algorithm is computationally simple, yet compresses data streams by factors much greater than two. The algorithm is not restricted to spacecraft or scientific uses: it is applicable to time-series data in general. The algorithm can also be applied to general multidimensional data that have been converted to time-series data, a typical example being image data acquired by raster scanning. However, unlike most prior image-data-compression algorithms, this algorithm neither depends on nor exploits the two-dimensional spatial correlations that are generally present in images. In order to understand the essence of this compression algorithm, it is necessary to understand that the net effect of this algorithm and the associated decompression algorithm is to approximate the original stream of data as a sequence of finite series of Chebyshev polynomials. For the purpose of this algorithm, a block of data or interval of time for which a Chebyshev polynomial series is fitted to the original data is denoted a fitting interval. Chebyshev approximation has two properties that make it particularly effective for compressing serial data streams with minimal loss of scientific information: The errors associated with a Chebyshev approximation are nearly uniformly distributed over the fitting interval (this is known in the art as the ""equal error property""); and the maximum deviations of the fitted Chebyshev polynomial from the original data have the smallest possible values (this is known in the art as the ""min-max property"")."
Radiometric resolution enhancement by lossy compression as compared to truncation followed by lossless compression,110.465454,lossy algorithm,['COMPUTER SYSTEMS'],"Recent advances in imaging technology make it possible to obtain imagery data of the Earth at high spatial, spectral and radiometric resolutions from Earth orbiting satellites. The rate at which the data is collected from these satellites can far exceed the channel capacity of the data downlink. Reducing the data rate to within the channel capacity can often require painful trade-offs in which certain scientific returns are sacrificed for the sake of others. In this paper we model the radiometric version of this form of lossy compression by dropping a specified number of least significant bits from each data pixel and compressing the remaining bits using an appropriate lossless compression technique. We call this approach 'truncation followed by lossless compression' or TLLC. We compare the TLLC approach with applying a lossy compression technique to the data for reducing the data rate to the channel capacity, and demonstrate that each of three different lossy compression techniques (JPEG/DCT, VQ and Model-Based VQ) give a better effective radiometric resolution than TLLC for a given channel rate."
Neural Network Repair of Lossy Compression Artifacts in the Sept 2015  March 2016 Duration of the MMS/FPI Dataset,103.9132,lossy algorithm,['Instrumentation and Photography'],"During the Sept 2015 March 2016 duration (sometimes referred to as Phase 1A) of the Magnetospheric Multiscale Mission (MMS), the Dual Electron Spectrometers (DES) were configured to generously utilize lossy compression. While this maximized the number of velocity distribution functions downlinked, it cameat the expense of lost information content for a fractionof the frames. Following this period of lossy compression, the DES was re-configured in a way that allowed for 95% of the framesto arrive to the ground without loss. Using this high-quality set offrameson-orbit observations, we compressed and decompressed the frameson the ground to create a side-by-side record of the compression effect.  This record was used to drive an optimization method that (a) derived basis functions capable of approximating the lossless sample space and with non-negative coefficients and (b) fitted a function which maps the lossy framesto basis weights that recreate the framewithout compression artifacts. This methodis introduced and evaluated in this paper.Data users should expect a higher level of confidence in the absolute scale of density/temperature measurements andnotice less sinusoidal bias in the velocity X and Y components(GSE)."
"A Novel, Real-Valued Genetic Algorithm for Optimizing Radar Absorbing Materials",91.969666,lossy algorithm,['Electronics and Electrical Engineering'],"A novel, real-valued Genetic Algorithm (GA) was designed and implemented to minimize the reflectivity and/or transmissivity of an arbitrary number of homogeneous, lossy dielectric or magnetic layers of arbitrary thickness positioned at either the center of an infinitely long rectangular waveguide, or adjacent to the perfectly conducting backplate of a semi-infinite, shorted-out rectangular waveguide. Evolutionary processes extract the optimal physioelectric constants falling within specified constraints which minimize reflection and/or transmission over the frequency band of interest. This GA extracted the unphysical dielectric and magnetic constants of three layers of fictitious material placed adjacent to the conducting backplate of a shorted-out waveguide such that the reflectivity of the configuration was 55 dB or less over the entire X-band. Examples of the optimization of realistic multi-layer absorbers are also presented. Although typical Genetic Algorithms require populations of many thousands in order to function properly and obtain correct results, verified correct results were obtained for all test cases using this GA with a population of only four."
The Linear Bicharacteristic Scheme for Electromagnetics,89.45959,lossy algorithm,['Electronics and Electrical Engineering'],"The upwind leapfrog or Linear Bicharacteristic Scheme (LBS) has previously been implemented and demonstrated on electromagnetic wave propagation problems. This paper extends the Linear Bicharacteristic Scheme for computational electromagnetics to model lossy dielectric and magnetic materials and perfect electrical conductors. This is accomplished by proper implementation of the LBS for homogeneous lossy dielectric and magnetic media and for perfect electrical conductors. Heterogeneous media are modeled through implementation of surface boundary conditions and no special extrapolations or interpolations at dielectric material boundaries are required. Results are presented for one-dimensional model problems on both uniform and nonuniform grids, and the FDTD algorithm is chosen as a convenient reference algorithm for comparison. The results demonstrate that the explicit LBS is a dissipation-free, second-order accurate algorithm which uses a smaller stencil than the FDTD algorithm, yet it has approximately one-third the phase velocity error. The LBS is also more accurate on nonuniform grids."
The Linear Bicharacteristic Scheme for Computational Electromagnetics,88.71867,lossy algorithm,['Communications and Radar'],"The upwind leapfrog or Linear Bicharacteristic Scheme (LBS) has previously been implemented and demonstrated on electromagnetic wave propagation problems. This paper extends the Linear Bicharacteristic Scheme for computational electromagnetics to treat lossy dielectric and magnetic materials and perfect electrical conductors. This is accomplished by proper implementation of the LBS for homogeneous lossy dielectric and magnetic media, and treatment of perfect electrical conductors (PECs) are shown to follow directly in the limit of high conductivity. Heterogeneous media are treated through implementation of surface boundary conditions and no special extrapolations or interpolations at dielectric material boundaries are required. Results are presented for one-dimensional model problems on both uniform and nonuniform grids, and the FDTD algorithm is chosen as a convenient reference algorithm for comparison. The results demonstrate that the explicit LBS is a dissipation-free, second-order accurate algorithm which uses a smaller stencil than the FDTD algorithm, yet it has approximately one-third the phase velocity error. The LBS is also more accurate on nonuniform grids."
Image Compression Algorithm Altered to Improve Stereo Ranging,88.35544,lossy algorithm,['Man/System Technology and Life Support'],"A report discusses a modification of the ICER image-data-compression algorithm to increase the accuracy of ranging computations performed on compressed stereoscopic image pairs captured by cameras aboard the Mars Exploration Rovers. (ICER and variants thereof were discussed in several prior NASA Tech Briefs articles.) Like many image compressors, ICER was designed to minimize a mean-square-error measure of distortion in reconstructed images as a function of the compressed data volume. The present modification of ICER was preceded by formulation of an alternative error measure, an image-quality metric that focuses on stereoscopic-ranging quality and takes account of image-processing steps in the stereoscopic-ranging process. This metric was used in empirical evaluation of bit planes of wavelet-transform subbands that are generated in ICER. The present modification, which is a change in a bit-plane prioritization rule in ICER, was adopted on the basis of this evaluation. This modification changes the order in which image data are encoded, such that when ICER is used for lossy compression, better stereoscopic-ranging results are obtained as a function of the compressed data volume."
Subband coding for image data archiving,87.23629,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],The use of subband coding on image data is discussed. An overview of subband coding is given. Advantages of subbanding for browsing and progressive resolution are presented. Implementations for lossless and lossy coding are discussed. Algorithm considerations and simple implementations of subband systems are given.
Subband coding for image data archiving,86.98779,lossy algorithm,['COMMUNICATIONS AND RADAR'],The use of subband coding on image data is discussed. An overview of subband coding is given. Advantages of subbanding for browsing and progressive resolution are presented. Implementations for lossless and lossy coding are discussed. Algorithm considerations and simple implementations of subband are given.
A Two-Dimensional Linear Bicharacteristic Scheme for Electromagnetics,86.23184,lossy algorithm,['Electronics and Electrical Engineering'],"The upwind leapfrog or Linear Bicharacteristic Scheme (LBS) has previously been implemented and demonstrated on one-dimensional electromagnetic wave propagation problems. This memorandum extends the Linear Bicharacteristic Scheme for computational electromagnetics to model lossy dielectric and magnetic materials and perfect electrical conductors in two dimensions. This is accomplished by proper implementation of the LBS for homogeneous lossy dielectric and magnetic media and for perfect electrical conductors. Both the Transverse Electric and Transverse Magnetic polarizations are considered. Computational requirements and a Fourier analysis are also discussed. Heterogeneous media are modeled through implementation of surface boundary conditions and no special extrapolations or interpolations at dielectric material boundaries are required. Results are presented for two-dimensional model problems on uniform grids, and the Finite Difference Time Domain (FDTD) algorithm is chosen as a convenient reference algorithm for comparison. The results demonstrate that the two-dimensional explicit LBS is a dissipation-free, second-order accurate algorithm which uses a smaller stencil than the FDTD algorithm, yet it has less phase velocity error."
Parallel image compression,85.24039,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A parallel compression algorithm for the 16,384 processor MPP machine was developed. The serial version of the algorithm can be viewed as a combination of on-line dynamic lossless test compression techniques (which employ simple learning strategies) and vector quantization. These concepts are described. How these concepts are combined to form a new strategy for performing dynamic on-line lossy compression is discussed. Finally, the implementation of this algorithm in a massively parallel fashion on the MPP is discussed."
A Real-Time High Performance Data Compression Technique For Space Applications,83.87259,lossy algorithm,['Earth Resources and Remote Sensing'],A high performance lossy data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on block-transform combined with bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate. The lossy coder is described. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Hardware implementations are in development; a functional chip set is expected by the end of 2001.
Hybrid LZW compression,82.64607,lossy algorithm,['INSTRUMENTATION AND PHOTOGRAPHY'],"The Science Data Management and Science Payload Operations subpanel reports from the NASA Conference on Scientific Data Compression (Snowbird, Utah in 1988) indicate the need for both lossless and lossy image data compression systems. The ranges developed by the subpanel suggest ratios of 2:1 to 4:1 for lossless coding and 2:1 to 6:1 for lossy predictive coding. For the NASA Freedom Science Video Processing Facility it would be highly desirable to implement one baseline compression system which would meet both of these criteria. Presented here is such a system, utilizing an LZW hybrid coding scheme which is adaptable to either type of compression. Simulation results are presented with the hybrid LZW algorithm operating in each of its modes."
Implementation issues in source coding,77.87858,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],An edge preserving image coding scheme which can be operated in both a lossy and a lossless manner was developed. The technique is an extension of the lossless encoding algorithm developed for the Mars observer spectral data. It can also be viewed as a modification of the DPCM algorithm. A packet video simulator was also developed from an existing modified packet network simulator. The coding scheme for this system is a modification of the mixture block coding (MBC) scheme described in the last report. Coding algorithms for packet video were also investigated.
Soil Moisture Active Passive (SMAP) Project Algorithm Theoretical Basis Document SMAP L1B Radiometer Data Product: L1B_TB,77.12539,lossy algorithm,"['Meteorology and Climatology', 'Instrumentation and Photography']","The purpose of the Soil Moisture Active Passive (SMAP) radiometer calibration algorithm is to convert Level 0 (L0) radiometer digital counts data into calibrated estimates of brightness temperatures referenced to the Earth's surface within the main beam. The algorithm theory in most respects is similar to what has been developed and implemented for decades for other satellite radiometers; however, SMAP includes two key features heretofore absent from most satellite borne radiometers: radio frequency interference (RFI) detection and mitigation, and measurement of the third and fourth Stokes parameters using digital correlation. The purpose of this document is to describe the SMAP radiometer and forward model, explain the SMAP calibration algorithm, including approximations, errors, and biases, provide all necessary equations for implementing the calibration algorithm and detail the RFI detection and mitigation process. Section 2 provides a summary of algorithm objectives and driving requirements. Section 3 is a description of the instrument and Section 4 covers the forward models, upon which the algorithm is based. Section 5 gives the retrieval algorithm and theory. Section 6 describes the orbit simulator, which implements the forward model and is the key for deriving antenna pattern correction coefficients and testing the overall algorithm."
A Linear Bicharacteristic FDTD Method,77.07945,lossy algorithm,['Numerical Analysis'],"The linear bicharacteristic scheme (LBS) was originally developed to improve unsteady solutions in computational acoustics and aeroacoustics [1]-[7]. It is a classical leapfrog algorithm, but is combined with upwind bias in the spatial derivatives. This approach preserves the time-reversibility of the leapfrog algorithm, which results in no dissipation, and it permits more flexibility by the ability to adopt a characteristic based method. The use of characteristic variables allows the LBS to treat the outer computational boundaries naturally using the exact compatibility equations. The LBS offers a central storage approach with lower dispersion than the Yee algorithm, plus it generalizes much easier to nonuniform grids. It has previously been applied to two and three-dimensional freespace electromagnetic propagation and scattering problems [3], [6], [7]. This paper extends the LBS to model lossy dielectric and magnetic materials. Results are presented for several one-dimensional model problems, and the FDTD algorithm is chosen as a convenient reference for comparison."
Analysis of lossy composite terminating structures,76.76372,lossy algorithm,['COMMUNICATIONS AND RADAR'],A finite element solution and computer code for the electromagnetic scattering of inhomogeneous penetrable bodies is presented. The application for the code is for the analysis and design of leading and trailing edge terminations when conducting and nonconducting materials are used. Examples of simple triangular shaped terminations are also presented.
Analysis of Compression Algorithm in Ground Collision Avoidance Systems (Auto-GCAS),76.56914,lossy algorithm,['Air Transportation and Safety'],"Automatic Ground Collision Avoidance Systems (Auto-GCAS) utilizes Digital Terrain Elevation Data (DTED) stored onboard a plane to determine potential recovery maneuvers. Because of the current limitations of computer hardware on military airplanes such as the F-22 and F-35, the DTED must be compressed through a lossy technique called binary-tree tip-tilt. The purpose of this study is to determine the accuracy of the compressed data with respect to the original DTED. This study is mainly interested in the magnitude of the error between the two as well as the overall distribution of the errors throughout the DTED. By understanding how the errors of the compression technique are affected by various factors (topography, density of sampling points, sub-sampling techniques, etc.), modifications can be made to the compression technique resulting in better accuracy. This, in turn, would minimize unnecessary activation of A-GCAS during flight as well as maximizing its contribution to fighter safety."
Locally adaptive vector quantization: Data compression with feature preservation,75.62059,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
A Two-Dimensional Linear Bicharacteristic FDTD Method,74.466805,lossy algorithm,['Acoustics'],"The linear bicharacteristic scheme (LBS) was originally developed to improve unsteady solutions in computational acoustics and aeroacoustics. The LBS has previously been extended to treat lossy materials for one-dimensional problems. It is a classical leapfrog algorithm, but is combined with upwind bias in the spatial derivatives. This approach preserves the time-reversibility of the leapfrog algorithm, which results in no dissipation, and it permits more flexibility by the ability to adopt a characteristic based method. The use of characteristic variables allows the LBS to include the Perfectly Matched Layer boundary condition with no added storage or complexity. The LBS offers a central storage approach with lower dispersion than the Yee algorithm, plus it generalizes much easier to nonuniform grids. It has previously been applied to two and three-dimensional free-space electromagnetic propagation and scattering problems. This paper extends the LBS to the two-dimensional case. Results are presented for point source radiation problems, and the FDTD algorithm is chosen as a convenient reference for comparison."
SMMR Simulator radiative transfer calibration model.  2:  Algorithm development,73.276215,lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],Passive microwave measurements performed from Earth orbit can be used to provide global data on a wide range of geophysical and meteorological phenomena. A Scanning Multichannel Microwave Radiometer (SMMR) is being flown on the Nimbus-G satellite. The SMMR Simulator duplicates the frequency bands utilized in the spacecraft instruments through an amalgamate of radiometer systems. The algorithm developed utilizes data from the fall 1978 NASA CV-990 Nimbus-G underflight test series and subsequent laboratory testing.
The effect of lossy image compression on image classification,72.66734,lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
On the Treatment of Electric and Magnetic Loss in the Linear Bicharacteristic Scheme for Electromagnetics,72.57823,lossy algorithm,['Communications and Radar'],The upwind leapfrog or Linear Bicharacteristic Scheme (LBS) has previously been extended to treat lossy dielectric and magnetic materials. This paper examines different methodologies for treatment of the electric loss term in the Linear Bicharacteristic Scheme for computational electromagnetics. Several different treatments of the electric loss term using the LBS are explored and compared on one-dimensional model problems involving reflection from lossy dielectric materials on both uniform and nonuniform grids. Results using these LBS implementations are also compared with the FDTD method for convenience.
The New CCSDS Image Compression Recommendation,71.3647,lossy algorithm,['Instrumentation and Photography'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-Earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An Application-Specific Integrated Circuit (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images."
Finite difference time domain electromagnetic scattering from frequency-dependent lossy materials,71.11424,lossy algorithm,['COMMUNICATIONS AND RADAR'],"Four different FDTD computer codes and companion Radar Cross Section (RCS) conversion codes on magnetic media are submitted. A single three dimensional dispersive FDTD code for both dispersive dielectric and magnetic materials was developed, along with a user's manual. The extension of FDTD to more complicated materials was made. The code is efficient and is capable of modeling interesting radar targets using a modest computer workstation platform. RCS results for two different plate geometries are reported. The FDTD method was also extended to computing far zone time domain results in two dimensions. Also the capability to model nonlinear materials was incorporated into FDTD and validated."
Resiliency of the Multiscale Retinex Image Enhancement Algorithm,70.87444,lossy algorithm,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) continues to prove itself in extensive testing to be very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition, However, issues remain with regard to the resiliency of the MSRCR to different image sources and arbitrary image manipulations which may have been applied prior to retinex processing. In this paper we define these areas of concern, provide experimental results, and, examine the effects of commonly occurring image manipulation on retinex performance. In virtually all cases the MSRCR is highly resilient to the effects of both the image source variations and commonly encountered prior image-processing. Significant artifacts are primarily observed for the case of selective color channel clipping in large dark zones in a image. These issues are of concerning the processing of digital image archives and other applications where there is neither control over the image acquisition process, nor knowledge about any processing done on th data beforehand."
Reflection Coefficients on Surfaces of Different Periodic Structure,69.73355,lossy algorithm,['Optics'],"Diffraction properties of lossy periodic gratings with the metal base were investigated by solving Maxwell's equations numerically using the differential method. Two periodic surfaces were employed in the simulation: triangle structure and tilting triangle structure. Based on the numerical solution and in conjunction with the algorithm of Adams-Moulton, we computed reflection coefficients of plane waves with different wavelengths and different incident angles. The dielectric properties were also explored using various dielectric constants. The results show that the reflection coefficients of both TE and TM waves are quite sensitive to the incident angles of the plane waves when the metal sheet exists, which is in good agreement with the experimental data."
Antenna pattern control using impedance surfaces,69.0503,lossy algorithm,['COMMUNICATIONS AND RADAR'],"During this research period, we have effectively transferred existing computer codes from CRAY supercomputer to work station based systems. The work station based version of our code preserved the accuracy of the numerical computations while giving a much better turn-around time than the CRAY supercomputer. Such a task relieved us of the heavy dependence of the supercomputer account budget and made codes developed in this research project more feasible for applications. The analysis of pyramidal horns with impedance surfaces was our major focus during this research period. Three different modeling algorithms in analyzing lossy impedance surfaces were investigated and compared with measured data. Through this investigation, we discovered that a hybrid Fourier transform technique, which uses the eigen mode in the stepped waveguide section and the Fourier transformed field distributions across the stepped discontinuities for lossy impedances coating, gives a better accuracy in analyzing lossy coatings. After a further refinement of the present technique, we will perform an accurate radiation pattern synthesis in the coming reporting period."
Real-time demonstration hardware for enhanced DPCM video compression algorithm,67.71493,lossy algorithm,['COMMUNICATIONS AND RADAR'],"The lack of available wideband digital links as well as the complexity of implementation of bandwidth efficient digital video CODECs (encoder/decoder) has worked to keep the cost of digital television transmission too high to compete with analog methods. Terrestrial and satellite video service providers, however, are now recognizing the potential gains that digital video compression offers and are proposing to incorporate compression systems to increase the number of available program channels. NASA is similarly recognizing the benefits of and trend toward digital video compression techniques for transmission of high quality video from space and therefore, has developed a digital television bandwidth compression algorithm to process standard National Television Systems Committee (NTSC) composite color television signals. The algorithm is based on differential pulse code modulation (DPCM), but additionally utilizes a non-adaptive predictor, non-uniform quantizer and multilevel Huffman coder to reduce the data rate substantially below that achievable with straight DPCM. The non-adaptive predictor and multilevel Huffman coder combine to set this technique apart from other DPCM encoding algorithms. All processing is done on a intra-field basis to prevent motion degradation and minimize hardware complexity. Computer simulations have shown the algorithm will produce broadcast quality reconstructed video at an average transmission rate of 1.8 bits/pixel. Hardware implementation of the DPCM circuit, non-adaptive predictor and non-uniform quantizer has been completed, providing realtime demonstration of the image quality at full video rates. Video sampling/reconstruction circuits have also been constructed to accomplish the analog video processing necessary for the real-time demonstration. Performance results for the completed hardware compare favorably with simulation results. Hardware implementation of the multilevel Huffman encoder/decoder is currently under development along with implementation of a buffer control algorithm to accommodate the variable data rate output of the multilevel Huffman encoder. A video CODEC of this type could be used to compress NTSC color television signals where high quality reconstruction is desirable (e.g., Space Station video transmission, transmission direct-to-the-home via direct broadcast satellite systems or cable television distribution to system headends and direct-to-the-home)."
Context Modeler for Wavelet Compression of Spectral Hyperspectral Images,66.9487,lossy algorithm,['Man/System Technology and Life Support'],"A context-modeling sub-algorithm has been developed as part of an algorithm that effects three-dimensional (3D) wavelet-based compression of hyperspectral image data. The context-modeling subalgorithm, hereafter denoted the context modeler, provides estimates of probability distributions of wavelet-transformed data being encoded. These estimates are utilized by an entropy coding subalgorithm that is another major component of the compression algorithm. The estimates make it possible to compress the image data more effectively than would otherwise be possible. The following background discussion is prerequisite to a meaningful summary of the context modeler. This discussion is presented relative to ICER-3D, which is the name attached to a particular compression algorithm and the software that implements it. The ICER-3D software is summarized briefly in the preceding article, ICER-3D Hyperspectral Image Compression Software (NPO-43238). Some aspects of this algorithm were previously described, in a slightly more general context than the ICER-3D software, in ""Improving 3D Wavelet-Based Compression of Hyperspectral Images"" (NPO-41381), NASA Tech Briefs, Vol. 33, No. 3 (March 2009), page 7a. In turn, ICER-3D is a product of generalization of ICER, another previously reported algorithm and computer program that can perform both lossless and lossy wavelet-based compression and decompression of gray-scale-image data. In ICER-3D, hyperspectral image data are decomposed using a 3D discrete wavelet transform (DWT). Following wavelet decomposition, mean values are subtracted from spatial planes of spatially low-pass subbands prior to encoding. The resulting data are converted to sign-magnitude form and compressed. In ICER-3D, compression is progressive, in that compressed information is ordered so that as more of the compressed data stream is received, successive reconstructions of the hyperspectral image data are of successively higher overall fidelity."
Compressing Image Data While Limiting the Effects of Data Losses,66.33602,lossy algorithm,['Man/System Technology and Life Support'],"ICER is computer software that can perform both lossless and lossy compression and decompression of gray-scale-image data using discrete wavelet transforms. Designed for primary use in transmitting scientific image data from distant spacecraft to Earth, ICER incorporates an error-containment scheme that limits the adverse effects of loss of data and is well suited to the data packets transmitted by deep-space probes. The error-containment scheme includes utilization of the algorithm described in ""Partitioning a Gridded Rectangle Into Smaller Rectangles "" (NPO-30479), NASA Tech Briefs, Vol. 28, No. 7 (July 2004), page 56. ICER has performed well in onboard compression of thousands of images transmitted from the Mars Exploration Rovers."
High-performance compression of astronomical images,65.99281,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Astronomical images have some rather unusual characteristics that make many existing image compression techniques either ineffective or inapplicable. A typical image consists of a nearly flat background sprinkled with point sources and occasional extended sources. The images are often noisy, so that lossless compression does not work very well; furthermore, the images are usually subjected to stringent quantitative analysis, so any lossy compression method must be proven not to discard useful information, but must instead discard only the noise. Finally, the images can be extremely large. For example, the Space Telescope Science Institute has digitized photographic plates covering the entire sky, generating 1500 images each having 14000 x 14000 16-bit pixels. Several astronomical groups are now constructing cameras with mosaics of large CCD's (each 2048 x 2048 or larger); these instruments will be used in projects that generate data at a rate exceeding 100 MBytes every 5 minutes for many years. An effective technique for image compression may be based on the H-transform (Fritze et al. 1977). The method that we have developed can be used for either lossless or lossy compression. The digitized sky survey images can be compressed by at least a factor of 10 with no noticeable losses in the astrometric and photometric properties of the compressed images. The method has been designed to be computationally efficient: compression or decompression of a 512 x 512 image requires only 4 seconds on a Sun SPARCstation 1. The algorithm uses only integer arithmetic, so it is completely reversible in its lossless mode, and it could easily be implemented in hardware for space applications."
Measurement of the properties of lossy materials inside a finite conducting cylinder,65.00731,lossy algorithm,['COMMUNICATIONS AND RADAR'],A computer code was developed to automatically perform swept frequency reflection and transmission measurements using a HP5510B Network Analyzer and computer. This software is used in conjunction with a modified high temperature test rig to obtain reflection measurements from a flat material sample. The software allows data processing to eliminate measurement errors and to obtain a reflection coefficient in the frequency or time domain. A description of the program is presented.
A Model-Based Probabilistic Inversion Framework for Wire Fault Detection Using TDR,64.42388,lossy algorithm,['Electronics and Electrical Engineering'],"Time-domain reflectometry (TDR) is one of the standard methods for diagnosing faults in electrical wiring and interconnect systems, with a long-standing history focused mainly on hardware development of both high-fidelity systems for laboratory use and portable hand-held devices for field deployment. While these devices can easily assess distance to hard faults such as sustained opens or shorts, their ability to assess subtle but important degradation such as chafing remains an open question. This paper presents a unified framework for TDR-based chafing fault detection in lossy coaxial cables by combining an S-parameter based forward modeling approach with a probabilistic (Bayesian) inference algorithm. Results are presented for the estimation of nominal and faulty cable parameters from laboratory data."
A fixed/variable bit-rate data compression architecture,63.67308,lossy algorithm,['COMPUTER OPERATIONS AND HARDWARE'],"A VLSI architecture for an adaptive data compression encoder capable of sustaining fixed or variable bit-rate output has been developed. There are three modes of operation: lossless with variable bit-rate, lossy with fixed bit-rate and lossy with variable bit-rate. For lossless encoding, the implementation is identical to the USES chip designed for Landsat 7. Obtaining a fixed bit-rate is achieved with a lossy DPCM algorithm using adaptive, nonuniform scalar quantization. In lossy mode, variable bit-rate coding uses the lossless sections of the encoder for post-DPCM entropy coding. The encoder shows excellent compression performance in comparison to other current data compression techniques. No external tables or memory are required for operation."
Compression through decomposition into browse and residual images,62.27288,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Economical archival and retrieval of image data is becoming increasingly important considering the unprecedented data volumes expected from the Earth Observing System (EOS) instruments. For cost effective browsing the image data (possibly from remote site), and retrieving the original image data from the data archive, we suggest an integrated image browse and data archive system employing incremental transmission. We produce our browse image data with the JPEG/DCT lossy compression approach. Image residual data is then obtained by taking the pixel by pixel differences between the original data and the browse image data. We then code the residual data with a form of variable length coding called diagonal coding. In our experiments, the JPEG/DCT is used at different quality factors (Q) to generate the browse and residual data. The algorithm has been tested on band 4 of two Thematic mapper (TM) data sets. The best overall compression ratios (of about 1.7) were obtained when a quality factor of Q=50 was used to produce browse data at a compression ratio of 10 to 11. At this quality factor the browse image data has virtually no visible distortions for the images tested."
Finite difference time domain implementation of surface impedance boundary conditions,61.640633,lossy algorithm,['COMMUNICATIONS AND RADAR'],"Surface impedance boundary conditions are employed to reduce the solution volume during the analysis of scattering from lossy dielectric objects. In a finite difference solution, they also can be utilized to avoid using small cells, made necessary by shorter wavelengths in conducting media throughout the solution volume. The standard approach is to approximate the surface impedance over a very small bandwidth by its value at the center frequency, and then use that result in the boundary condition. Two implementations of the surface impedance boundary condition are presented. One implementation is a constant surface impedance boundary condition and the other is a dispersive surface impedance boundary condition that is applicable over a very large frequency bandwidth and over a large range of conductivities. Frequency domain results are presented in one dimension for two conductivity values and are compared with exact results. Scattering width results from an infinite square cylinder are presented as a 2-D demonstration. Extensions to 3-D should be straightforward."
Implementation of interconnect simulation tools in spice,60.994293,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Accurate computer simulation of high speed digital computer circuits and communication circuits requires a multimode approach to simulate both the devices and the interconnects between devices. Classical circuit analysis algorithms (lumped parameter) are needed for circuit devices and the network formed by the interconnected devices. The interconnects, however, have to be modeled as transmission lines which incorporate electromagnetic field analysis. An approach to writing a multimode simulator is to take an existing software package which performs either lumped parameter analysis or field analysis and add the missing type of analysis routines to the package. In this work a traditionally lumped parameter simulator, SPICE, is modified so that it will perform lossy transmission line analysis using a different model approach. Modifying SPICE3E2 or any other large software package is not a trivial task. An understanding of the programming conventions used, simulation software, and simulation algorithms is required. This thesis was written to clarify the procedure for installing a device into SPICE3E2. The installation of three devices is documented and the installations of the first two provide a foundation for installation of the lossy line which is the third device. The details of discussions are specific to SPICE, but the concepts will be helpful when performing installations into other circuit analysis packages."
Planning/scheduling techniques for VQ-based image compression,60.49807,lossy algorithm,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
ICER-3D Hyperspectral Image Compression Software,60.493538,lossy algorithm,['Man/System Technology and Life Support'],"Software has been developed to implement the ICER-3D algorithm. ICER-3D effects progressive, three-dimensional (3D), wavelet-based compression of hyperspectral images. If a compressed data stream is truncated, the progressive nature of the algorithm enables reconstruction of hyperspectral data at fidelity commensurate with the given data volume. The ICER-3D software is capable of providing either lossless or lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The compression algorithm, which was derived from the ICER image compression algorithm, includes wavelet-transform, context-modeling, and entropy coding subalgorithms. The 3D wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of sets of hyperspectral image data, while facilitating elimination of spectral ringing artifacts, using a technique summarized in ""Improving 3D Wavelet-Based Compression of Spectral Images"" (NPO-41381), NASA Tech Briefs, Vol. 33, No. 3 (March 2009), page 7a. Correlation is further exploited by a context-modeling subalgorithm, which exploits spectral dependencies in the wavelet-transformed hyperspectral data, using an algorithm that is summarized in ""Context Modeler for Wavelet Compression of Hyperspectral Images"" (NPO-43239), which follows this article. An important feature of ICER-3D is a scheme for limiting the adverse effects of loss of data during transmission. In this scheme, as in the similar scheme used by ICER, the spatial-frequency domain is partitioned into rectangular error-containment regions. In ICER-3D, the partitions extend through all the wavelength bands. The data in each partition are compressed independently of those in the other partitions, so that loss or corruption of data from any partition does not affect the other partitions. Furthermore, because compression is progressive within each partition, when data are lost, any data from that partition received prior to the loss can be used to reconstruct that partition at lower fidelity. By virtue of the compression improvement it achieves relative to previous means of onboard data compression, this software enables (1) increased return of hyperspectral scientific data in the presence of limits on the rates of transmission of data from spacecraft to Earth via radio communication links and/or (2) reduction in spacecraft radio-communication power and/or cost through reduction in the amounts of data required to be downlinked and stored onboard prior to downlink. The software is also suitable for compressing hyperspectral images for ground storage or archival purposes."
A Numerical Analysis of Electromagnetic Scattering from Two-Dimensional Edge Terminations,59.93453,lossy algorithm,['Communications and Radar'],"Several techniques that influence the low frequency scattering from penetrable edge terminations are evaluated using a hybrid finite element and boundary element method code. The edge terminations consist of a dielectric skin forming an exterior shape with an internal conducting bulkhead. Some of the techniques considered are bulkhead shaping, internal material loading, placement of resistive cards, and the placement of lossy dielectric material rods. The intent of the various treatments is to find a combination or combinations that influence(s) the backscattered field to acceptable levels over a range of frequencies for both transverse magnetic and transverse electric polarizations."
Emerging standards for still image compression: A software implementation and simulation study,59.491257,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],The software implementation is described of an emerging standard for the lossy compression of continuous tone still images. This software program can be used to compress planetary images and other 2-D instrument data. It provides a high compression image coding capability that preserves image fidelity at compression rates competitive or superior to most known techniques. This software implementation confirms the usefulness of such data compression and allows its performance to be compared with other schemes used in deep space missions and for data based storage.
"Adjustable lossless image compression based on a natural splitting of an image into drawing, shading, and fine-grained components",58.569332,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The compression, or efficient coding, of single band or multispectral still images is becoming an increasingly important topic. While lossy compression approaches can produce reconstructions that are visually close to the original, many scientific and engineering applications require exact (lossless) reconstructions. However, the most popular and efficient lossless compression techniques do not fully exploit the two-dimensional structural links existing in the image data. We describe here a general approach to lossless data compression that effectively exploits two-dimensional structural links of any length. After describing in detail two main variants on this scheme, we discuss experimental results."
Data Compression Techniques for Advanced Space Transportation Systems,58.42115,lossy algorithm,['Documentation and Information Science'],"Advanced space transportation systems, including vehicle state of health systems, will produce large amounts of data which must be stored on board the vehicle and or transmitted to the ground and stored. The cost of storage or transmission of the data could be reduced if the number of bits required to represent the data is reduced by the use of data compression techniques. Most of the work done in this study was rather generic and could apply to many data compression systems, but the first application area to be considered was launch vehicle state of health telemetry systems. Both lossless and lossy compression techniques were considered in this study."
The CCDS Data Compression Recommendations: Development and Status,57.41395,lossy algorithm,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
Compression of color-mapped images,56.806786,lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,56.698467,lossy algorithm,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,55.88876,lossy algorithm,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Technology Directions for the 21st Century,55.454483,lossy algorithm,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
Entropy reduction via simplified image contourization,55.428986,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The process of contourization is presented which converts a raster image into a set of plateaux or contours. These contours can be grouped into a hierarchical structure, defining total spatial inclusion, called a contour tree. A contour coder has been developed which fully describes these contours in a compact and efficient manner and is the basis for an image compression method. Simplification of the contour tree has been undertaken by merging contour tree nodes thus lowering the contour tree's entropy. This can be exploited by the contour coder to increase the image compression ratio. By applying general and simple rules derived from physiological experiments on the human vision system, lossy image compression can be achieved which minimizes noticeable artifacts in the simplified image."
Toward an image compression algorithm for the high-resolution electronic still camera,54.241417,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Taking pictures with a camera that uses a digital recording medium instead of film has the advantage of recording and transmitting images without the use of a darkroom or a courier. However, high-resolution images contain an enormous amount of information and strain data-storage systems. Image compression will allow multiple images to be stored in the High-Resolution Electronic Still Camera. The camera is under development at Johnson Space Center. Fidelity of the reproduced image and compression speed are of tantamount importance. Lossless compression algorithms are fast and faithfully reproduce the image, but their compression ratios will be unacceptably low due to noise in the front end of the camera. Future efforts will include exploring methods that will reduce the noise in the image and increase the compression ratio."
Technique for Performing Dielectric Property Measurements at Microwave Frequencies,53.908268,lossy algorithm,['Man/System Technology and Life Support'],"A paper discusses the need to perform accurate dielectric property measurements on larger sized samples, particularly liquids at microwave frequencies. These types of measurements cannot be obtained using conventional cavity perturbation methods, particularly for liquids or powdered or granulated solids that require a surrounding container. To solve this problem, a model has been developed for the resonant frequency and quality factor of a cylindrical microwave cavity containing concentric cylindrical samples. This model can then be inverted to obtain the real and imaginary dielectric constants of the material of interest. This approach is based on using exact solutions to Maxwell s equations for the resonant properties of a cylindrical microwave cavity and also using the effective electrical conductivity of the cavity walls that is estimated from the measured empty cavity quality factor. This new approach calculates the complex resonant frequency and associated electromagnetic fields for a cylindrical microwave cavity with lossy walls that is loaded with concentric, axially aligned, lossy dielectric cylindrical samples. In this approach, the calculated complex resonant frequency, consisting of real and imaginary parts, is related to the experimentally measured quantities. Because this approach uses Maxwell's equations to determine the perturbed electromagnetic fields in the cavity with the material(s) inserted, one can calculate the expected wall losses using the fields for the loaded cavity rather than just depending on the value of the fields obtained from the empty cavity quality factor. These additional calculations provide a more accurate determination of the complex dielectric constant of the material being studied. The improved approach will be particularly important when working with larger samples or samples with larger dielectric constants that will further perturb the cavity electromagnetic fields. Also, this approach enables the ability to have a larger sample of interest, such as a liquid or powdered or granulated solid, inside a cylindrical container."
Finite difference time domain implementation of surface impedance boundary conditions,53.22733,lossy algorithm,['COMMUNICATIONS AND RADAR'],"Surface impedance boundary conditions are employed to reduce the solution volume during the analysis of scattering from lossy dielectric objects. In the finite difference solution, they also can be utilized to avoid using small cells, made necessary by shorter wavelengths in conducting media throughout the solution volume. The standard approach is to approximate the surface impedance over a very small bandwidth by its value at the center frequency, and then use that result in the boundary condition. Here, two implementations of the surface impedance boundary condition are presented. One implementation is a constant surface impedance boundary condition and the other is a dispersive surface impedance boundary condition that is applicable over a very large frequency bandwidth and over a large range of conductivities. Frequency domain results are presented in one dimension for two conductivity values and are compared with exact results. Scattering width results from an infinite square cylinder are presented as a two dimensional demonstration. Extensions to three dimensions should be straightforward."
Pre-coding method and apparatus for multiple source or time-shifted single source data and corresponding inverse post-decoding method and apparatus,53.1502,lossy algorithm,['Computer Systems'],"A pre-coding method and device for improving data compression performance by removing correlation between a first original data set and a second original data set, each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference data set. The method and device produce a double-difference data set, i.e., an adjacent-delta calculation performed on a cross-delta data set or a cross-delta calculation performed on two adjacent-delta data sets, from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted data sets coming from a single source. The resulting double-difference data set is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original data set having been represented by such a double-difference data set."
Pre-coding method and apparatus for multiple source or time-shifted single source data and corresponding inverse post-decoding method and apparatus,53.074474,lossy algorithm,['Computer Operations and Hardware'],"A pre-coding method and device for improving data compression performance by removing correlation between a first original data set and a second original data set, each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference data set. The method and device produce a double-difference data set, i.e., an adjacent-delta calculation performed on a cross-delta data set or a cross-delta calculation performed on two adjacent-delta data sets, from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted data sets coming from a single source. The resulting double-difference data set is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original data set having been represented by such a double-difference data set."
Lossless compression of image data products on th e FIFE CD-ROM series,52.909863,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"How do you store enough of the key data sets, from a total of 120 gigabytes of data collected for a scientific experiment, on a collection of CD-ROM's, small enough to distribute to a broad scientific community? In such an application where information loss in unacceptable, lossless compression algorithms are the only choice. Although lossy compression algorithms can provide an order of magnitude improvement in compression ratios over lossless algorithms the information that is lost is often part of the key scientific precision of the data. Therefore, lossless compression algorithms are and will continue to be extremely important in minimizing archiving storage requirements and distribution of large earth and space (ESS) data sets while preserving the essential scientific precision of the data."
Data compression for near Earth and deep space to Earth transmission,52.260864,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Key issues of data compression for near Earth and deep space to Earth transmission discussion group are briefly presented. Specific recommendations as made by the group are as follows: (1) since data compression is a cost effective way to improve communications and storage capacity, NASA should use lossless data compression wherever possible; (2) NASA should conduct experiments and studies on the value and effectiveness of lossy data compression; (3) NASA should develop and select approaches to high ratio compression of operational data such as voice and video; (4) NASA should develop data compression integrated circuits for a few key approaches identified in the preceding recommendation; (5) NASA should examine new data compression approaches such as combining source and channel encoding, where high payoff gaps are identified in currently available schemes; and (6) users and developers of data compression technologies should be in closer communication within NASA and with academia, industry, and other government agencies."
Effects of Tunable Data Compression on Geophysical Products Retrieved from Surface Radar Observations with Applications to Spaceborne Meteorological Radars,52.007736,lossy algorithm,['Earth Resources and Remote Sensing'],"This paper presents results and analyses of applying an international space data compression standard to weather radar measurements that can easily span 8 orders of magnitude and typically require a large storage capacity as well as significant bandwidth for transmission. By varying the degree of the data compression, we analyzed the non-linear response of models that relate measured radar reflectivity and/or Doppler spectra to the moments and properties of the particle size distribution characterizing clouds and precipitation. Preliminary results for the meteorologically important phenomena of clouds and light rain indicate that for a 0.5 dB calibration uncertainty, typical for the ground-based pulsed-Doppler 94 GHz (or 3.2 mm, W-band) weather radar used as a proxy for spaceborne radar in this study, a lossless compression ratio of only 1.2 is achievable. However, further analyses of the non-linear response of various models of rainfall rate, liquid water content and median volume diameter show that a lossy data compression ratio exceeding 15 is realizable. The exploratory analyses presented are relevant to future satellite missions, where the transmission bandwidth is premium and storage requirements of vast volumes of data, potentially problematic."
High Performance Compression of Science Data,50.387375,lossy algorithm,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
The New CCSDS Image Compression Recommendation,49.433243,lossy algorithm,['Mathematical and Computer Sciences (General)'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists a two dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An ASIC implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm."
"User's manual for CBS3DS, version 1.0",48.905197,lossy algorithm,['COMMUNICATIONS AND RADAR'],"CBS3DS is a computer code written in FORTRAN 77 to compute the backscattering radar cross section of cavity backed apertures in infinite ground plane and slots in thick infinite ground plane. CBS3DS implements the hybrid Finite Element Method (FEM) and Method of Moments (MoM) techniques. This code uses the tetrahedral elements, with vector edge basis functions for FEM in the volume of the cavity/slot and the triangular elements with the basis functions for MoM at the apertures. By virtue of FEM, this code can handle any arbitrarily shaped three-dimensional cavities filled with inhomogeneous lossy materials; due to MoM, the apertures can be of any arbitrary shape. The User's Manual is written to make the user acquainted with the operation of the code. The user is assumed to be familiar with the FORTRAN 77 language and the operating environment of the computer the code is intended to run."
An Implicit LU/AF FDTD Method,48.78147,lossy algorithm,['Mathematical and Computer Sciences (General)'],"There has been some recent work to develop two and three-dimensional alternating direction implicit (ADI) FDTD schemes. These ADI schemes are based upon the original ADI concept developed by Peaceman and Rachford and Douglas and Gunn, which is a popular solution method in Computational Fluid Dynamics (CFD). These ADI schemes work well and they require solution of a tridiagonal system of equations. A new approach proposed in this paper applies a LU/AF approximate factorization technique from CFD to Maxwell s equations in flux conservative form for one space dimension. The result is a scheme that will retain its unconditional stability in three space dimensions, but does not require the solution of tridiagonal systems. The theory for this new algorithm is outlined in a one-dimensional context for clarity. An extension to two and threedimensional cases is discussed. Results of Fourier analysis are discussed for both stability and dispersion/damping properties of the algorithm. Results are presented for a one-dimensional model problem, and the explicit FDTD algorithm is chosen as a convenient reference for comparison."
An Implicit Characteristic Based Method for Electromagnetics,48.687275,lossy algorithm,['Electronics and Electrical Engineering'],"An implicit characteristic-based approach for numerical solution of Maxwell's time-dependent curl equations in flux conservative form is introduced. This method combines a characteristic based finite difference spatial approximation with an implicit lower-upper approximate factorization (LU/AF) time integration scheme. This approach is advantageous for three-dimensional applications because the characteristic differencing enables a two-factor approximate factorization that retains its unconditional stability in three space dimensions, and it does not require solution of tridiagonal systems. Results are given both for a Fourier analysis of stability, damping and dispersion properties, and for one-dimensional model problems involving propagation and scattering for free space and dielectric materials using both uniform and nonuniform grids. The explicit Finite Difference Time Domain Method (FDTD) algorithm is used as a convenient reference algorithm for comparison. The one-dimensional results indicate that for low frequency problems on a highly resolved uniform or nonuniform grid, this LU/AF algorithm can produce accurate solutions at Courant numbers significantly greater than one, with a corresponding improvement in efficiency for simulating a given period of time. This approach appears promising for development of dispersion optimized LU/AF schemes for three dimensional applications."
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,48.559128,lossy algorithm,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
Unsteady Analysis of Inlet-Compressor Acoustic Interactions Using Coupled 3-D and 1-D CFD Codes,48.105217,lossy algorithm,['Aircraft Propulsion and Power'],"It is well known that the dynamic response of a mixed compression supersonic inlet is very sensitive to the boundary condition imposed at the subsonic exit (engine face) of the inlet. In previous work, a 3-D computational fluid dynamics (CFD) inlet code (NPARC) was coupled at the engine face to a 3-D turbomachinery code (ADPAC) simulating an isolated rotor and the coupled simulation used to study the unsteady response of the inlet. The main problem with this approach is that the high fidelity turbomachinery simulation becomes prohibitively expensive as more stages are included in the simulation. In this paper, an alternative approach is explored, wherein the inlet code is coupled to a lesser fidelity 1-D transient compressor code (DYNTECC) which simulates the whole compressor. The specific application chosen for this evaluation is the collapsing bump experiment performed at the University of Cincinnati, wherein reflections of a large-amplitude acoustic pulse from a compressor were measured. The metrics for comparison are the pulse strength (time integral of the pulse amplitude) and wave form (shape). When the compressor is modeled by stage characteristics the computed strength is about ten percent greater than that for the experiment, but the wave shapes are in poor agreement. An alternate approach that uses a fixed rise in duct total pressure and temperature (so-called 'lossy' duct) to simulate a compressor gives good pulse shapes but the strength is about 30 percent low."
System considerations for efficient communication and storage of MSTI image data,47.800682,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
User's Manual for FEMOM3DR,47.27992,lossy algorithm,['Communications and Radar'],"FEMoM3DR is a computer code written in FORTRAN 77 to compute radiation characteristics of antennas on 3D body using combined Finite Element Method (FEM)/Method of Moments (MoM) technique. The code is written to handle different feeding structures like coaxial line, rectangular waveguide, and circular waveguide. This code uses the tetrahedral elements, with vector edge basis functions for FEM and triangular elements with roof-top basis functions for MoM. By virtue of FEM, this code can handle any arbitrary shaped three dimensional bodies with inhomogeneous lossy materials; and due to MoM the computational domain can be terminated in any arbitrary shape. The User's Manual is written to make the user acquainted with the operation of the code. The user is assumed to be familiar with the FORTRAN 77 language and the operating environment of the computers on which the code is intended to run."
Generalized three-dimensional experimental lightning code (G3DXL) user's manual,46.48211,lossy algorithm,['METEOROLOGY AND CLIMATOLOGY'],"Information concerning the programming, maintenance and operation of the G3DXL computer program is presented and the theoretical basis for the code is described. The program computes time domain scattering fields and surface currents and charges induced by a driving function on and within a complex scattering object which may be perfectly conducting or a lossy dielectric. This is accomplished by modeling the object with cells within a three-dimensional, rectangular problem space, enforcing the appropriate boundary conditions and differencing Maxwell's equations in time. In the present version of the program, the driving function can be either the field radiated by a lightning strike or a direct lightning strike. The F-106 B aircraft is used as an example scattering object."
"Development of Fast Algorithms Using Recursion, Nesting and Iterations for Computational Electromagnetics",46.398464,lossy algorithm,['Computer Programming and Software'],"In the first phase of our work, we have concentrated on laying the foundation to develop fast algorithms, including the use of recursive structure like the recursive aggregate interaction matrix algorithm (RAIMA), the nested equivalence principle algorithm (NEPAL), the ray-propagation fast multipole algorithm (RPFMA), and the multi-level fast multipole algorithm (MLFMA). We have also investigated the use of curvilinear patches to build a basic method of moments code where these acceleration techniques can be used later. In the second phase, which is mainly reported on here, we have concentrated on implementing three-dimensional NEPAL on a massively parallel machine, the Connection Machine CM-5, and have been able to obtain some 3D scattering results. In order to understand the parallelization of codes on the Connection Machine, we have also studied the parallelization of 3D finite-difference time-domain (FDTD) code with PML material absorbing boundary condition (ABC). We found that simple algorithms like the FDTD with material ABC can be parallelized very well allowing us to solve within a minute a problem of over a million nodes. In addition, we have studied the use of the fast multipole method and the ray-propagation fast multipole algorithm to expedite matrix-vector multiplication in a conjugate-gradient solution to integral equations of scattering. We find that these methods are faster than LU decomposition for one incident angle, but are slower than LU decomposition when many incident angles are needed as in the monostatic RCS calculations."
Image Compression Devices,46.07588,lossy algorithm,['Computer Programming and Software'],"The Rice algorithm is a ""lossless"" compression algorithm; it takes an image or other data that has been broken down into short strings of digital data, then processes each string mathematically to reduce the amount of memory required to store or transmit them. It is particularly useful in medical, scientific or engineering applications where all data must be preserved. Originally developed at Jet Propulsion Laboratory, the technology is marketed by Advanced Hardware Architectures, a company started by a former employee of the NASA Microelectronics Research Center."
Dielectric Heaters for Testing Spacecraft Nuclear Reactors,45.466385,lossy algorithm,['Man/System Technology and Life Support'],"A document proposes the development of radio-frequency-(RF)-driven dielectric heaters for non-nuclear thermal testing of the cores of nuclear-fission reactors for spacecraft. Like the electrical-resistance heaters used heretofore for such testing, the dielectric heaters would be inserted in the reactors in place of nuclear fuel rods. A typical heater according to the proposal would consist of a rod of lossy dielectric material sized and shaped like a fuel rod and containing an electrically conductive rod along its center line. Exploiting the dielectric loss mechanism that is usually considered a nuisance in other applications, an RF signal, typically at a frequency .50 MHz and an amplitude between 2 and 5 kV, would be applied to the central conductor to heat the dielectric material. The main advantage of the proposal is that the wiring needed for the RF dielectric heating would be simpler and easier to fabricate than is the wiring needed for resistance heating. In some applications, it might be possible to eliminate all heater wiring and, instead, beam the RF heating power into the dielectric rods from external antennas."
Non-orthogonal subband/transform coder,44.367455,lossy algorithm,['COMMUNICATIONS AND RADAR'],The present invention is directed to a simplified digital subband coder/decoder. In the present invention a signal is fed into a coder. The coder uses a non-orthogonal algorithm that is simply implemented in the coder hardware. The simple non-orthogonal design is then used in the implementation of the decoder to decode the signal.
Low-Complexity Lossless Compression of Hyperspectral Imagery via Adaptive Filtering,44.22474,lossy algorithm,['Optics'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral data is presented. The technique relies on the sign algorithm from the repertoire of adaptive filtering. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
Modelling of multijunction cascade photovoltaics for space applications,44.09279,lossy algorithm,['ENERGY PRODUCTION AND CONVERSION'],"An alternative class of photovoltaics was presented, which is designed to overcome two problem areas with conventional cascade designs: poor upper subcell performance and lossy intercell ohmic contact (IOC). It was shown that upper subcell quality can be improved by incorporating additional junctions into the upper subcell and that the problems with monolithic IOCs may be circumvented by using complementary pairs of three-terminal cells or a 1 x 2 voltage-matched configuration. Realistic simulations show that AlGaAs-GaAs and AlGaAs-InGaAs multijunction, multiband-gap solar cells (MJSC) may achieve benginning-of-life (BOL) one-sun, AMO efficiencies of 26 and 28 percent, respectively. Complementary cells made in the AlGaAs-InGaAs system can achieve BOL one-sun AMO efficiencies in excess of 27 percent. Seven-layer MJSCs are most advantageous for space applications due to their superior tolerance to radiation degradation."
Communications and information research: Improved space link performance via concatenated forward error correction coding,43.61649,lossy algorithm,['Earth Resources and Remote Sensing'],"With the development of new advanced instruments for remote sensing applications, sensor data will be generated at a rate that not only requires increased onboard processing and storage capability, but imposes demands on the space to ground communication link and ground data management-communication system. Data compression and error control codes provide viable means to alleviate these demands. Two types of data compression have been studied by many researchers in the area of information theory: a lossless technique that guarantees full reconstruction of the data, and a lossy technique which generally gives higher data compaction ratio but incurs some distortion in the reconstructed data. To satisfy the many science disciplines which NASA supports, lossless data compression becomes a primary focus for the technology development. While transmitting the data obtained by any lossless data compression, it is very important to use some error-control code. For a long time, convolutional codes have been widely used in satellite telecommunications. To more efficiently transform the data obtained by the Rice algorithm, it is required to meet the a posteriori probability (APP) for each decoded bit. A relevant algorithm for this purpose has been proposed which minimizes the bit error probability in the decoding linear block and convolutional codes and meets the APP for each decoded bit. However, recent results on iterative decoding of 'Turbo codes', turn conventional wisdom on its head and suggest fundamentally new techniques. During the past several months of this research, the following approaches have been developed: (1) a new lossless data compression algorithm, which is much better than the extended Rice algorithm for various types of sensor data, (2) a new approach to determine the generalized Hamming weights of the algebraic-geometric codes defined by a large class of curves in high-dimensional spaces, (3) some efficient improved geometric Goppa codes for disk memory systems and high-speed mass memory systems, and (4) a tree based approach for data compression using dynamic programming."
Mixture block coding with progressive transmission in packet video. Appendix 1: Item 2,42.93556,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Video transmission will become an important part of future multimedia communication because of dramatically increasing user demand for video, and rapid evolution of coding algorithm and VLSI technology. Video transmission will be part of the broadband-integrated services digital network (B-ISDN). Asynchronous transfer mode (ATM) is a viable candidate for implementation of B-ISDN due to its inherent flexibility, service independency, and high performance. According to the characteristics of ATM, the information has to be coded into discrete cells which travel independently in the packet switching network. A practical realization of an ATM video codec called Mixture Block Coding with Progressive Transmission (MBCPT) is presented. This variable bit rate coding algorithm shows how a constant quality performance can be obtained according to user demand. Interactions between codec and network are emphasized including packetization, service synchronization, flow control, and error recovery. Finally, some simulation results based on MBCPT coding with error recovery are presented."
A High Performance Image Data Compression Technique for Space Applications,42.710484,lossy algorithm,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
The development of lossless data compression technology for remote sensing applications,42.27916,lossy algorithm,['COMPUTER SYSTEMS'],"Lossless data compression has been studied for many NASA missions to achieve the benefit of increased science return, reduced onboard memory requirement, station contact time and communication bandwidth. This paper first addresses the requirement for onboard applications and provides rational for the selection of the Rice algorithm among other available techniques. A top-level description of the Rice algorithm will be given, along with some new capabilities already implemented in both software and hardware VLSI forms. The paper then addresses systems issues important for onboard implementation including sensor calibration, error propagation and data packetization. The latter part of the paper provides several case study examples drawn from a broad spectrum of science instruments including the thematic mapper, x-ray telescope, gamma-ray spectrometer, and acousto-optical spectrometer."
Aquarius L-Band Microwave Radiometer: Three Years of Radiometric Performance and Systematic Effects,42.271065,lossy algorithm,['Oceanography'],"The Aquarius L-band microwave radiometer is a three-beam pushbroom instrument designed to measure sea surface salinity. Results are analyzed for performance and systematic effects over three years of operation. The thermal control system maintains tight temperature stability promoting good gain stability. The gain spectrum exhibits expected orbital variations with 1f noise appearing at longer time periods. The on-board detection and integration scheme coupled with the calibration algorithm produce antenna temperatures with NEDT 0.16 K for 1.44-s samples. Nonlinearity is characterized before launch and the derived correction is verified with cold-sky calibration data. Finally, long-term drift is discovered in all channels with 1-K amplitude and 100-day time constant. Nonetheless, it is adeptly corrected using an exponential model."
"Numerical Treatment of Degenerate Diffusion Equations via Feller's Boundary Classification, and Applications",41.829357,lossy algorithm,['Numerical Analysis'],"A numerical method is devised to solve a class of linear boundary-value problems for one-dimensional parabolic equations degenerate at the boundaries. Feller theory, which classifies the nature of the boundary points, is used to decide whether boundary conditions are needed to ensure uniqueness, and, if so, which ones they are. The algorithm is based on a suitable preconditioned implicit finite-difference scheme, grid, and treatment of the boundary data. Second-order accuracy, unconditional stability, and unconditional convergence of solutions of the finite-difference scheme to a constant as the time-step index tends to infinity are further properties of the method. Several examples, pertaining to financial mathematics, physics, and genetics, are presented for the purpose of illustration."
High performance compression of science data,41.765476,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
Model-Checking with Edge-Valued Decision Diagrams,41.45426,lossy algorithm,['Numerical Analysis'],"We describe an algebra of Edge-Valued Decision Diagrams (EVMDDs) to encode arithmetic functions and its implementation in a model checking library along with state-of-the-art algorithms for building the transition relation and the state space of discrete state systems. We provide efficient algorithms for manipulating EVMDDs and give upper bounds of the theoretical time complexity of these algorithms for all basic arithmetic and relational operators. We also demonstrate that the time complexity of the generic recursive algorithm for applying a binary operator on EVMDDs is no worse than that of Multi-Terminal Decision Diagrams. We have implemented a new symbolic model checker with the intention to represent in one formalism the best techniques available at the moment across a spectrum of existing tools: EVMDDs for encoding arithmetic expressions, identity-reduced MDDs for representing the transition relation, and the saturation algorithm for reachability analysis. We compare our new symbolic model checking EVMDD library with the widely used CUDD package and show that, in many cases, our tool is several orders of magnitude faster than CUDD."
Proposed data compression schemes for the Galileo S-band contingency mission,41.173447,lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Galileo spacecraft is currently on its way to Jupiter and its moons. In April 1991, the high gain antenna (HGA) failed to deploy as commanded. In case the current efforts to deploy the HGA fails, communications during the Jupiter encounters will be through one of two low gain antenna (LGA) on an S-band (2.3 GHz) carrier. A lot of effort has been and will be conducted to attempt to open the HGA. Also various options for improving Galileo's telemetry downlink performance are being evaluated in the event that the HGA will not open at Jupiter arrival. Among all viable options the most promising and powerful one is to perform image and non-image data compression in software onboard the spacecraft. This involves in-flight re-programming of the existing flight software of Galileo's Command and Data Subsystem processors and Attitude and Articulation Control System (AACS) processor, which have very limited computational and memory resources. In this article we describe the proposed data compression algorithms and give their respective compression performance. The planned image compression algorithm is a 4 x 4 or an 8 x 8 multiplication-free integer cosine transform (ICT) scheme, which can be viewed as an integer approximation of the popular discrete cosine transform (DCT) scheme. The implementation complexity of the ICT schemes is much lower than the DCT-based schemes, yet the performances of the two algorithms are indistinguishable. The proposed non-image compression algorith is a Lempel-Ziv-Welch (LZW) variant, which is a lossless universal compression algorithm based on a dynamic dictionary lookup table. We developed a simple and efficient hashing function to perform the string search."
Data compression using adaptive transform coding. Appendix 1: Item 1,41.057274,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Adaptive low-rate source coders are described in this dissertation. These coders adapt by adjusting the complexity of the coder to match the local coding difficulty of the image. This is accomplished by using a threshold driven maximum distortion criterion to select the specific coder used. The different coders are built using variable blocksized transform techniques, and the threshold criterion selects small transform blocks to code the more difficult regions and larger blocks to code the less complex regions. A theoretical framework is constructed from which the study of these coders can be explored. An algorithm for selecting the optimal bit allocation for the quantization of transform coefficients is developed. The bit allocation algorithm is more fully developed, and can be used to achieve more accurate bit assignments than the algorithms currently used in the literature. Some upper and lower bounds for the bit-allocation distortion-rate function are developed. An obtainable distortion-rate function is developed for a particular scalar quantizer mixing method that can be used to code transform coefficients at any rate."
Visually Lossless Data Compression for Real-Time Frame/Pushbroom Space Science Imagers,40.791,lossy algorithm,['Computer Programming and Software'],"A visually lossless data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also applicable to frame based imaging and is error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on a block transform of a hybrid of modulated lapped transform (MLT) and discrete cosine transform (DCT), or a 2-dimensional lapped transform, followed by bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate as desired by the user. The approach requires no unique table to maximize its performance. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Flight qualified hardware implementations are in development; a functional chip set is expected by the end of 2001. The chip set is being designed to compress data in excess of 20 Msamples/sec and support quantizations from 2 to 16 bits."
Data compression using Chebyshev transform,40.744392,lossy algorithm,['Computer Programming and Software'],"The present invention is a method, system, and computer program product for implementation of a capable, general purpose compression algorithm that can be engaged on the fly. This invention has particular practical application with time-series data, and more particularly, time-series data obtained form a spacecraft, or similar situations where cost, size and/or power limitations are prevalent, although it is not limited to such applications. It is also particularly applicable to the compression of serial data streams and works in one, two, or three dimensions. The original input data is approximated by Chebyshev polynomials, achieving very high compression ratios on serial data streams with minimal loss of scientific information."
Digital storage and analysis of color Doppler echocardiograms,40.353653,lossy algorithm,['Life Sciences (General)'],"Color Doppler flow mapping has played an important role in clinical echocardiography. Most of the clinical work, however, has been primarily qualitative. Although qualitative information is very valuable, there is considerable quantitative information stored within the velocity map that has not been extensively exploited so far. Recently, many researchers have shown interest in using the encoded velocities to address the clinical problems such as quantification of valvular regurgitation, calculation of cardiac output, and characterization of ventricular filling. In this article, we review some basic physics and engineering aspects of color Doppler echocardiography, as well as drawbacks of trying to retrieve velocities from video tape data. Digital storage, which plays a critical role in performing quantitative analysis, is discussed in some detail with special attention to velocity encoding in DICOM 3.0 (medical image storage standard) and the use of digital compression. Lossy compression can considerably reduce file size with minimal loss of information (mostly redundant); this is critical for digital storage because of the enormous amount of data generated (a 10 minute study could require 18 Gigabytes of storage capacity). Lossy JPEG compression and its impact on quantitative analysis has been studied, showing that images compressed at 27:1 using the JPEG algorithm compares favorably with directly digitized video images, the current goldstandard. Some potential applications of these velocities in analyzing the proximal convergence zones, mitral inflow, and some areas of future development are also discussed in the article."
Automatic commanding of the Mars Observer Camera,40.224277,lossy algorithm,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Mars Observer, launched in September 1992, was intended to be a 'survey-type' mission that acquired global coverage of Mars from a low, circular, near-polar orbit during an entire Martian year. As such, most of its instruments had fixed data rates, wide fields of view, and relatively low resolution, with fairly limited requirements for commanding. An exception is the Mars Observer Camera, or MOC. The MOC consists of a two-color Wide Angle (WA) system that can acquire both global images at low resolution (7.5 km/pixel) and regional images at commandable resolutions up to 250 m/pixel. Complementing the WA is the Narrow Angle (NA) system, that can acquire images at 8 resolutions from 12 m/pixel to 1.5 m/pixel, with a maximum crosstrack dimension of 3 km. The MOC also provides various forms of data compression (both lossless and lossy), and is designed to work at data rates from 700 bits per second (bps) to over 80k bps. Because of this flexibility, developing MOC command sequences is much more difficult than the routine mode-changing that characterizes other instrument operations. Although the MOC cannot be pointed (the spacecraft is fixed nadir-pointing and has no scan platform), the timing, downlink stream allocation, compression type and parameters, and image dimensions of each image must be commanded from the ground, subject to the constraints inherent in the MOC and the spacecraft. To minimize the need for a large operations staff, the entire command generation process has been automated within the MOC Ground Data System. Following the loss of the Mars Observer spacecraft in August 1993, NASA intends to launch a new spacecraft, Mars Global Surveyor (MGS), in late 1996. This spacecraft will carry the MOC flight spare (MOC 2). The MOC 2 operations plan will be largely identical to that developed for MOC, and all of the algorithms described here are applicable to it."
Millimeter-wave passive ultra-compact imaging technology for synthetic vision & mobile platforms,39.883244,lossy algorithm,['Communications and Radar'],"Substantial technical progress was made on all of the three high-risk subsystems of this program. The subsystems include dielectric antenna, G-band receiver, and electro-optic image processor. Progress is approximately on-schedule for both the receiver and the electro-optic processor development, while greater than anticipated challenges have been discovered in the dielectric antenna development. Much of the information in this report was covered in greater detail in the One-Year Review Meeting held at TTC on 22 February 1996. The performance goals of the dielectric antenna project are: Scan Angle -- 20 deg. desired; Loss -- 6 dB end to end (3 dB average); Frequency -- 206-218 GHz (6% bandwidth); Beam width -- 0.25 deg.; and Length -- 12 inches. The scan angle requirement was chosen to satisfy the needs of aircraft pilots. This requirement, coupled with the presently limited bandwidth processors (1 GHz state-of-the-art and 12 GHz in development in this program) forces the antenna to be dielectric (high scan angle air-filled waveguide-based antennas would be too lossy and their performance would vary too much as a function of frequency). A high dielectric constant (e.g., 10) was initially chosen for the dielectric material. This choice lead to the following fabrication challenges: total thickness variation (TTV) tolerance is 1 micrometer; coupler spacing tolerance is 1 micrometer; width tolerance is larger, but unknown, and the surfaces must have mirror finish. Also of importance is the difficulty in obtaining raw materials that satisfy the overall length requirement of 12 inches while simultaneously satisfying the above specifications."
Delta modulation,39.703346,lossy algorithm,['COMMUNICATIONS'],"The overshoot suppression algorithm has been more extensively studied. Computer generated test-pictures show a radical improvement due to the overshoot suppression algorithm. Considering the delta modulator link as a nonlinear digital filter, a formula that relates the minimum rise time that can be handled for given filter parameters and voltage swings has been developed. The settling time has been calculated for the case of overshoot suppression as well as when no suppression is employed. The results indicate a significant decrease in settling time when overshoot suppression is used. An algorithm for correcting channel errors has been developed. It is shown that pulse stuffing PCM words in the DM bit stream results in a significant reduction in error length."
Discrete Random Media Techniques for Microwave Modeling of Vegetated Terrain,39.68106,lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"Microwave remote sensing of vegetated terrain has been studied. Vegetation is modeled so that backscattered radar signals can be used to infer parameters which characterize the vegetation and underlying ground. The vegetation is modeled by discrete lossy dielectric scatterers with prescribed characteristics. The goal of the modeling effort is to remotely sense vegetation type (classification), growth stage, and plant/ground moisture. This information can then be used as input into agricultural, forestry and global circulation models. The microwave frequency spectrum, particularly L and C bands, are especially appropriate for this purpose since the wavelength is comparable to plant leaf and stem size. The resulting resonant interaction leads to backscattered data highly depend on plant shape and orientation. In addition, the transparent nature of the atmosphere in this frequency regime allows for algorithm development which requires no atmospheric correction."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,39.33062,lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
Searching for patterns in remote sensing image databases using neural networks,39.2294,lossy algorithm,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
A seismic data compression system using subband coding,39.214176,lossy algorithm,['COMMUNICATIONS AND RADAR'],"This article presents a study of seismic data compression techniques and a compression algorithm based on subband coding. The algorithm includes three stages: a decorrelation stage, a quantization stage that introduces a controlled amount of distortion to allow for high compression ratios, and a lossless entropy coding stage based on a simple but efficient arithmetic coding method. Subband coding methods are particularly suited to the decorrelation of nonstationary processes such as seismic events. Adaptivity to the nonstationary behavior of the waveform is achieved by dividing the data into separate blocks that are encoded separately with an adaptive arithmetic encoder. This is done with high efficiency due to the low overhead introduced by the arithmetic encoder in specifying its parameters. The technique could be used as a progressive transmission system, where successive refinements of the data can be requested by the user. This allows seismologists to first examine a coarse version of waveforms with minimal usage of the channel and then decide where refinements are required. Rate-distortion performance results are presented and comparisons are made with two block transform methods."
Acoustic scattering from ellipses by the modal element method,38.950485,lossy algorithm,['ACOUSTICS'],"The modal element method is used to study acoustic scattering from ellipses, which may be acoustically soft (absorbing) or hard (reflecting). Because exact solutions are available, the results provide a benchmark for algorithm performance for scattering from airfoils and similar shapes. Numerical results for scattering from rigid ellipses are presented for a wide variety of eccentricities at moderate frequencies. These results indicate that the method is practical."
Optimal Compression of Floating-Point Astronomical Images Without Significant Loss of Information,38.89509,lossy algorithm,['Documentation and Information Science'],"We describe a compression method for floating-point astronomical images that gives compression ratios of 6 - 10 while still preserving the scientifically important information in the image. The pixel values are first preprocessed by quantizing them into scaled integer intensity levels, which removes some of the uncompressible noise in the image. The integers are then losslessly compressed using the fast and efficient Rice algorithm and stored in a portable FITS format file. Quantizing an image more coarsely gives greater image compression, but it also increases the noise and degrades the precision of the photometric and astrometric measurements in the quantized image. Dithering the pixel values during the quantization process greatly improves the precision of measurements in the more coarsely quantized images. We perform a series of experiments on both synthetic and real astronomical CCD images to quantitatively demonstrate that the magnitudes and positions of stars in the quantized images can be measured with the predicted amount of precision. In order to encourage wider use of these image compression methods, we have made available a pair of general-purpose image compression programs, called fpack and funpack, which can be used to compress any FITS format image."
Information-Adaptive Image Encoding and Restoration,38.42396,lossy algorithm,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) has shown itself to be a very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition. A number of algorithms exist that provide one or more of these features, but not all. In this paper we compare the performance of the MSRCR with techniques that are widely used for image enhancement. Specifically, we compare the MSRCR with color adjustment methods such as gamma correction and gain/offset application, histogram modification techniques such as histogram equalization and manual histogram adjustment, and other more powerful techniques such as homomorphic filtering and 'burning and dodging'. The comparison is carried out by testing the suite of image enhancement methods on a set of diverse images. We find that though some of these techniques work well for some of these images, only the MSRCR performs universally well oil the test set."
"NASA Tech Briefs, June 2012",38.177223,lossy algorithm,['Man/System Technology and Life Support'],"Topics covered include: iGlobe Interactive Visualization and Analysis of Spatial Data; Broad-Bandwidth FPGA-Based Digital Polyphase Spectrometer; Small Aircraft Data Distribution System; Earth Science Datacasting v2.0; Algorithm for Compressing Time-Series Data; Onboard Science and Applications Algorithm for Hyperspectral Data Reduction; Sampling Technique for Robust Odorant Detection Based on MIT RealNose Data; Security Data Warehouse Application; Integrated Laser Characterization, Data Acquisition, and Command and Control Test System; Radiation-Hard SpaceWire/Gigabit Ethernet-Compatible Transponder; Hardware Implementation of Lossless Adaptive Compression of Data From a Hyperspectral Imager; High-Voltage, Low-Power BNC Feedthrough Terminator; SpaceCube Mini; Dichroic Filter for Separating W-Band and Ka-Band; Active Mirror Predictive and Requirement Verification Software (AMP-ReVS); Navigation/Prop Software Suite; Personal Computer Transport Analysis Program; Pressure Ratio to Thermal Environments; Probabilistic Fatigue Damage Program (FATIG); ASCENT Program; JPL Genesis and Rapid Intensification Processes (GRIP) Portal; Data::Downloader; Fault Tolerance Middleware for a Multi-Core System; DspaceOgreTerrain 3D Terrain Visualization Tool; Trick Simulation Environment 07; Geometric Reasoning for Automated Planning; Water Detection Based on Color Variation; Single-Layer, All-Metal Patch Antenna Element with Wide Bandwidth; Scanning Laser Infrared Molecular Spectrometer (SLIMS); Next-Generation Microshutter Arrays for Large-Format Imaging and Spectroscopy; Detection of Carbon Monoxide Using Polymer-Composite Films with a Porphyrin-Functionalized Polypyrrole; Enhanced-Adhesion Multiwalled Carbon Nanotubes on Titanium Substrates for Stray Light Control; Three-Dimensional Porous Particles Composed of Curved, Two-Dimensional, Nano-Sized Layers for Li-Ion Batteries 23 Ultra-Lightweight; and Ultra-Lightweight Nanocomposite Foams and Sandwich Structures for Space Structure Applications."
An adaptive vector quantization scheme,38.12462,lossy algorithm,['COMMUNICATIONS AND RADAR'],"Vector quantization is known to be an effective compression scheme to achieve a low bit rate so as to minimize communication channel bandwidth and also to reduce digital memory storage while maintaining the necessary fidelity of the data. However, the large number of computations required in vector quantizers has been a handicap in using vector quantization for low-rate source coding. An adaptive vector quantization algorithm is introduced that is inherently suitable for simple hardware implementation because it has a simple architecture. It allows fast encoding and decoding because it requires only addition and subtraction operations."
Reduction of blocking effects for the JPEG baseline image compression standard,195.43304,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"Transform coding has been chosen for still image compression in the Joint Photographic Experts Group (JPEG) standard. Although transform coding performs superior to many other image compression methods and has fast algorithms for implementation, it is limited by a blocking effect at low bit rates. The blocking effect is inherent in all nonoverlapping transforms. This paper presents a technique for reducing blocking while remaining compatible with the JPEG standard. Simulations show that the system results in subjective performance improvements, sacrificing only a marginal increase in bit rate."
A comparison of the fractal and JPEG algorithms,182.37756,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"A proprietary fractal image compression algorithm and the Joint Photographic Experts Group (JPEG) industry standard algorithm for image compression are compared. In every case, the JPEG algorithm was superior to the fractal method at a given compression ratio according to a root mean square criterion and a peak signal to noise criterion."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,165.93399,jpeg compression,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,154.96045,jpeg compression,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
JPEG 2000 Encoding with Perceptual Distortion Control,153.56902,jpeg compression,['Man/System Technology and Life Support'],"An alternative approach has been devised for encoding image data in compliance with JPEG 2000, the most recent still-image data-compression standard of the Joint Photographic Experts Group. Heretofore, JPEG 2000 encoding has been implemented by several related schemes classified as rate-based distortion-minimization encoding. In each of these schemes, the end user specifies a desired bit rate and the encoding algorithm strives to attain that rate while minimizing a mean squared error (MSE). While rate-based distortion minimization is appropriate for transmitting data over a limited-bandwidth channel, it is not the best approach for applications in which the perceptual quality of reconstructed images is a major consideration. A better approach for such applications is the present alternative one, denoted perceptual distortion control, in which the encoding algorithm strives to compress data to the lowest bit rate that yields at least a specified level of perceptual image quality. Some additional background information on JPEG 2000 is prerequisite to a meaningful summary of JPEG encoding with perceptual distortion control. The JPEG 2000 encoding process includes two subprocesses known as tier-1 and tier-2 coding. In order to minimize the MSE for the desired bit rate, a rate-distortion- optimization subprocess is introduced between the tier-1 and tier-2 subprocesses. In tier-1 coding, each coding block is independently bit-plane coded from the most-significant-bit (MSB) plane to the least-significant-bit (LSB) plane, using three coding passes (except for the MSB plane, which is coded using only one ""clean up"" coding pass). For M bit planes, this subprocess involves a total number of (3M - 2) coding passes. An embedded bit stream is then generated for each coding block. Information on the reduction in distortion and the increase in the bit rate associated with each coding pass is collected. This information is then used in a rate-control procedure to determine the contribution of each coding block to the output compressed bit stream."
Scan-Based Implementation of JPEG 2000 Extensions,152.82037,jpeg compression,['Computer Programming and Software'],"JPEG 2000 Part 2 (Extensions) contains a number of technologies that are of potential interest in remote sensing applications. These include arbitrary wavelet transforms, techniques to limit boundary artifacts in tiles, multiple component transforms, and trellis-coded quantization (TCQ). We are investigating the addition of these features to the low-memory (scan-based) implementation of JPEG 2000 Part 1. A scan-based implementation of TCQ has been realized and tested, with a very small performance loss as compared with the full image (frame-based) version. A proposed amendment to JPEG 2000 Part 2 will effect the syntax changes required to make scan-based TCQ compatible with the standard."
Compression through decomposition into browse and residual images,149.96393,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Economical archival and retrieval of image data is becoming increasingly important considering the unprecedented data volumes expected from the Earth Observing System (EOS) instruments. For cost effective browsing the image data (possibly from remote site), and retrieving the original image data from the data archive, we suggest an integrated image browse and data archive system employing incremental transmission. We produce our browse image data with the JPEG/DCT lossy compression approach. Image residual data is then obtained by taking the pixel by pixel differences between the original data and the browse image data. We then code the residual data with a form of variable length coding called diagonal coding. In our experiments, the JPEG/DCT is used at different quality factors (Q) to generate the browse and residual data. The algorithm has been tested on band 4 of two Thematic mapper (TM) data sets. The best overall compression ratios (of about 1.7) were obtained when a quality factor of Q=50 was used to produce browse data at a compression ratio of 10 to 11. At this quality factor the browse image data has virtually no visible distortions for the images tested."
Radiometric resolution enhancement by lossy compression as compared to truncation followed by lossless compression,140.27629,jpeg compression,['COMPUTER SYSTEMS'],"Recent advances in imaging technology make it possible to obtain imagery data of the Earth at high spatial, spectral and radiometric resolutions from Earth orbiting satellites. The rate at which the data is collected from these satellites can far exceed the channel capacity of the data downlink. Reducing the data rate to within the channel capacity can often require painful trade-offs in which certain scientific returns are sacrificed for the sake of others. In this paper we model the radiometric version of this form of lossy compression by dropping a specified number of least significant bits from each data pixel and compressing the remaining bits using an appropriate lossless compression technique. We call this approach 'truncation followed by lossless compression' or TLLC. We compare the TLLC approach with applying a lossy compression technique to the data for reducing the data rate to the channel capacity, and demonstrate that each of three different lossy compression techniques (JPEG/DCT, VQ and Model-Based VQ) give a better effective radiometric resolution than TLLC for a given channel rate."
The effect of lossy image compression on image classification,137.90784,jpeg compression,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
The effects of video compression on acceptability of images for monitoring life sciences experiments,134.12698,jpeg compression,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Future manned space operations for Space Station Freedom will call for a variety of carefully planned multimedia digital communications, including full-frame-rate color video, to support remote operations of scientific experiments. This paper presents the results of an investigation to determine if video compression is a viable solution to transmission bandwidth constraints. It reports on the impact of different levels of compression and associated calculational parameters on image acceptability to investigators in life-sciences research at ARC. Three nonhuman life-sciences disciplines (plant, rodent, and primate biology) were selected for this study. A total of 33 subjects viewed experimental scenes in their own scientific disciplines. Ten plant scientists viewed still images of wheat stalks at various stages of growth. Each image was compressed to four different compression levels using the Joint Photographic Expert Group (JPEG) standard algorithm, and the images were presented in random order. Twelve and eleven staffmembers viewed 30-sec videotaped segments showing small rodents and a small primate, respectively. Each segment was repeated at four different compression levels in random order using an inverse cosine transform (ICT) algorithm. Each viewer made a series of subjective image-quality ratings. There was a significant difference in image ratings according to the type of scene viewed within disciplines; thus, ratings were scene dependent. Image (still and motion) acceptability does, in fact, vary according to compression level. The JPEG still-image-compression levels, even with the large range of 5:1 to 120:1 in this study, yielded equally high levels of acceptability. In contrast, the ICT algorithm for motion compression yielded a sharp decline in acceptability below 768 kb/sec. Therefore, if video compression is to be used as a solution for overcoming transmission bandwidth constraints, the effective management of the ratio and compression parameters according to scientific discipline and experiment type is critical to the success of remote experiments."
Fast computational scheme of image compression for 32-bit microprocessors,129.26715,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"This paper presents a new computational scheme of image compression based on the discrete cosine transform (DCT), underlying JPEG and MPEG International Standards. The algorithm for the 2-d DCT computation uses integer operations (register shifts and additions / subtractions only); its computational complexity is about 8 additions per image pixel. As a meaningful example of an on-board image compression application we consider the software implementation of the algorithm for the Mars Rover (Marsokhod, in Russian) imaging system being developed as a part of Mars-96 International Space Project. It's shown that fast software solution for 32-bit microprocessors may compete with the DCT-based image compression hardware."
High performance compression of science data,128.84622,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
High Performance Compression of Science Data,128.01634,jpeg compression,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
"The Pixon Method for Data Compression Image Classification, and Image Reconstruction",127.308685,jpeg compression,['Documentation and Information Science'],"As initially proposed, this program had three goals: (1) continue to develop the highly successful Pixon method for image reconstruction and support other scientist in implementing this technique for their applications; (2) develop image compression techniques based on the Pixon method; and (3) develop artificial intelligence algorithms for image classification based on the Pixon approach for simplifying neural networks. Subsequent to proposal review the scope of the program was greatly reduced and it was decided to investigate the ability of the Pixon method to provide superior restorations of images compressed with standard image compression schemes, specifically JPEG-compressed images."
Low-Complexity Adaptive Lossless Compression of Hyperspectral Imagery,111.90407,jpeg compression,['Instrumentation and Photography'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral imagery is described. This technique is designed to be suitable for implementation in hardware such as a field programmable gate array (FPGA); such an implementation could be used for high-speed compression of hyperspectral imagery onboard a spacecraft. The predictive step of the technique makes use of the sign algorithm, which is a relative of the least mean square (LMS) algorithm from the field of low-complexity adaptive filtering. The compressed data stream consists of prediction residuals encoded using a method similar to that of the JPEG-LS lossless image compression standard. Compression results are presented for several datasets including some raw Airborne Visible/ Infrared Imaging Spectrometer (AVIRIS) datasets and raw Atmospheric Infrared Sounder (AIRS) datasets. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
Image coding by way of wavelets,111.39924,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"The application of two wavelet transforms to image compression is discussed. It is noted that the Haar transform, with proper bit allocation, has performance that is visually superior to an algorithm based on a Daubechies filter and to the discrete cosine transform based Joint Photographic Experts Group (JPEG) algorithm at compression ratios exceeding 20:1. In terms of the root-mean-square error, the performance of the Haar transform method is basically comparable to that of the JPEG algorithm. The implementation of the Haar transform can be achieved in integer arithmetic, making it very suitable for applications requiring real-time performance."
KRESKA: A compression system for small and very large images,110.44302,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"An effective lossless compression system for grayscale images is presented using finite context variable order Markov models. A new method to accurately estimate the probability of the escape symbol is proposed. The choice of the best model order and rules for selecting context pixels are discussed. Two context precision and two symbol precision techniques to handle noisy image data with Markov models are introduced. Results indicate that finite context variable order Markov models lead to effective lossless compression systems for small and very large images. The system achieves higher compression ratios than some of the better known image compression techniques such as lossless JPEG, JBIG, or FELICS."
Compression of spectral meteorological imagery,108.5937,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression is essential to current low-earth-orbit spectral sensors with global coverage, e.g., meteorological sensors. Such sensors routinely produce in excess of 30 Gb of data per orbit (over 4 Mb/s for about 110 min) while typically limited to less than 10 Gb of downlink capacity per orbit (15 minutes at 10 Mb/s). Astro-Space Division develops spaceborne compression systems for compression ratios from as little as three to as much as twenty-to-one for high-fidelity reconstructions. Current hardware production and development at Astro-Space Division focuses on discrete cosine transform (DCT) systems implemented with the GE PFFT chip, a 32x32 2D-DCT engine. Spectral relations in the data are exploited through block mean extraction followed by orthonormal transformation. The transformation produces blocks with spatial correlation that are suitable for further compression with any block-oriented spatial compression system, e.g., Astro-Space Division's Laplacian modeler and analytic encoder of DCT coefficients."
High-performance compression of astronomical images,107.3361,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Astronomical images have some rather unusual characteristics that make many existing image compression techniques either ineffective or inapplicable. A typical image consists of a nearly flat background sprinkled with point sources and occasional extended sources. The images are often noisy, so that lossless compression does not work very well; furthermore, the images are usually subjected to stringent quantitative analysis, so any lossy compression method must be proven not to discard useful information, but must instead discard only the noise. Finally, the images can be extremely large. For example, the Space Telescope Science Institute has digitized photographic plates covering the entire sky, generating 1500 images each having 14000 x 14000 16-bit pixels. Several astronomical groups are now constructing cameras with mosaics of large CCD's (each 2048 x 2048 or larger); these instruments will be used in projects that generate data at a rate exceeding 100 MBytes every 5 minutes for many years. An effective technique for image compression may be based on the H-transform (Fritze et al. 1977). The method that we have developed can be used for either lossless or lossy compression. The digitized sky survey images can be compressed by at least a factor of 10 with no noticeable losses in the astrometric and photometric properties of the compressed images. The method has been designed to be computationally efficient: compression or decompression of a 512 x 512 image requires only 4 seconds on a Sun SPARCstation 1. The algorithm uses only integer arithmetic, so it is completely reversible in its lossless mode, and it could easily be implemented in hardware for space applications."
Fractal image compression: A resolution independent representation for imagery,106.03293,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"A deterministic fractal is an image which has low information content and no inherent scale. Because of their low information content, deterministic fractals can be described with small data sets. They can be displayed at high resolution since they are not bound by an inherent scale. A remarkable consequence follows. Fractal images can be encoded at very high compression ratios. This fern, for example is encoded in less than 50 bytes and yet can be displayed at resolutions with increasing levels of detail appearing. The Fractal Transform was discovered in 1988 by Michael F. Barnsley. It is the basis for a new image compression scheme which was initially developed by myself and Michael Barnsley at Iterated Systems. The Fractal Transform effectively solves the problem of finding a fractal which approximates a digital 'real world image'."
Proposed data compression schemes for the Galileo S-band contingency mission,104.823425,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Galileo spacecraft is currently on its way to Jupiter and its moons. In April 1991, the high gain antenna (HGA) failed to deploy as commanded. In case the current efforts to deploy the HGA fails, communications during the Jupiter encounters will be through one of two low gain antenna (LGA) on an S-band (2.3 GHz) carrier. A lot of effort has been and will be conducted to attempt to open the HGA. Also various options for improving Galileo's telemetry downlink performance are being evaluated in the event that the HGA will not open at Jupiter arrival. Among all viable options the most promising and powerful one is to perform image and non-image data compression in software onboard the spacecraft. This involves in-flight re-programming of the existing flight software of Galileo's Command and Data Subsystem processors and Attitude and Articulation Control System (AACS) processor, which have very limited computational and memory resources. In this article we describe the proposed data compression algorithms and give their respective compression performance. The planned image compression algorithm is a 4 x 4 or an 8 x 8 multiplication-free integer cosine transform (ICT) scheme, which can be viewed as an integer approximation of the popular discrete cosine transform (DCT) scheme. The implementation complexity of the ICT schemes is much lower than the DCT-based schemes, yet the performances of the two algorithms are indistinguishable. The proposed non-image compression algorith is a Lempel-Ziv-Welch (LZW) variant, which is a lossless universal compression algorithm based on a dynamic dictionary lookup table. We developed a simple and efficient hashing function to perform the string search."
The Space and Earth Science Data Compression Workshop,104.10385,jpeg compression,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
Techniques for containing error propagation in compression/decompression schemes,102.07486,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has the potential for increasing the risk of data loss. It can also cause bit error propagation, resulting in catastrophic failures. There are a number of approaches possible for containing error propagation due to data compression: (1) data retransmission; (2) data interpolation; (3) error containment; and (4) error correction. The most fruitful techniques will be ones where error containment and error correction are integrated with data compression to provide optimal performance for both. The error containment characteristics of existing compression schemes should be analyzed for their behavior under different data and error conditions. The error tolerance requirements of different data sets need to be understood, so guidelines can then be developed for matching error requirements to suitable compression algorithms."
"Adjustable lossless image compression based on a natural splitting of an image into drawing, shading, and fine-grained components",101.89719,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"The compression, or efficient coding, of single band or multispectral still images is becoming an increasingly important topic. While lossy compression approaches can produce reconstructions that are visually close to the original, many scientific and engineering applications require exact (lossless) reconstructions. However, the most popular and efficient lossless compression techniques do not fully exploit the two-dimensional structural links existing in the image data. We describe here a general approach to lossless data compression that effectively exploits two-dimensional structural links of any length. After describing in detail two main variants on this scheme, we discuss experimental results."
Planning/scheduling techniques for VQ-based image compression,101.87378,jpeg compression,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
The 1993 Space and Earth Science Data Compression Workshop,100.15709,jpeg compression,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
Compression of color-mapped images,98.10596,jpeg compression,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
Image compression system and method having optimized quantization tables,97.325035,jpeg compression,['Instrumentation and Photography'],"A digital image compression preprocessor for use in a discrete cosine transform-based digital image compression device is provided. The preprocessor includes a gathering mechanism for determining discrete cosine transform statistics from input digital image data. A computing mechanism is operatively coupled to the gathering mechanism to calculate a image distortion array and a rate of image compression array based upon the discrete cosine transform statistics for each possible quantization value. A dynamic programming mechanism is operatively coupled to the computing mechanism to optimize the rate of image compression array against the image distortion array such that a rate-distortion-optimal quantization table is derived. In addition, a discrete cosine transform-based digital image compression device and a discrete cosine transform-based digital image compression and decompression system are provided. Also, a method for generating a rate-distortion-optimal quantization table, using discrete cosine transform-based digital image compression, and operating a discrete cosine transform-based digital image compression and decompression system are provided."
Low-Complexity Lossless Compression of Hyperspectral Imagery via Adaptive Filtering,97.19392,jpeg compression,['Optics'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral data is presented. The technique relies on the sign algorithm from the repertoire of adaptive filtering. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
The 1995 Science Information Management and Data Compression Workshop,96.87013,jpeg compression,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Data compression using Chebyshev transform,96.17694,jpeg compression,['Computer Programming and Software'],"The present invention is a method, system, and computer program product for implementation of a capable, general purpose compression algorithm that can be engaged on the fly. This invention has particular practical application with time-series data, and more particularly, time-series data obtained form a spacecraft, or similar situations where cost, size and/or power limitations are prevalent, although it is not limited to such applications. It is also particularly applicable to the compression of serial data streams and works in one, two, or three dimensions. The original input data is approximated by Chebyshev polynomials, achieving very high compression ratios on serial data streams with minimal loss of scientific information."
Data compression for full motion video transmission,96.14856,jpeg compression,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
Fast Lossless Compression of Multispectral-Image Data,96.0999,jpeg compression,['Computer Programming and Software'],"An algorithm that effects fast lossless compression of multispectral-image data is based on low-complexity, proven adaptive-filtering algorithms. This algorithm is intended for use in compressing multispectral-image data aboard spacecraft for transmission to Earth stations. Variants of this algorithm could be useful for lossless compression of three-dimensional medical imagery and, perhaps, for compressing image data in general."
1994 Science Information Management and Data Compression Workshop,96.06847,jpeg compression,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Blocking reduction of Landsat Thematic Mapper JPEG browse images using optimal PSNR estimated spectra adaptive postfiltering,95.28465,jpeg compression,['COMPUTER SYSTEMS'],"Two representative sample images of Band 4 of the Landsat Thematic Mapper are compressed with the JPEG algorithm at 8:1, 16:1 and 24:1 Compression Ratios for experimental browsing purposes. We then apply the Optimal PSNR Estimated Spectra Adaptive Postfiltering (ESAP) algorithm to reduce the DCT blocking distortion. ESAP reduces the blocking distortion while preserving most of the image's edge information by adaptively postfiltering the decoded image using the block's spectral information already obtainable from each block's DCT coefficients. The algorithm iteratively applied a one dimensional log-sigmoid weighting function to the separable interpolated local block estimated spectra of the decoded image until it converges to the optimal PSNR with respect to the original using a 2-D steepest ascent search. Convergence is obtained in a few iterations for integer parameters. The optimal logsig parameters are transmitted to the decoder as a negligible byte of overhead data. A unique maxima is guaranteed due to the 2-D asymptotic exponential overshoot shape of the surface generated by the algorithm. ESAP is based on a DFT analysis of the DCT basis functions. It is implemented with pixel-by-pixel spatially adaptive separable FIR postfilters. PSNR objective improvements between 0.4 to 0.8 dB are shown together with their corresponding optimal PSNR adaptive postfiltered images."
The CCDS Data Compression Recommendations: Development and Status,94.85349,jpeg compression,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
Data compression by wavelet transforms,94.7453,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"A wavelet transform algorithm is applied to image compression. It is observed that the algorithm does not suffer from the blockiness characteristic of the DCT-based algorithms at compression ratios exceeding 25:1, but the edges do not appear as sharp as they do with the latter method. Some suggestions for the improved performance of the wavelet transform method are presented."
The importance of robust error control in data compression applications,94.65065,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
A Posteriori Restoration of Block Transform-Compressed Data,93.838936,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Galileo spacecraft will use lossy data compression for the transmission of its science imagery over the low-bandwidth communication system.  The technique chosen for image  compression is a block transform technique based on the Integer Cosine Transform, a derivative of the JPEG image compression standard.  Considered here are two known a posteriori  enhancement techniques, which are adapted."
Learning random networks for compression of still and moving images,93.783676,jpeg compression,['CYBERNETICS'],"Image compression for both still and moving images is an extremely important area of investigation, with numerous applications to videoconferencing, interactive education, home entertainment, and potential applications to earth observations, medical imaging, digital libraries, and many other areas. We describe work on a neural network methodology to compress/decompress still and moving images. We use the 'point-process' type neural network model which is closer to biophysical reality than standard models, and yet is mathematically much more tractable. We currently achieve compression ratios of the order of 120:1 for moving grey-level images, based on a combination of motion detection and compression. The observed signal-to-noise ratio varies from values above 25 to more than 35. The method is computationally fast so that compression and decompression can be carried out in real-time. It uses the adaptive capabilities of a set of neural networks so as to select varying compression ratios in real-time as a function of quality achieved. It also uses a motion detector which will avoid retransmitting portions of the image which have varied little from the previous frame. Further improvements can be achieved by using on-line learning during compression, and by appropriate compensation of nonlinearities in the compression/decompression scheme. We expect to go well beyond the 250:1 compression level for color images with good quality levels."
Progressive transmission and compression images,93.753174,jpeg compression,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Video Compression Study: h.265 vs h.264,93.260025,jpeg compression,['Instrumentation and Photography'],"H.265 video compression (also known as High Efficiency Video Encoding (HEVC)) promises to provide double the video quality at half the bandwidth, or the same quality at half the bandwidth of h.264 video compression [1]. This study uses a Tektronix PQA500 to determine the video quality gains by using h.265 encoding. This study also compares two video encoders to see how different implementations of h.264 and h.265 impact video quality at various bandwidths. "
A High Performance Image Data Compression Technique for Space Applications,93.07477,jpeg compression,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
Comparisons of theoretical limits for source coding with practical compression algorithms,92.86325,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"The performance achieved by some specific data compression algorithms is compared with absolute limits prescribed by rate distortion theory for Gaussian sources under the mean square error distortion criterion. These results show the gains available from source coding and can be used as a reference for the evaluation of future compression schemes. Some current schemes perform well, but there is still room for improvement."
Low bit rate coding of Earth science images,92.70012,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"In this paper, the authors discuss compression based on some new ideas in vector quantization and their incorporation in a sub-band coding framework. Several variations are considered, which collectively address many of the individual compression needs within the earth science community. The approach taken in this work is based on some recent advances in the area of variable rate residual vector quantization (RVQ). This new RVQ method is considered separately and in conjunction with sub-band image decomposition. Very good results are achieved in coding a variety of earth science images. The last section of the paper provides some comparisons that illustrate the improvement in performance attributable to this approach relative the the JPEG coding standard."
Integer cosine transform for image compression,92.62673,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"This article describes a recently introduced transform algorithm called the integer cosine transform (ICT), which is used in transform-based data compression schemes. The ICT algorithm requires only integer operations on small integers and at the same time gives a rate-distortion performance comparable to that offered by the floating-point discrete cosine transform (DCT). The article addresses the issue of implementation complexity, which is of prime concern for source coding applications of interest in deep-space communications. Complexity reduction in the transform stage of the compression scheme is particularly relevant, since this stage accounts for most (typically over 80 percent) of the computational load."
"Modeling of video traffic in packet networks, low rate video compression, and the development of a lossy+lossless image compression algorithm",92.25473,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this reporting period we have worked on three somewhat different problems. These are modeling of video traffic in packet networks, low rate video compression, and the development of a lossy + lossless image compression algorithm, which might have some application in browsing algorithms. The lossy + lossless scheme is an extension of work previously done under this grant. It provides a simple technique for incorporating browsing capability. The low rate coding scheme is also a simple variation on the standard discrete cosine transform (DCT) coding approach. In spite of its simplicity, the approach provides surprisingly high quality reconstructions. The modeling approach is borrowed from the speech recognition literature, and seems to be promising in that it provides a simple way of obtaining an idea about the second order behavior of a particular coding scheme. Details about these are presented."
Progressive Transmission and Compression of Images,91.93054,jpeg compression,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,91.711624,jpeg compression,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
Image data compression having minimum perceptual error,91.637924,jpeg compression,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
A visual detection model for DCT coefficient quantization,91.502785,jpeg compression,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
Non-linear Post Processing Image Enhancement,91.463425,jpeg compression,"['Cybernetics, Artificial Intelligence and Robotics']","A non-linear filter for image post processing based on the feedforward Neural Network topology is presented. This study was undertaken to investigate the usefulness of ""smart"" filters in image post processing. The filter has shown to be useful in recovering high frequencies, such as those lost during the JPEG compression-decompression process. The filtered images have a higher signal to noise ratio, and a higher perceived image quality. Simulation studies comparing the proposed filter with the optimum mean square non-linear filter, showing examples of the high frequency recovery, and the statistical properties of the filter are given,"
Compression of regions in the global advanced very high resolution radiometer 1-km data set,90.988304,jpeg compression,['COMPUTER SYSTEMS'],"The global advanced very high resolution radiometer (AVHRR) 1-km data set is a 10-band image produced at USGS' EROS Data Center for the study of the world's land surfaces. The image contains masked regions for non-land areas which are identical in each band but vary between data sets. They comprise over 75 percent of this 9.7 gigabyte image. The mask is compressed once and stored separately from the land data which is compressed for each of the 10 bands. The mask is stored in a hierarchical format for multi-resolution decompression of geographic subwindows of the image. The land for each band is compressed by modifying a method that ignores fill values. This multi-spectral region compression efficiently compresses the region data and precludes fill values from interfering with land compression statistics. Results show that the masked regions in a one-byte test image (6.5 Gigabytes) compress to 0.2 percent of the 557,756,146 bytes they occupy in the original image, resulting in a compression ratio of 89.9 percent for the entire image."
Studies on image compression and image reconstruction,90.85465,jpeg compression,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Implementation of CCSDS Lossless Data Compression in HDF,90.73607,jpeg compression,['Documentation and Information Science'],The Earth Science Data and Information System (ESDIS) handles over one terabyte (10(exp 12) bytes) of data daily and is using the Hierarchical Data Format (EDF) for data archiving and distribution. This report provides the progress and status of our effort to alleviate bandwidth and storage burdens by first performing compression studies on various science data products and later integrating the selected compression scheme into HDF.
Quantization Distortion in Block Transform-Compressed Data,90.669785,jpeg compression,['Documentation and Information Science'],"The popular JPEG image compression standard is an example of a block transform-based compression scheme; the image is systematically subdivided into block that are individually  transformed, quantized, and encoded.  The compression is achieved by quantizing the transformed data, reducing the data entropy and thus facilitating efficient encoding.  A generic  block transform model is introduced."
Synthetic aperture radar signal data compression using block adaptive quantization,90.365814,jpeg compression,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
A seismic data compression system using subband coding,89.932785,jpeg compression,['COMMUNICATIONS AND RADAR'],"This article presents a study of seismic data compression techniques and a compression algorithm based on subband coding. The algorithm includes three stages: a decorrelation stage, a quantization stage that introduces a controlled amount of distortion to allow for high compression ratios, and a lossless entropy coding stage based on a simple but efficient arithmetic coding method. Subband coding methods are particularly suited to the decorrelation of nonstationary processes such as seismic events. Adaptivity to the nonstationary behavior of the waveform is achieved by dividing the data into separate blocks that are encoded separately with an adaptive arithmetic encoder. This is done with high efficiency due to the low overhead introduced by the arithmetic encoder in specifying its parameters. The technique could be used as a progressive transmission system, where successive refinements of the data can be requested by the user. This allows seismologists to first examine a coarse version of waveforms with minimal usage of the channel and then decide where refinements are required. Rate-distortion performance results are presented and comparisons are made with two block transform methods."
Locally adaptive vector quantization: Data compression with feature preservation,89.85037,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
"Data compression for data archival, browse or quick-look",89.8396,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Soon after space and Earth science data is collected, it is stored in one or more archival facilities for later retrieval and analysis. Since the purpose of the archival process is to keep an accurate and complete record of data, any data compression used in an archival system must be lossless, and protect against propagation of error in the storage media. A browse capability for space and Earth science data is needed to enable scientists to check the appropriateness and quality of particular data sets before obtaining the full data set(s) for detailed analysis. Browse data produced for these purposes could be used to facilitate the retrieval of data from an archival facility. Quick-look data is data obtained directly from the sensor for either previewing the data or for an application that requires very timely analysis of the space or Earth science data. Two main differences between data compression techniques appropriate to browse and quick-look cases, are that quick-look can be more specifically tailored, and it must be limited in complexity by the relatively limited computational power available on space platforms."
System considerations for efficient communication and storage of MSTI image data,89.804504,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
Emerging standards for still image compression: A software implementation and simulation study,89.640564,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],The software implementation is described of an emerging standard for the lossy compression of continuous tone still images. This software program can be used to compress planetary images and other 2-D instrument data. It provides a high compression image coding capability that preserves image fidelity at compression rates competitive or superior to most known techniques. This software implementation confirms the usefulness of such data compression and allows its performance to be compared with other schemes used in deep space missions and for data based storage.
A comparison of model-based VQ compression with other VQ approaches,89.351,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"In our previous work on Model-Based Vector Quantization (MVQ), we presented some performance comparisons (both rate distortion and decompression time) with VQ and JPEG/DCT. In this paper, we compare the MVQ's rate distortion performance with Mean Removed Vector Quantization (MRVQ) and include our previous comparison with VQ. MVQ is similar to MRVQ in many ways. Both of these techniques extract means of the vectors (raster-scanned image blocks) and reduce them to mean removed residuals by subtracting block means from the elements of the vectors. In the case of MRVQ, a codebook of residual vectors is generated using a training set. For every vector from the input image, the block mean and address of the codevector from the codebook that matches the input vector closest are transmitted to the decoder. The codebook is generated using generalized Lloyd algorithm on training set of residual vectors. For MVQ the pairs consist of vector means and address of the closest matching vector from codebook generated by models based on statistical properties of the residuals and Human Visual System (HVS). In our experiments, we found that MVQ performance in rate distortion sense is almost always better than VQ and is comparable to MRVQ. Further, MVQ is much easier to use than either VQ or MRVQ, since the training and managing of explicit codebooks is not required."
Visually Lossless Data Compression for Real-Time Frame/Pushbroom Space Science Imagers,88.9469,jpeg compression,['Computer Programming and Software'],"A visually lossless data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also applicable to frame based imaging and is error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on a block transform of a hybrid of modulated lapped transform (MLT) and discrete cosine transform (DCT), or a 2-dimensional lapped transform, followed by bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate as desired by the user. The approach requires no unique table to maximize its performance. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Flight qualified hardware implementations are in development; a functional chip set is expected by the end of 2001. The chip set is being designed to compress data in excess of 20 Msamples/sec and support quantizations from 2 to 16 bits."
Image-adapted visually weighted quantization matrices for digital image compression,88.83501,jpeg compression,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Data compression in remote sensing applications,88.54578,jpeg compression,['EARTH RESOURCES AND REMOTE SENSING'],"A survey of current data compression techniques which are being used to reduce the amount of data in remote sensing applications is provided. The survey aspect is far from complete, reflecting the substantial activity in this area. The purpose of the survey is more to exemplify the different approaches being taken rather than to provide an exhaustive list of the various proposed approaches."
Perceptual Image Compression in Telemedicine,88.49306,jpeg compression,['Aerospace Medicine'],"The next era of space exploration, especially the ""Mission to Planet Earth"" will generate immense quantities of image data. For example, the Earth Observing System (EOS) is expected to generate in excess of one terabyte/day. NASA confronts a major technical challenge in managing this great flow of imagery: in collection, pre-processing, transmission to earth, archiving, and distribution to scientists at remote locations. Expected requirements in most of these areas clearly exceed current technology. Part of the solution to this problem lies in efficient image compression techniques. For much of this imagery, the ultimate consumer is the human eye. In this case image compression should be designed to match the visual capacities of the human observer. We have developed three techniques for optimizing image compression for the human viewer. The first consists of a formula, developed jointly with IBM and based on psychophysical measurements, that computes a DCT quantization matrix for any specified combination of viewing distance, display resolution, and display brightness. This DCT quantization matrix is used in most recent standards for digital image compression (JPEG, MPEG, CCITT H.261). The second technique optimizes the DCT quantization matrix for each individual image, based on the contents of the image. This is accomplished by means of a model of visual sensitivity to compression artifacts. The third technique extends the first two techniques to the realm of wavelet compression. Together these two techniques will allow systematic perceptual optimization of image compression in NASA imaging systems. Many of the image management challenges faced by NASA are mirrored in the field of telemedicine. Here too there are severe demands for transmission and archiving of large image databases, and the imagery is ultimately used primarily by human observers, such as radiologists. In this presentation I will describe some of our preliminary explorations of the applications of our technology to the special problems of telemedicine."
Sub-band/transform compression of video sequences,86.98135,jpeg compression,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Making the Best of JPEG at Very High Compression Ratios: Rectangular Pixel,84.41762,jpeg compression,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Space and Earth Science Data Compression Workshop,84.271255,jpeg compression,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The focus was on scientists' data requirements, as well as constraints imposed by the data collection, transmission, distribution, and archival systems. The workshop consisted of several invited papers; two described information systems for space and Earth science data, four depicted analysis scenarios for extracting information of scientific interest from data collected by Earth orbiting and deep space platforms, and a final one was a general tutorial on image data compression."
Wavelet Compression of Satellite-Transmitted Digital Mammograms,84.0055,jpeg compression,['Life Sciences (General)'],"Breast cancer is one of the major causes of cancer death in women in the United States. The most effective way to treat breast cancer is to detect it at an early stage by screening patients periodically. Conventional film-screening mammography uses X-ray films which are effective in detecting early abnormalities of the breast. Direct digital mammography has the potential to improve the image quality and to take advantages of convenient storage, efficient transmission, and powerful computer-aided diagnosis, etc. One effective alternative to direct digital imaging is secondary digitization of X-ray films. This technique may not provide as high an image quality as the direct digital approach, but definitely have other advantages inherent to digital images. One of them is the usage of satellite-transmission technique for transferring digital mammograms between a remote image-acquisition site and a central image-reading site. This technique can benefit a large population of women who reside in remote areas where major screening and diagnosing facilities are not available. The NASA-Lewis Research Center (LeRC), in collaboration with the Cleveland Clinic Foundation (CCF), has begun a pilot study to investigate the application of the Advanced Communications Technology Satellite (ACTS) network to telemammography. The bandwidth of the T1 transmission is limited (1.544 Mbps) while the size of a mammographic image is huge. It takes a long time to transmit a single mammogram. For example, a mammogram of 4k by 4k pixels with 16 bits per pixel needs more than 4 minutes to transmit. Four images for a typical screening exam would take more than 16 minutes. This is too long a time period for a convenient screening. Consequently, compression is necessary for making satellite-transmission of mammographic images practically possible. The Wavelet Research Group of the Department of Electrical Engineering at The Ohio State University (OSU) participated in the LeRC-CCF collaboration by providing advanced compression technology using wavelet transform. OSU developed a time-efficient software package with various wavelets to compress a serious of mammographic images. This documents reports the result of the compression activities."
An image compression technique for use on token ring networks,83.878265,jpeg compression,['COMMUNICATIONS AND RADAR'],A low complexity technique for compression of images for transmission over local area networks is presented. The technique uses the synchronous traffic as a side channel for improving the performance of an adaptive differential pulse code modulation (ADPCM) based coder.
Phoenix Telemetry Processor,83.504654,jpeg compression,['Computer Programming and Software'],"Phxtelemproc is a C/C++ based telemetry processing program that processes SFDU telemetry packets from the Telemetry Data System (TDS). It generates Experiment Data Records (EDRs) for several instruments including surface stereo imager (SSI); robotic arm camera (RAC); robotic arm (RA); microscopy, electrochemistry, and conductivity analyzer (MECA); and the optical microscope (OM). It processes both uncompressed and compressed telemetry, and incorporates unique subroutines for the following compression algorithms: JPEG Arithmetic, JPEG Huffman, Rice, LUT3, RA, and SX4. This program was in the critical path for the daily command cycle of the Phoenix mission. The products generated by this program were part of the RA commanding process, as well as the SSI, RAC, OM, and MECA image and science analysis process. Its output products were used to advance science of the near polar regions of Mars, and were used to prove that water is found in abundance there. Phxtelemproc is part of the MIPL (Multi-mission Image Processing Laboratory) system. This software produced Level 1 products used to analyze images returned by in situ spacecraft. It ultimately assisted in operations, planning, commanding, science, and outreach."
Evaluation of Algorithms for Compressing Hyperspectral Data,82.82457,jpeg compression,['Computer Programming and Software'],"With EO-1 Hyperion in orbit NASA is showing their continued commitment to hyperspectral imaging (HSI). As HSI sensor technology continues to mature, the ever-increasing amounts of sensor data generated will result in a need for more cost effective communication and data handling systems. Lockheed Martin, with considerable experience in spacecraft design and developing special purpose onboard processors, has teamed with Applied Signal & Image Technology (ASIT), who has an extensive heritage in HSI spectral compression and Mapping Science (MSI) for JPEG 2000 spatial compression expertise, to develop a real-time and intelligent onboard processing (OBP) system to reduce HSI sensor downlink requirements. Our goal is to reduce the downlink requirement by a factor > 100, while retaining the necessary spectral and spatial fidelity of the sensor data needed to satisfy the many science, military, and intelligence goals of these systems. Our compression algorithms leverage commercial-off-the-shelf (COTS) spectral and spatial exploitation algorithms. We are currently in the process of evaluating these compression algorithms using statistical analysis and NASA scientists. We are also developing special purpose processors for executing these algorithms onboard a spacecraft."
An efficient system for reliably transmitting image and video data over low bit rate noisy channels,79.0075,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"This research project is intended to develop an efficient system for reliably transmitting image and video data over low bit rate noisy channels. The basic ideas behind the proposed approach are the following: employ statistical-based image modeling to facilitate pre- and post-processing and error detection, use spare redundancy that the source compression did not remove to add robustness, and implement coded modulation to improve bandwidth efficiency and noise rejection. Over the last six months, progress has been made on various aspects of the project. Through our studies of the integrated system, a list-based iterative Trellis decoder has been developed. The decoder accepts feedback from a post-processor which can detect channel errors in the reconstructed image. The error detection is based on the Huber Markov random field image model for the compressed image. The compression scheme used here is that of JPEG (Joint Photographic Experts Group). Experiments were performed and the results are quite encouraging. The principal ideas here are extendable to other compression techniques. In addition, research was also performed on unequal error protection channel coding, subband vector quantization as a means of source coding, and post processing for reducing coding artifacts. Our studies on unequal error protection (UEP) coding for image transmission focused on examining the properties of the UEP capabilities of convolutional codes. The investigation of subband vector quantization employed a wavelet transform with special emphasis on exploiting interband redundancy. The outcome of this investigation included the development of three algorithms for subband vector quantization. The reduction of transform coding artifacts was studied with the aid of a non-Gaussian Markov random field model. This results in improved image decompression. These studies are summarized and the technical papers included in the appendices."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,76.76753,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
Method and system for efficient video compression with low-complexity encoder,76.66687,jpeg compression,['Instrumentation and Photography'],"Disclosed are a method and system for video compression, wherein the video encoder has low computational complexity and high compression efficiency. The disclosed system comprises a video encoder and a video decoder, wherein the method for encoding includes the steps of converting a source frame into a space-frequency representation; estimating conditional statistics of at least one vector of space-frequency coefficients; estimating encoding rates based on the said conditional statistics; and applying Slepian-Wolf codes with the said computed encoding rates. The preferred method for decoding includes the steps of; generating a side-information vector of frequency coefficients based on previously decoded source data, encoder statistics, and previous reconstructions of the source frequency vector; and performing Slepian-Wolf decoding of at least one source frequency vector based on the generated side-information, the Slepian-Wolf code bits and the encoder statistics."
A simplified Integer Cosine Transform and its application in image compression,75.669495,jpeg compression,['NUMERICAL ANALYSIS'],"A simplified version of the integer cosine transform (ICT) is described. For practical reasons, the transform is considered jointly with the quantization of its coefficients. It differs from conventional ICT algorithms in that the combined factors for normalization and quantization are approximated by powers of two. In conventional algorithms, the normalization/quantization stage typically requires as many integer divisions as the number of transform coefficients. By restricting the factors to powers of two, these divisions can be performed by variable shifts in the binary representation of the coefficients, with speed and cost advantages to the hardware implementation of the algorithm. The error introduced by the factor approximations is compensated for in the inverse ICT operation, executed with floating point precision. The simplified ICT algorithm has potential applications in image-compression systems with disparate cost and speed requirements in the encoder and decoder ends. For example, in deep space image telemetry, the image processors on board the spacecraft could take advantage of the simplified, faster encoding operation, which would be adjusted on the ground, with high-precision arithmetic. A dual application is found in compressed video broadcasting. Here, a fast, high-performance processor at the transmitter would precompensate for the factor approximations in the inverse ICT operation, to be performed in real time, at a large number of low-cost receivers."
The effects of video compression on acceptability of images for monitoring life sciences' experiments,75.594574,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"Current plans indicate that there will be a large number of life science experiments carried out during the thirty year-long mission of the Biological Flight Research Laboratory (BFRL) on board Space Station Freedom (SSF). Non-human life science experiments will be performed in the BFRL. Two distinct types of activities have already been identified for this facility: (1) collect, store, distribute, analyze and manage engineering and science data from the Habitats, Glovebox and Centrifuge, (2) perform a broad range of remote science activities in the Glovebox and Habitat chambers in conjunction with the remotely located principal investigator (PI). These activities require extensive video coverage, viewing and/or recording and distribution to video displays on board SSF and to the ground. This paper concentrates mainly on the second type of activity. Each of the two BFRL habitat racks are designed to be configurable for either six rodent habitats per rack, four plant habitats per rack, or a combination of the above. Two video cameras will be installed in each habitat with a spare attachment for a third camera when needed. Therefore, a video system that can accommodate up to 12-18 camera inputs per habitat rack must be considered."
Performance of customized DCT quantization tables on scientific data,69.332344,jpeg compression,['COMPUTER SYSTEMS'],"We show that it is desirable to use data-specific or customized quantization tables for scaling the spatial frequency coefficients obtained using the Discrete Cosine Transform (DCT). DCT is widely used for image and video compression (MP89, PM93) but applications typically use default quantization matrices. Using actual scientific data gathered from divers sources such as spacecrafts and electron-microscopes, we show that the default compression/quality tradeoffs can be significantly improved upon by using customized tables. We also show that significant improvements are possible for the standard test images Lena and Baboon. This work is part of an effort to develop a practical scheme for optimizing quantization matrices for any given image or video stream, under any given quality or compression constraints."
Study and simulation of low rate video coding schemes,68.56163,jpeg compression,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
Compressing subbanded image data with Lempel-Ziv-based coders,67.03324,jpeg compression,['COMMUNICATIONS AND RADAR'],"A method of improving the compression of image data using Lempel-Ziv-based coding is presented. Image data is first processed with a simple transform, such as the Walsh Hadamard Transform, to produce subbands. The subbanded data can be rounded to eight bits or it can be quantized for higher compression at the cost of some reduction in the quality of the reconstructed image. The data is then run-length coded to take advantage of the large runs of zeros produced by quantization. Compression results are presented and contrasted with a subband compression method using quantization followed by run-length coding and Huffman coding. The Lempel-Ziv-based coding in conjunction with run-length coding produces the best compression results at the same reconstruction quality (compared with the Huffman-based coding) on the image data used."
The Visibility of DCT Quantization Noise: Spatial Frequency Summation,66.76941,jpeg compression,['Computer Programming and Software'],"Computational models of the ability to detect image compression artifacts facilitate the optimization of compression parameters, such as the JPEG quantization matrix. For simplicity, some models assume that the visibility of artifacts containing different spatial frequency components is determined by the most visible component, that is, no summation over components. Using the type of noise generated by quantization in the Discrete Cosine Transform (DCT) domain, we find a degree of summation between that of probability summation and contrast energy summation."
Compressed/reconstructed test images for CRAF/Cassini,65.282196,jpeg compression,['ASTROPHYSICS'],"A set of compressed, then reconstructed, test images submitted to the Comet Rendezvous Asteroid Flyby (CRAF)/Cassini project is presented as part of its evaluation of near lossless high compression algorithms for representing image data. A total of seven test image files were provided by the project. The seven test images were compressed, then reconstructed with high quality (root mean square error of approximately one or two gray levels on an 8 bit gray scale), using discrete cosine transforms or Hadamard transforms and efficient entropy coders. The resulting compression ratios varied from about 2:1 to about 10:1, depending on the activity or randomness in the source image. This was accomplished without any special effort to optimize the quantizer or to introduce special postprocessing to filter the reconstruction errors. A more complete set of measurements, showing the relative performance of the compression algorithms over a wide range of compression ratios and reconstruction errors, shows that additional compression is possible at a small sacrifice in fidelity."
Transform coding for space applications,64.62311,jpeg compression,['COMMUNICATIONS AND RADAR'],"Data compression coding requirements for aerospace applications differ somewhat from the compression requirements for entertainment systems. On the one hand, entertainment applications are bit rate driven with the goal of getting the best quality possible with a given bandwidth. Science applications are quality driven with the goal of getting the lowest bit rate for a given level of reconstruction quality. In the past, the required quality level has been nothing less than perfect allowing only the use of lossless compression methods (if that). With the advent of better, faster, cheaper missions, an opportunity has arisen for lossy data compression methods to find a use in science applications as requirements for perfect quality reconstruction runs into cost constraints. This paper presents a review of the data compression problem from the space application perspective. Transform coding techniques are described and some simple, integer transforms are presented. The application of these transforms to space-based data compression problems is discussed. Integer transforms have an advantage over conventional transforms in computational complexity. Space applications are different from broadcast or entertainment in that it is desirable to have a simple encoder (in space) and tolerate a more complicated decoder (on the ground) rather than vice versa. Energy compaction with new transforms are compared with the Walsh-Hadamard (WHT), Discrete Cosine (DCT), and Integer Cosine (ICT) transforms."
COxSwAIN: Compressive Sensing for Advanced Imaging and Navigation,64.222534,jpeg compression,"['Mathematical and Computer Sciences (General)', 'Communications and Radar']","The COxSwAIN project focuses on building an image and video compression scheme that can be implemented in a small or low-power satellite. To do this, we used Compressive Sensing, where the compression is performed by matrix multiplications on the satellite and reconstructed on the ground. Our paper explains our methodology and demonstrates the results of the scheme, being able to achieve high quality image compression that is robust to noise and corruption."
Orbiter/External Tank Mate 3-D Solid Modeling,63.046482,jpeg compression,['Mathematical and Computer Sciences (General)'],"This research and development project presents an overview of the work completed while attending a summer 2004 American Society of Engineering Education/National Aeronautics and Space Administration (ASEE/NASA) Faculty Fellowship. This fellowship was completed at the Kennedy Space Center, Florida. The scope of the project was to complete parts, assemblies, and drawings that could be used by Ground Support Equipment (GSE) personnel to simulate situations and scenarios commonplace to the space shuttle Orbiter/External Tank (ET) Mate (50004). This mate takes place in the Vehicle Assembly Building (VAB). These simulations could then be used by NASA engineers as decision-making tools. During the summer of 2004, parts were created that defined the Orbiter/ET structural interfaces. Emphasis was placed upon assemblies that included the Orbiter/ET forward attachment (EO-1), aft left thrust strut (EO-2), aft right tripod support structure (EO-3), and crossbeam and aft feedline/umbilical supports. These assemblies are used to attach the Orbiter to the ET. The Orbiter/ET Mate assembly was then used to compare and analyze clearance distances using different Orbiter hang angles. It was found that a 30-minute arc angle change in Orbiter hang angle affected distance at the bipod strut to Orbiter yoke fitting 8.11 inches. A 3-D solid model library was established as a result of this project. This library contains parts, assemblies, and drawings translated into several formats. This library contains a collection of the following files: sti for sterolithography, stp for neutral file work, shrinkwrap for compression. tiff for photoshop work, jpeg for Internet use, and prt and asm for Pro/Engineer use. This library was made available to NASA engineers so that they could access its contents to make angle, load, and clearance analysis studies. These decision-making tools may be used by Pro/Engineer users and non-users."
Techniques for video compression,61.10102,jpeg compression,['COMPUTER PROGRAMMING AND SOFTWARE'],"In this report, we present our study on multiprocessor implementation of a MPEG2 encoding algorithm. First, we compare two approaches to implementing video standards, VLSI technology and multiprocessor processing, in terms of design complexity, applications, and cost. Then we evaluate the functional modules of MPEG2 encoding process in terms of their computation time. Two crucial modules are identified based on this evaluation. Then we present our experimental study on the multiprocessor implementation of the two crucial modules. Data partitioning is used for job assignment. Experimental results show that high speedup ratio and good scalability can be achieved by using this kind of job assignment strategy."
Digital storage and analysis of color Doppler echocardiograms,59.101585,jpeg compression,['Life Sciences (General)'],"Color Doppler flow mapping has played an important role in clinical echocardiography. Most of the clinical work, however, has been primarily qualitative. Although qualitative information is very valuable, there is considerable quantitative information stored within the velocity map that has not been extensively exploited so far. Recently, many researchers have shown interest in using the encoded velocities to address the clinical problems such as quantification of valvular regurgitation, calculation of cardiac output, and characterization of ventricular filling. In this article, we review some basic physics and engineering aspects of color Doppler echocardiography, as well as drawbacks of trying to retrieve velocities from video tape data. Digital storage, which plays a critical role in performing quantitative analysis, is discussed in some detail with special attention to velocity encoding in DICOM 3.0 (medical image storage standard) and the use of digital compression. Lossy compression can considerably reduce file size with minimal loss of information (mostly redundant); this is critical for digital storage because of the enormous amount of data generated (a 10 minute study could require 18 Gigabytes of storage capacity). Lossy JPEG compression and its impact on quantitative analysis has been studied, showing that images compressed at 27:1 using the JPEG algorithm compares favorably with directly digitized video images, the current goldstandard. Some potential applications of these velocities in analyzing the proximal convergence zones, mitral inflow, and some areas of future development are also discussed in the article."
Providing Internet Access to High-Resolution Mars Images,57.900772,jpeg compression,['Man/System Technology and Life Support'],"The OnMars server is a computer program that provides Internet access to high-resolution Mars images, maps, and elevation data, all suitable for use in geographical information system (GIS) software for generating images, maps, and computational models of Mars. The OnMars server is an implementation of the Open Geospatial Consortium (OGC) Web Map Service (WMS) server. Unlike other Mars Internet map servers that provide Martian data using an Earth coordinate system, the OnMars WMS server supports encoding of data in Mars-specific coordinate systems. The OnMars server offers access to most of the available high-resolution Martian image and elevation data, including an 8-meter-per-pixel uncontrolled mosaic of most of the Mars Global Surveyor (MGS) Mars Observer Camera Narrow Angle (MOCNA) image collection, which is not available elsewhere. This server can generate image and map files in the tagged image file format (TIFF), Joint Photographic Experts Group (JPEG), 8- or 16-bit Portable Network Graphics (PNG), or Keyhole Markup Language (KML) format. Image control is provided by use of the OGC Style Layer Descriptor (SLD) protocol. The OnMars server also implements tiled WMS protocol and super-overlay KML for high-performance client application programs."
A visual detection model for DCT coefficient quantization,57.422836,jpeg compression,['CYBERNETICS'],"The discrete cosine transform (DCT) is widely used in image compression, and is part of the JPEG and MPEG compression standards. The degree of compression, and the amount of distortion in the decompressed image are determined by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. Our approach is to set the quantization level for each coefficient so that the quantization error is at the threshold of visibility. Here we combine results from our previous work to form our current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color."
"Model-based VQ for image data archival, retrieval and distribution",55.454227,jpeg compression,['DOCUMENTATION AND INFORMATION SCIENCE'],"An ideal image compression technique for image data archival, retrieval and distribution would be one with the asymmetrical computational requirements of Vector Quantization (VQ), but without the complications arising from VQ codebooks. Codebook generation and maintenance are stumbling blocks which have limited the use of VQ as a practical image compression algorithm. Model-based VQ (MVQ), a variant of VQ described here, has the computational properties of VQ but does not require explicit codebooks. The codebooks are internally generated using mean removed error and Human Visual System (HVS) models. The error model assumed is the Laplacian distribution with mean, lambda-computed from a sample of the input image. A Laplacian distribution with mean, lambda, is generated with uniform random number generator. These random numbers are grouped into vectors. These vectors are further conditioned to make them perceptually meaningful by filtering the DCT coefficients from each vector. The DCT coefficients are filtered by multiplying by a weight matrix that is found to be optimal for human perception. The inverse DCT is performed to produce the conditioned vectors for the codebook. The only image dependent parameter used in the generation of codebook is the mean, lambda, that is included in the coded file to repeat the codebook generation process for decoding."
Providing Internet Access to High-Resolution Lunar Images,55.429947,jpeg compression,['Man/System Technology and Life Support'],"The OnMoon server is a computer program that provides Internet access to high-resolution Lunar images, maps, and elevation data, all suitable for use in geographical information system (GIS) software for generating images, maps, and computational models of the Moon. The OnMoon server implements the Open Geospatial Consortium (OGC) Web Map Service (WMS) server protocol and supports Moon-specific extensions. Unlike other Internet map servers that provide Lunar data using an Earth coordinate system, the OnMoon server supports encoding of data in Moon-specific coordinate systems. The OnMoon server offers access to most of the available high-resolution Lunar image and elevation data. This server can generate image and map files in the tagged image file format (TIFF) or the Joint Photographic Experts Group (JPEG), 8- or 16-bit Portable Network Graphics (PNG), or Keyhole Markup Language (KML) format. Image control is provided by use of the OGC Style Layer Descriptor (SLD) protocol. Full-precision spectral arithmetic processing is also available, by use of a custom SLD extension. This server can dynamically add shaded relief based on the Lunar elevation to any image layer. This server also implements tiled WMS protocol and super-overlay KML for high-performance client application programs."
Technology Directions for the 21st Century,54.317818,jpeg compression,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
Visual optimization of DCT quantization matrices for individual images,54.234863,jpeg compression,['CYBERNETICS'],"Many image compression standards (JPEG, MPEG, H.261) are based on the Discrete Cosine Transform (DCT). However, these standards do not specify the actual DCT quantization matrix. We have previously provided mathematical formulae to compute a perceptually lossless quantization matrix. Here I show how to compute a matrix that is optimized for a particular image. The method treats each DCT coefficient as an approximation to the local response of a visual 'channel'. For a given quantization matrix, the DCT quantization errors are adjusted by contrast sensitivity, light adaptation, and contrast masking, and are pooled non-linearly over the blocks of the image. This yields an 8x8 'perceptual error matrix'. A second non-linear pooling over the perceptual error matrix yields total perceptual error. With this model we may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
"NASA Tech Briefs, September 2008",53.104286,jpeg compression,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
Remote driving with reduced bandwidth communication,51.86066,jpeg compression,['COMMUNICATIONS AND RADAR'],"Oak Ridge National Laboratory has developed a real-time video transmission system for low bandwidth remote operations. The system supports both continuous transmission of video for remote driving and progressive transmission of still images. Inherent in the system design is a spatiotemporal limitation to the effects of channel errors. The average data rate of the system is 64,000 bits/s, a compression of approximately 1000:1 for the black and white National Television Standard Code video. The image quality of the transmissions is maintained at a level that supports teleoperation of a high mobility multipurpose wheeled vehicle at speeds up to 15 mph on a moguled dirt track. Video compression is achieved by using Laplacian image pyramids and a combination of classical techniques. Certain subbands of the image pyramid are transmitted by using interframe differencing with a periodic refresh to aid in bandwidth reduction. Images are also foveated to concentrate image detail in a steerable region. The system supports dynamic video quality adjustments between frame rate, image detail, and foveation rate. A typical configuration for the system used during driving has a frame rate of 4 Hz, a compression per frame of 125:1, and a resulting latency of less than 1s."
"Visual information processing II; Proceedings of the Meeting, Orlando, FL, Apr. 14-16, 1993",51.661133,jpeg compression,['CYBERNETICS'],"Various papers on visual information processing are presented. Individual topics addressed include: aliasing as noise, satellite image processing using a hammering neural network, edge-detetion method using visual perception, adaptive vector median filters, design of a reading test for low-vision image warping, spatial transformation architectures, automatic image-enhancement method, redundancy reduction in image coding, lossless gray-scale image compression by predictive GDF, information efficiency in visual communication, optimizing JPEG quantization matrices for different applications, use of forward error correction to maintain image fidelity, effect of peanoscanning on image compression. Also discussed are: computer vision for autonomous robotics in space, optical processor for zero-crossing edge detection, fractal-based image edge detection, simulation of the neon spreading effect by bandpass filtering, wavelet transform (WT) on parallel SIMD architectures, nonseparable 2D wavelet image representation, adaptive image halftoning based on WT, wavelet analysis of global warming, use of the WT for signal detection, perfect reconstruction two-channel rational filter banks, N-wavelet coding for pattern classification, simulation of image of natural objects, number-theoretic coding for iconic systems."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,51.537872,jpeg compression,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
Resiliency of the Multiscale Retinex Image Enhancement Algorithm,50.43205,jpeg compression,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) continues to prove itself in extensive testing to be very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition, However, issues remain with regard to the resiliency of the MSRCR to different image sources and arbitrary image manipulations which may have been applied prior to retinex processing. In this paper we define these areas of concern, provide experimental results, and, examine the effects of commonly occurring image manipulation on retinex performance. In virtually all cases the MSRCR is highly resilient to the effects of both the image source variations and commonly encountered prior image-processing. Significant artifacts are primarily observed for the case of selective color channel clipping in large dark zones in a image. These issues are of concerning the processing of digital image archives and other applications where there is neither control over the image acquisition process, nor knowledge about any processing done on th data beforehand."
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,171.15125,jpeg quality,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
JPEG 2000 Encoding with Perceptual Distortion Control,153.42096,jpeg quality,['Man/System Technology and Life Support'],"An alternative approach has been devised for encoding image data in compliance with JPEG 2000, the most recent still-image data-compression standard of the Joint Photographic Experts Group. Heretofore, JPEG 2000 encoding has been implemented by several related schemes classified as rate-based distortion-minimization encoding. In each of these schemes, the end user specifies a desired bit rate and the encoding algorithm strives to attain that rate while minimizing a mean squared error (MSE). While rate-based distortion minimization is appropriate for transmitting data over a limited-bandwidth channel, it is not the best approach for applications in which the perceptual quality of reconstructed images is a major consideration. A better approach for such applications is the present alternative one, denoted perceptual distortion control, in which the encoding algorithm strives to compress data to the lowest bit rate that yields at least a specified level of perceptual image quality. Some additional background information on JPEG 2000 is prerequisite to a meaningful summary of JPEG encoding with perceptual distortion control. The JPEG 2000 encoding process includes two subprocesses known as tier-1 and tier-2 coding. In order to minimize the MSE for the desired bit rate, a rate-distortion- optimization subprocess is introduced between the tier-1 and tier-2 subprocesses. In tier-1 coding, each coding block is independently bit-plane coded from the most-significant-bit (MSB) plane to the least-significant-bit (LSB) plane, using three coding passes (except for the MSB plane, which is coded using only one ""clean up"" coding pass). For M bit planes, this subprocess involves a total number of (3M - 2) coding passes. An embedded bit stream is then generated for each coding block. Information on the reduction in distortion and the increase in the bit rate associated with each coding pass is collected. This information is then used in a rate-control procedure to determine the contribution of each coding block to the output compressed bit stream."
Scan-Based Implementation of JPEG 2000 Extensions,148.63466,jpeg quality,['Computer Programming and Software'],"JPEG 2000 Part 2 (Extensions) contains a number of technologies that are of potential interest in remote sensing applications. These include arbitrary wavelet transforms, techniques to limit boundary artifacts in tiles, multiple component transforms, and trellis-coded quantization (TCQ). We are investigating the addition of these features to the low-memory (scan-based) implementation of JPEG 2000 Part 1. A scan-based implementation of TCQ has been realized and tested, with a very small performance loss as compared with the full image (frame-based) version. A proposed amendment to JPEG 2000 Part 2 will effect the syntax changes required to make scan-based TCQ compatible with the standard."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,120.20804,jpeg quality,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,108.838776,jpeg quality,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
Compression through decomposition into browse and residual images,92.11507,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Economical archival and retrieval of image data is becoming increasingly important considering the unprecedented data volumes expected from the Earth Observing System (EOS) instruments. For cost effective browsing the image data (possibly from remote site), and retrieving the original image data from the data archive, we suggest an integrated image browse and data archive system employing incremental transmission. We produce our browse image data with the JPEG/DCT lossy compression approach. Image residual data is then obtained by taking the pixel by pixel differences between the original data and the browse image data. We then code the residual data with a form of variable length coding called diagonal coding. In our experiments, the JPEG/DCT is used at different quality factors (Q) to generate the browse and residual data. The algorithm has been tested on band 4 of two Thematic mapper (TM) data sets. The best overall compression ratios (of about 1.7) were obtained when a quality factor of Q=50 was used to produce browse data at a compression ratio of 10 to 11. At this quality factor the browse image data has virtually no visible distortions for the images tested."
Non-linear Post Processing Image Enhancement,83.79599,jpeg quality,"['Cybernetics, Artificial Intelligence and Robotics']","A non-linear filter for image post processing based on the feedforward Neural Network topology is presented. This study was undertaken to investigate the usefulness of ""smart"" filters in image post processing. The filter has shown to be useful in recovering high frequencies, such as those lost during the JPEG compression-decompression process. The filtered images have a higher signal to noise ratio, and a higher perceived image quality. Simulation studies comparing the proposed filter with the optimum mean square non-linear filter, showing examples of the high frequency recovery, and the statistical properties of the filter are given,"
Image coding by way of wavelets,80.35781,jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"The application of two wavelet transforms to image compression is discussed. It is noted that the Haar transform, with proper bit allocation, has performance that is visually superior to an algorithm based on a Daubechies filter and to the discrete cosine transform based Joint Photographic Experts Group (JPEG) algorithm at compression ratios exceeding 20:1. In terms of the root-mean-square error, the performance of the Haar transform method is basically comparable to that of the JPEG algorithm. The implementation of the Haar transform can be achieved in integer arithmetic, making it very suitable for applications requiring real-time performance."
The effects of video compression on acceptability of images for monitoring life sciences experiments,73.75007,jpeg quality,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Future manned space operations for Space Station Freedom will call for a variety of carefully planned multimedia digital communications, including full-frame-rate color video, to support remote operations of scientific experiments. This paper presents the results of an investigation to determine if video compression is a viable solution to transmission bandwidth constraints. It reports on the impact of different levels of compression and associated calculational parameters on image acceptability to investigators in life-sciences research at ARC. Three nonhuman life-sciences disciplines (plant, rodent, and primate biology) were selected for this study. A total of 33 subjects viewed experimental scenes in their own scientific disciplines. Ten plant scientists viewed still images of wheat stalks at various stages of growth. Each image was compressed to four different compression levels using the Joint Photographic Expert Group (JPEG) standard algorithm, and the images were presented in random order. Twelve and eleven staffmembers viewed 30-sec videotaped segments showing small rodents and a small primate, respectively. Each segment was repeated at four different compression levels in random order using an inverse cosine transform (ICT) algorithm. Each viewer made a series of subjective image-quality ratings. There was a significant difference in image ratings according to the type of scene viewed within disciplines; thus, ratings were scene dependent. Image (still and motion) acceptability does, in fact, vary according to compression level. The JPEG still-image-compression levels, even with the large range of 5:1 to 120:1 in this study, yielded equally high levels of acceptability. In contrast, the ICT algorithm for motion compression yielded a sharp decline in acceptability below 768 kb/sec. Therefore, if video compression is to be used as a solution for overcoming transmission bandwidth constraints, the effective management of the ratio and compression parameters according to scientific discipline and experiment type is critical to the success of remote experiments."
The effect of lossy image compression on image classification,68.139946,jpeg quality,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
MPEG-2 Over Asynchronous Transfer Mode (ATM) Over Satellite Quality of Service (QoS) Experiments: Laboratory Tests,67.748764,jpeg quality,"['Space Communications, Spacecraft Communications, Command and Tracking']","Asynchronous transfer mode (ATM) quality of service (QoS) experiments were performed using MPEG-2 (ATM application layer 5, AAL5) over ATM over an emulated satellite link. The purpose of these experiments was to determine the free-space link quality necessary to transmit high-quality multimedia information by using the ATM protocol. The detailed test plan and test configuration are described herein as are the test results. MPEG-2 transport streams were baselined in an errored environment, followed by a series of tests using, MPEG-2 over ATM. Errors were created both digitally as well as in an IF link by using a satellite modem and commercial gaussian noise test set for two different MPEG-2 decoder implementations. The results show that ITU-T Recommendation 1.356 Class 1, stringent ATM applications will require better link quality than currently specified; in particular, cell loss ratios of better than 1.0 x 10(exp -8) and cell error ratios of better than 1.0 x 10(exp -7) are needed. These tests were conducted at the NASA Lewis Research Center in support of satellite-ATM interoperability research."
"MODIS Land Data Products: Generation, Quality Assurance and Validation",67.32938,jpeg quality,['Earth Resources and Remote Sensing'],"The Moderate Resolution Imaging Spectrometer (MODIS) on-board NASA's Earth Observing System (EOS) Terra and Aqua Satellites are key instruments for providing data on global land, atmosphere, and ocean dynamics. Derived MODIS land, atmosphere and ocean products are central to NASA's mission to monitor and understand the Earth system. NASA has developed and generated on a systematic basis a suite of MODIS products starting with the first Terra MODIS data sensed February 22, 2000 and continuing with the first MODIS-Aqua data sensed July 2, 2002. The MODIS Land products are divided into three product suites: radiation budget products, ecosystem products, and land cover characterization products. The production and distribution of the MODIS Land products are described, from initial software delivery by the MODIS Land Science Team, to operational product generation and quality assurance, delivery to EOS archival and distribution centers, and product accuracy assessment and validation. Progress and lessons learned since the first MODIS data were in early 2000 are described."
Method and Apparatus for Evaluating the Visual Quality of Processed Digital Video Sequences,66.971634,jpeg quality,['Electronics and Electrical Engineering'],"A Digital Video Quality (DVQ) apparatus and method that incorporate a model of human visual sensitivity to predict the visibility of artifacts. The DVQ method and apparatus are used for the evaluation of the visual quality of processed digital video sequences and for adaptively controlling the bit rate of the processed digital video sequences without compromising the visual quality. The DVQ apparatus minimizes the required amount of memory and computation. The input to the DVQ apparatus is a pair of color image sequences: an original (R) non-compressed sequence, and a processed (T) sequence. Both sequences (R) and (T) are sampled, cropped, and subjected to color transformations. The sequences are then subjected to blocking and discrete cosine transformation, and the results are transformed to local contrast. The next step is a time filtering operation which implements the human sensitivity to different time frequencies. The results are converted to threshold units by dividing each discrete cosine transform coefficient by its respective visual threshold. At the next stage the two sequences are subtracted to produce an error sequence. The error sequence is subjected to a contrast masking operation, which also depends upon the reference sequence (R). The masked errors can be pooled in various ways to illustrate the perceptual error over various dimensions, and the pooled error can be converted to a visual quality measure."
Second Harmonic Imaging improves Echocardiograph Quality on board the International Space Station,65.8842,jpeg quality,['Aerospace Medicine'],"Ultrasound (US) capabilities have been part of the Human Research Facility (HRF) on board the International Space Station (ISS) since 2001. The US equipment on board the ISS includes a first-generation Tissue Harmonic Imaging (THI) option. Harmonic imaging (HI) is the second harmonic response of the tissue to the ultrasound beam and produces robust tissue detail and signal. Since this is a first-generation THI, there are inherent limitations in tissue penetration. As a breakthrough technology, HI extensively advanced the field of ultrasound. In cardiac applications, it drastically improves endocardial border detection and has become a common imaging modality. U.S. images were captured and stored as JPEG stills from the ISS video downlink. US images with and without harmonic imaging option were randomized and provided to volunteers without medical education or US skills for identification of endocardial border. The results were processed and analyzed using applicable statistical calculations. The measurements in US images using HI improved measurement consistency and reproducibility among observers when compared to fundamental imaging. HI has been embraced by the imaging community at large as it improves the quality and data validity of US studies, especially in difficult-to-image cases. Even with the limitations of the first generation THI, HI improved the quality and measurability of many of the downlinked images from the ISS and should be an option utilized with cardiac imaging on board the ISS in all future space missions."
BOREAS Level-0 C-130 Aerial Photography,65.81828,jpeg quality,['Earth Resources and Remote Sensing'],"For BOReal Ecosystem-Atmosphere Study (BOREAS), C-130 and other aerial photography was collected to provide finely detailed and spatially extensive documentation of the condition of the primary study sites. The NASA C-130 Earth Resources aircraft can accommodate two mapping cameras during flight, each of which can be fitted with 6- or 12-inch focal-length lenses and black-and-white, natural-color, or color-IR film, depending upon requirements. Both cameras were often in operation simultaneously, although sometimes only the lower resolution camera was deployed. When both cameras were in operation, the higher resolution camera was often used in a more limited fashion. The acquired photography covers the period of April to September 1994. The aerial photography was delivered as rolls of large format (9 x 9 inch) color transparency prints, with imagery from multiple missions (hundreds of prints) often contained within a single roll. A total of 1533 frames were collected from the C-130 platform for BOREAS in 1994. Note that the level-0 C-130 transparencies are not contained on the BOREAS CD-ROM set. An inventory file is supplied on the CD-ROM to inform users of all the data that were collected. Some photographic prints were made from the transparencies. In addition, BORIS staff digitized a subset of the tranparencies and stored the images in JPEG format. The CD-ROM set contains a small subset of the collected aerial photography that were the digitally scanned and stored as JPEG files for most tower and auxiliary sites in the NSA and SSA. See Section 15 for information about how to acquire additional imagery."
BOREAS RSS-3 Imagery and Snapshots from a Helicopter-Mounted Video Camera,65.69482,jpeg quality,['Earth Resources and Remote Sensing'],"The BOREAS RSS-3 team collected helicopter-based video coverage of forested sites acquired during BOREAS as well as single-frame ""snapshots"" processed to still images. Helicopter data used in this analysis were collected during all three 1994 IFCs (24-May to 16-Jun, 19-Jul to 10-Aug, and 30-Aug to 19-Sep), at numerous tower and auxiliary sites in both the NSA and the SSA. The VHS-camera observations correspond to other coincident helicopter measurements. The field of view of the camera is unknown. The video tapes are in both VHS and Beta format. The still images are stored in JPEG format."
A visual detection model for DCT coefficient quantization,65.12362,jpeg quality,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
DCTune Perceptual Optimization of Compressed Dental X-Rays,63.845345,jpeg quality,['Aerospace Medicine'],"In current dental practice, x-rays of completed dental work are often sent to the insurer for verification. It is faster and cheaper to transmit instead digital scans of the x-rays. Further economies result if the images are sent in compressed form. DCTune is a technology for optimizing DCT (digital communication technology) quantization matrices to yield maximum perceptual quality for a given bit-rate, or minimum bit-rate for a given perceptual quality. Perceptual optimization of DCT color quantization matrices. In addition, the technology provides a means of setting the perceptual quality of compressed imagery in a systematic way. The purpose of this research was, with respect to dental x-rays, 1) to verify the advantage of DCTune over standard JPEG (Joint Photographic Experts Group), 2) to verify the quality control feature of DCTune, and 3) to discover regularities in the optimized matrices of a set of images. We optimized matrices for a total of 20 images at two resolutions (150 and 300 dpi) and four bit-rates (0.25, 0.5, 0.75, 1.0 bits/pixel), and examined structural regularities in the resulting matrices. We also conducted psychophysical studies (1) to discover the DCTune quality level at which the images became 'visually lossless,' and (2) to rate the relative quality of DCTune and standard JPEG images at various bitrates. Results include: (1) At both resolutions, DCTune quality is a linear function of bit-rate. (2) DCTune quantization matrices for all images at all bitrates and resolutions are modeled well by an inverse Gaussian, with parameters of amplitude and width. (3) As bit-rate is varied, optimal values of both amplitude and width covary in an approximately linear fashion. (4) Both amplitude and width vary in systematic and orderly fashion with either bit-rate or DCTune quality; simple mathematical functions serve to describe these relationships. (5) In going from 150 to 300 dpi, amplitude parameters are substantially lower and widths larger at corresponding bit-rates or qualities. (6) Visually lossless compression occurs at a DCTune quality value of about 1. (7) At 0.25 bits/pixel, comparative ratings give DCTune a substantial advantage over standard JPEG. As visually lossless bit-rates are approached, this advantage of necessity diminishes. We have concluded that DCTune optimized quantization matrices provide better visual quality than standard JPEG. Meaningful quality levels may be specified by means of the DCTune metric. Optimized matrices are very similar across the class of dental x-rays, suggesting the possibility of a 'class-optimal' matrix. DCTune technology appears to provide some value in the context of compressed dental x-rays."
Low bit rate coding of Earth science images,63.68698,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"In this paper, the authors discuss compression based on some new ideas in vector quantization and their incorporation in a sub-band coding framework. Several variations are considered, which collectively address many of the individual compression needs within the earth science community. The approach taken in this work is based on some recent advances in the area of variable rate residual vector quantization (RVQ). This new RVQ method is considered separately and in conjunction with sub-band image decomposition. Very good results are achieved in coding a variety of earth science images. The last section of the paper provides some comparisons that illustrate the improvement in performance attributable to this approach relative the the JPEG coding standard."
High performance compression of science data,62.656788,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
High Performance Compression of Science Data,61.98098,jpeg quality,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
"NASA Tech Briefs, September 2008",60.29873,jpeg quality,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
System considerations for efficient communication and storage of MSTI image data,56.046757,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
A comparison of model-based VQ compression with other VQ approaches,55.273888,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"In our previous work on Model-Based Vector Quantization (MVQ), we presented some performance comparisons (both rate distortion and decompression time) with VQ and JPEG/DCT. In this paper, we compare the MVQ's rate distortion performance with Mean Removed Vector Quantization (MRVQ) and include our previous comparison with VQ. MVQ is similar to MRVQ in many ways. Both of these techniques extract means of the vectors (raster-scanned image blocks) and reduce them to mean removed residuals by subtracting block means from the elements of the vectors. In the case of MRVQ, a codebook of residual vectors is generated using a training set. For every vector from the input image, the block mean and address of the codevector from the codebook that matches the input vector closest are transmitted to the decoder. The codebook is generated using generalized Lloyd algorithm on training set of residual vectors. For MVQ the pairs consist of vector means and address of the closest matching vector from codebook generated by models based on statistical properties of the residuals and Human Visual System (HVS). In our experiments, we found that MVQ performance in rate distortion sense is almost always better than VQ and is comparable to MRVQ. Further, MVQ is much easier to use than either VQ or MRVQ, since the training and managing of explicit codebooks is not required."
DCTune Perceptual Optimization of Compressed Dental X-Rays,54.82259,jpeg quality,['Aerospace Medicine'],"In current dental practice, x-rays of completed dental work are often sent to the insurer for verification. It is faster and cheaper to transmit instead digital scans of the x-rays. Further economies result if the images are sent in compressed form. DCtune is a technology for optimizing DCT quantization matrices to yield maximum perceptual quality for a given bit-rate, or minimum bit-rate for a given perceptual quality. In addition, the technology provides a means of setting the perceptual quality of compressed imagery in a systematic way. The purpose of this research was, with respect to dental x-rays: (1) to verify the advantage of DCTune over standard JPEG; (2) to verify the quality control feature of DCTune; and (3) to discover regularities in the optimized matrices of a set of images. Additional information is contained in the original extended abstract."
Video Compression Study: h.265 vs h.264,54.72412,jpeg quality,['Instrumentation and Photography'],"H.265 video compression (also known as High Efficiency Video Encoding (HEVC)) promises to provide double the video quality at half the bandwidth, or the same quality at half the bandwidth of h.264 video compression [1]. This study uses a Tektronix PQA500 to determine the video quality gains by using h.265 encoding. This study also compares two video encoders to see how different implementations of h.264 and h.265 impact video quality at various bandwidths. "
Ultrasonic Nondestructive Evaluation Techniques Applied to the Quantitative Characterization of Textile Composite Materials,54.36046,jpeg quality,['Quality Assurance and Reliability'],"In this Progress Report, we describe our further development of advanced ultrasonic nondestructive evaluation methods applied to the characterization of anisotropic materials. We present images obtained from experimental measurements of ultrasonic diffraction patterns transmitted through water only and transmitted through water and a thin woven composite. All images of diffraction patterns have been included on the accompanying CD-ROM in the JPEG format and Adobe TM Portable Document Format (PDF), in addition to the inclusion of hardcopies of the images contained in this report. In our previous semi-annual Progress Report (NAG 1-1848, December, 1996), we proposed a simple model to simulate the effect of a thin woven composite on an insonifying ultrasonic pressure field. This initial approach provided an avenue to begin development of a robust measurement method for nondestructive evaluation of anisotropic materials. In this Progress Report, we extend that work by performing experimental measurements on a single layer of a five-harness biaxial woven composite to investigate how a thin, yet architecturally complex, material interacts with the insonifying ultrasonic field. In Section 2 of this Progress Report we describe the experimental arrangement and methods for data acquisition of the ultrasonic diffraction patterns upon transmission through a thin woven composite. We also briefly describe the thin composite specimen investigated. Section 3 details the analysis of the experimental data followed by the experimental results in Section 4. Finally, a discussion of the observations and conclusions is found in Section 5."
Transform coding for space applications,53.49826,jpeg quality,['COMMUNICATIONS AND RADAR'],"Data compression coding requirements for aerospace applications differ somewhat from the compression requirements for entertainment systems. On the one hand, entertainment applications are bit rate driven with the goal of getting the best quality possible with a given bandwidth. Science applications are quality driven with the goal of getting the lowest bit rate for a given level of reconstruction quality. In the past, the required quality level has been nothing less than perfect allowing only the use of lossless compression methods (if that). With the advent of better, faster, cheaper missions, an opportunity has arisen for lossy data compression methods to find a use in science applications as requirements for perfect quality reconstruction runs into cost constraints. This paper presents a review of the data compression problem from the space application perspective. Transform coding techniques are described and some simple, integer transforms are presented. The application of these transforms to space-based data compression problems is discussed. Integer transforms have an advantage over conventional transforms in computational complexity. Space applications are different from broadcast or entertainment in that it is desirable to have a simple encoder (in space) and tolerate a more complicated decoder (on the ground) rather than vice versa. Energy compaction with new transforms are compared with the Walsh-Hadamard (WHT), Discrete Cosine (DCT), and Integer Cosine (ICT) transforms."
An efficient system for reliably transmitting image and video data over low bit rate noisy channels,53.19744,jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"This research project is intended to develop an efficient system for reliably transmitting image and video data over low bit rate noisy channels. The basic ideas behind the proposed approach are the following: employ statistical-based image modeling to facilitate pre- and post-processing and error detection, use spare redundancy that the source compression did not remove to add robustness, and implement coded modulation to improve bandwidth efficiency and noise rejection. Over the last six months, progress has been made on various aspects of the project. Through our studies of the integrated system, a list-based iterative Trellis decoder has been developed. The decoder accepts feedback from a post-processor which can detect channel errors in the reconstructed image. The error detection is based on the Huber Markov random field image model for the compressed image. The compression scheme used here is that of JPEG (Joint Photographic Experts Group). Experiments were performed and the results are quite encouraging. The principal ideas here are extendable to other compression techniques. In addition, research was also performed on unequal error protection channel coding, subband vector quantization as a means of source coding, and post processing for reducing coding artifacts. Our studies on unequal error protection (UEP) coding for image transmission focused on examining the properties of the UEP capabilities of convolutional codes. The investigation of subband vector quantization employed a wavelet transform with special emphasis on exploiting interband redundancy. The outcome of this investigation included the development of three algorithms for subband vector quantization. The reduction of transform coding artifacts was studied with the aid of a non-Gaussian Markov random field model. This results in improved image decompression. These studies are summarized and the technical papers included in the appendices."
Performance of customized DCT quantization tables on scientific data,50.850735,jpeg quality,['COMPUTER SYSTEMS'],"We show that it is desirable to use data-specific or customized quantization tables for scaling the spatial frequency coefficients obtained using the Discrete Cosine Transform (DCT). DCT is widely used for image and video compression (MP89, PM93) but applications typically use default quantization matrices. Using actual scientific data gathered from divers sources such as spacecrafts and electron-microscopes, we show that the default compression/quality tradeoffs can be significantly improved upon by using customized tables. We also show that significant improvements are possible for the standard test images Lena and Baboon. This work is part of an effort to develop a practical scheme for optimizing quantization matrices for any given image or video stream, under any given quality or compression constraints."
Improved image decompression for reduced transform coding artifacts,48.362053,jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
COxSwAIN: Compressive Sensing for Advanced Imaging and Navigation,46.041935,jpeg quality,"['Mathematical and Computer Sciences (General)', 'Communications and Radar']","The COxSwAIN project focuses on building an image and video compression scheme that can be implemented in a small or low-power satellite. To do this, we used Compressive Sensing, where the compression is performed by matrix multiplications on the satellite and reconstructed on the ground. Our paper explains our methodology and demonstrates the results of the scheme, being able to achieve high quality image compression that is robust to noise and corruption."
"Modeling of video traffic in packet networks, low rate video compression, and the development of a lossy+lossless image compression algorithm",45.276684,jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this reporting period we have worked on three somewhat different problems. These are modeling of video traffic in packet networks, low rate video compression, and the development of a lossy + lossless image compression algorithm, which might have some application in browsing algorithms. The lossy + lossless scheme is an extension of work previously done under this grant. It provides a simple technique for incorporating browsing capability. The low rate coding scheme is also a simple variation on the standard discrete cosine transform (DCT) coding approach. In spite of its simplicity, the approach provides surprisingly high quality reconstructions. The modeling approach is borrowed from the speech recognition literature, and seems to be promising in that it provides a simple way of obtaining an idea about the second order behavior of a particular coding scheme. Details about these are presented."
Synthetic aperture radar signal data compression using block adaptive quantization,43.979523,jpeg quality,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
Methods of evaluating the effects of coding on SAR data,43.94754,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"It is recognized that mean square error (MSE) is not a sufficient criterion for determining the acceptability of an image reconstructed from data that has been compressed and decompressed using an encoding algorithm. In the case of Synthetic Aperture Radar (SAR) data, it is also deemed to be insufficient to display the reconstructed image (and perhaps error image) alongside the original and make a (subjective) judgment as to the quality of the reconstructed data. In this paper we suggest a number of additional evaluation criteria which we feel should be included as evaluation metrics in SAR data encoding experiments. These criteria have been specifically chosen to provide a means of ensuring that the important information in the SAR data is preserved. The paper also presents the results of an investigation into the effects of coding on SAR data fidelity when the coding is applied in (1) the signal data domain, and (2) the image domain. An analysis of the results highlights the shortcomings of the MSE criterion, and shows which of the suggested additional criterion have been found to be most important."
Image-adapted visually weighted quantization matrices for digital image compression,43.406178,jpeg quality,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Learning random networks for compression of still and moving images,42.653008,jpeg quality,['CYBERNETICS'],"Image compression for both still and moving images is an extremely important area of investigation, with numerous applications to videoconferencing, interactive education, home entertainment, and potential applications to earth observations, medical imaging, digital libraries, and many other areas. We describe work on a neural network methodology to compress/decompress still and moving images. We use the 'point-process' type neural network model which is closer to biophysical reality than standard models, and yet is mathematically much more tractable. We currently achieve compression ratios of the order of 120:1 for moving grey-level images, based on a combination of motion detection and compression. The observed signal-to-noise ratio varies from values above 25 to more than 35. The method is computationally fast so that compression and decompression can be carried out in real-time. It uses the adaptive capabilities of a set of neural networks so as to select varying compression ratios in real-time as a function of quality achieved. It also uses a motion detector which will avoid retransmitting portions of the image which have varied little from the previous frame. Further improvements can be achieved by using on-line learning during compression, and by appropriate compensation of nonlinearities in the compression/decompression scheme. We expect to go well beyond the 250:1 compression level for color images with good quality levels."
Evaluation of the VIIRS Land Algorithms at Land PEATE,42.620277,jpeg quality,['Earth Resources and Remote Sensing'],"The Land Product Evaluation and Algorithm Testing Element (Land PEATE), a component of the Science Data Segment of the National Polar-orbiting Operational Environmental Satellite System (NPOESS) Preparatory Project (NPP), is being developed at the NASA Goddard Space Flight Center (GSFC). The primary task of the Land PEATE is to assess the quality of the Visible Infrared Imaging Radiometer Suite (VIIRS) Land data products made by the Interface Data Processing System (IDPS) using the Operational (OPS) Code during the NPP era and to recommend improvements to the algorithms in the IDPS OPS code. The Land PEATE uses a version of the MODIS Adaptive Processing System (MODAPS), NPPDAPS, that has been modified to produce products from the IDPS OPS code and software provided by the VIIRS Science Team, and uses the MODIS Land Data Operational Product Evaluation (LDOPE) team for evaluation of the data records generated by the NPPDAPS. Land PEATE evaluates the algorithms by comparing data products generated using different versions of the algorithm and also by comparing to heritage products generated from different instrument such as MODIS using various quality assessment tools developed at LDOPE. This paper describes the Land PEATE system and some of the approaches used by the Land PEATE for evaluating the VIIRS Land algorithms during the pre-launch period of the NPP mission and the proposed plan for long term monitoring of the quality of the VIIRS Land products post-launch."
The Apollo Lunar Sample Image Collection: Digital Archiving and Online Access,42.160767,jpeg quality,"['Documentation and Information Science', 'Lunar and Planetary Science and Exploration']","The primary goal of the Apollo Program was to land human beings on the Moon and bring them safely back to Earth. This goal was achieved during six missions - Apollo 11, 12, 14, 15, 16, and 17 - that took place between 1969 and 1972. Among the many noteworthy engineering and scientific accomplishments of these missions, perhaps the most important in terms of scientific impact was the return of 382 kg (842 lb.) of lunar rocks, core samples, pebbles, sand, and dust from the lunar surface to Earth. Returned samples were curated at JSC (then known as the Manned Spacecraft Center) and, as part of the original processing, high-quality photographs were taken of each sample. The top, bottom, and sides of each rock sample were photographed, along with 16 stereo image pairs taken at 45-degree intervals. Photographs were also taken whenever a sample was subdivided and when thin sections were made. This collection of lunar sample images consists of roughly 36,000 photographs; all six Apollo missions are represented."
Image data compression having minimum perceptual error,42.114388,jpeg quality,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Compressing subbanded image data with Lempel-Ziv-based coders,41.884743,jpeg quality,['COMMUNICATIONS AND RADAR'],"A method of improving the compression of image data using Lempel-Ziv-based coding is presented. Image data is first processed with a simple transform, such as the Walsh Hadamard Transform, to produce subbands. The subbanded data can be rounded to eight bits or it can be quantized for higher compression at the cost of some reduction in the quality of the reconstructed image. The data is then run-length coded to take advantage of the large runs of zeros produced by quantization. Compression results are presented and contrasted with a subband compression method using quantization followed by run-length coding and Huffman coding. The Lempel-Ziv-based coding in conjunction with run-length coding produces the best compression results at the same reconstruction quality (compared with the Huffman-based coding) on the image data used."
Compressed/reconstructed test images for CRAF/Cassini,41.350533,jpeg quality,['ASTROPHYSICS'],"A set of compressed, then reconstructed, test images submitted to the Comet Rendezvous Asteroid Flyby (CRAF)/Cassini project is presented as part of its evaluation of near lossless high compression algorithms for representing image data. A total of seven test image files were provided by the project. The seven test images were compressed, then reconstructed with high quality (root mean square error of approximately one or two gray levels on an 8 bit gray scale), using discrete cosine transforms or Hadamard transforms and efficient entropy coders. The resulting compression ratios varied from about 2:1 to about 10:1, depending on the activity or randomness in the source image. This was accomplished without any special effort to optimize the quantizer or to introduce special postprocessing to filter the reconstruction errors. A more complete set of measurements, showing the relative performance of the compression algorithms over a wide range of compression ratios and reconstruction errors, shows that additional compression is possible at a small sacrifice in fidelity."
Innovative Video Diagnostic Equipment for Material Science,40.893288,jpeg quality,['Space Processing'],"Materials science experiments under microgravity increasingly rely on advanced optical systems to determine the physical properties of the samples under investigation. This includes video systems with high spatial and temporal resolution. The acquisition, handling, storage and transmission to ground of the resulting video data are very challenging. Since the available downlink data rate is limited, the capability to compress the video data significantly without compromising the data quality is essential. We report on the development of a Digital Video System (DVS) for EML (Electro Magnetic Levitator) which provides real-time video acquisition, high compression using advanced Wavelet algorithms, storage and transmission of a continuous flow of video with different characteristics in terms of image dimensions and frame rates. The DVS is able to operate with the latest generation of high-performance cameras acquiring high resolution video images up to 4Mpixels@60 fps or high frame rate video images up to about 1000 fps@512x512pixels."
Sarnoff JND Vision Model for Flat-Panel Design,39.994873,jpeg quality,['Electronics and Electrical Engineering'],"This document describes adaptation of the basic Sarnoff JND Vision Model created in response to the NASA/ARPA need for a general-purpose model to predict the perceived image quality attained by flat-panel displays. The JND model predicts the perceptual ratings that humans will assign to a degraded color-image sequence relative to its nondegraded counterpart. Substantial flexibility is incorporated into this version of the model so it may be used to model displays at the sub-pixel and sub-frame level. To model a display (e.g., an LCD), the input-image data can be sampled at many times the pixel resolution and at many times the digital frame rate. The first stage of the model downsamples each sequence in time and in space to physiologically reasonable rates, but with minimum interpolative artifacts and aliasing. Luma and chroma parts of the model generate (through multi-resolution pyramid representation) a map of differences-between test and reference called the JND map, from which a summary rating predictor is derived. The latest model extensions have done well in calibration against psychophysical data and against image-rating data given a CRT-based front-end. THe software was delivered to NASA Ames and is being integrated with LCD display models at that facility,"
Sub-band/transform compression of video sequences,39.892036,jpeg quality,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Remote driving with reduced bandwidth communication,39.431343,jpeg quality,['COMMUNICATIONS AND RADAR'],"Oak Ridge National Laboratory has developed a real-time video transmission system for low bandwidth remote operations. The system supports both continuous transmission of video for remote driving and progressive transmission of still images. Inherent in the system design is a spatiotemporal limitation to the effects of channel errors. The average data rate of the system is 64,000 bits/s, a compression of approximately 1000:1 for the black and white National Television Standard Code video. The image quality of the transmissions is maintained at a level that supports teleoperation of a high mobility multipurpose wheeled vehicle at speeds up to 15 mph on a moguled dirt track. Video compression is achieved by using Laplacian image pyramids and a combination of classical techniques. Certain subbands of the image pyramid are transmitted by using interframe differencing with a periodic refresh to aid in bandwidth reduction. Images are also foveated to concentrate image detail in a steerable region. The system supports dynamic video quality adjustments between frame rate, image detail, and foveation rate. A typical configuration for the system used during driving has a frame rate of 4 Hz, a compression per frame of 125:1, and a resulting latency of less than 1s."
The Suomi National Polar-Orbiting Partnership (SNPP): Continuing NASA Research and Applications,39.101555,jpeg quality,['Earth Resources and Remote Sensing']," The Suomi National Polar-orbiting Partnership (SNPP) satellite was successfully launched into a polar orbit on October 28, 2011 carrying 5 remote sensing instruments designed to provide data to improve weather forecasts and to increase understanding of long-term climate change. SNPP provides operational continuity of satellite-based observations for NOAA's Polar-orbiting Operational Environmental Satellites (POES) and continues the long-term record of climate quality observations established by NASA's Earth Observing System (EOS) satellites. In the 2003 to 2011 pre-launch timeframe, NASA's SNPP Science Team assessed the adequacy of the operational Raw Data Records (RDRs), Sensor Data Records (SDRs), and Environmental Data Records (EDRs) from the SNPP instruments for use in NASA Earth Science research, examined the operational algorithms used to produce those data records, and proposed a path forward for the production of climate quality products from SNPP. In order to perform these tasks, a distributed data system, the NASA Science Data Segment (SDS), ingested RDRs, SDRs, and EDRs from the NOAA Archive and Distribution and Interface Data Processing Segments, ADS and IDPS, respectively. The SDS also obtained operational algorithms for evaluation purposes from the NOAA Government Resource for Algorithm Verification, Independent Testing and Evaluation (GRAVITE). Within the NASA SDS, five Product Evaluation and Test Elements (PEATEs) received, ingested, and stored data and performed NASA's data processing, evaluation, and analysis activities. The distributed nature of this data distribution system was established by physically housing each PEATE within one of five Climate Analysis Research Systems (CARS) located at either at a NASA or a university institution. The CARS were organized around 5 key EDRs directly in support of the following NASA Earth Science focus areas: atmospheric sounding, ocean, land, ozone, and atmospheric composition products. The PEATES provided the system level interface with members of the NASA SNPP Science Team and other science investigators within each CARS. A sixth Earth Radiation Budget CARS was established at NASA Langley Research Center (NASA LaRC) to support instrument performance, data evaluation, and analysis for the SNPP Clouds and the Earth's Radiant Budget Energy System (CERES) instrument. Following the 2011 launch of SNPP, spacecraft commissioning, and instrument activation, the NASA SNPP Science Team evaluated the operational RDRs, SDRs, and EDRs produced by the NOAA ADS and IDPS. A key part in that evaluation was the NASA Science Team's independent processing of operational RDRs and SDRs to EDRs using the latest NASA science algorithms. The NASA science evaluation was completed in the December 2012 to April 2014 timeframe with the release of a series of NASA Science Team Discipline Reports. In summary, these reports indicated that the RDRs produced by the SNPP instruments were of sufficiently high quality to be used to create data products suitable for NASA Earth System science and applications. However, the quality of the SDRs and EDRs were found to vary greatly when considering suitability for NASA science. The need for improvements in operational algorithms, adoption of different algorithmic approaches, greater monitoring of on-orbit instrument calibration, greater attention to data product validation, and data reprocessing were prominent findings in the reports. In response to these findings, NASA, in late 2013, directed the NASA SNPP Science Team to use SNPP instrument data to develop data products of sufficiently high quality to enable the continuation of EOS time series data records and to develop innovative, practical applications of SNPP data. This direction necessitated a transition of the SDS data system from its pre-launch assessment mode to one of full data processing and production. To do this, the PEATES, which served as NASA's data product testing environment during the prelaunch and early on-orbit periods, were transitioned to Science Investigator-led Processing Systems (SIPS). The distributed data architecture was maintained in this new system by locating the SIPS at the same institutions at which the CARS and PEATES were located. The SIPS acquire raw SNPP instrument Level 0 (i.e. RDR) data over the full SNPP mission from the NOAA ADS and IDPS through the NASA SDS Data Distribution and Depository Element (SD3E). The SIPS process those data into NASA Level 1, Level 2, and global, gridded Level 3 standard products using peer-reviewed algorithms provided by members of the NASA Science Team. The SIPS work with the NASA SNPP Science Team in obtaining enhanced, refined, or alternate real-time algorithms to support the capabilities of the Direct Readout Laboratory (DRL). All data products, algorithm source codes, coefficients, and auxiliary data used in product generation are archived in an assigned NASA Distributed Active Archive Center (DAAC). "
Clementine High Resolution Camera Mosaicking Project,38.861546,jpeg quality,['Lunar and Planetary Science and Exploration'],"This report constitutes the final report for NASA Contract NASW-5054. This project processed Clementine I high resolution images of the Moon, mosaicked these images together, and created a 22-disk set of compact disk read-only memory (CD-ROM) volumes. The mosaics were produced through semi-automated registration and calibration of the high resolution (HiRes) camera's data against the geometrically and photometrically controlled Ultraviolet/Visible (UV/Vis) Basemap Mosaic produced by the US Geological Survey (USGS). The HiRes mosaics were compiled from non-uniformity corrected, 750 nanometer (""D"") filter high resolution nadir-looking observations. The images were spatially warped using the sinusoidal equal-area projection at a scale of 20 m/pixel for sub-polar mosaics (below 80 deg. latitude) and using the stereographic projection at a scale of 30 m/pixel for polar mosaics. Only images with emission angles less than approximately 50 were used. Images from non-mapping cross-track slews, which tended to have large SPICE errors, were generally omitted. The locations of the resulting image population were found to be offset from the UV/Vis basemap by up to 13 km (0.4 deg.). Geometric control was taken from the 100 m/pixel global and 150 m/pixel polar USGS Clementine Basemap Mosaics compiled from the 750 nm Ultraviolet/Visible Clementine imaging system. Radiometric calibration was achieved by removing the image nonuniformity dominated by the HiRes system's light intensifier. Also provided are offset and scale factors, achieved by a fit of the HiRes data to the corresponding photometrically calibrated UV/Vis basemap, that approximately transform the 8-bit HiRes data to photometric units. The sub-polar mosaics are divided into tiles that cover approximately 1.75 deg. of latitude and span the longitude range of the mosaicked frames. Images from a given orbit are map projected using the orbit's nominal central latitude. Polar mosaics are tiled into squares 2250 pixels on a side, which spans approximately 2.2 deg. Two mosaics are provided for each pole: one corresponding to data acquired while periapsis was in the south, the other while periapsis was in the north. The CD-ROMs also contain ancillary data files that support the HiRes mosaic. These files include browse images with UV/Vis context stored in a Joint Photographic Experts Group (JPEG) format, index files ('imgindx.tab' and 'srcindx.tab') that tabulate the contents of the CD, and documentation files."
Wavelet Compression of Satellite-Transmitted Digital Mammograms,37.521492,jpeg quality,['Life Sciences (General)'],"Breast cancer is one of the major causes of cancer death in women in the United States. The most effective way to treat breast cancer is to detect it at an early stage by screening patients periodically. Conventional film-screening mammography uses X-ray films which are effective in detecting early abnormalities of the breast. Direct digital mammography has the potential to improve the image quality and to take advantages of convenient storage, efficient transmission, and powerful computer-aided diagnosis, etc. One effective alternative to direct digital imaging is secondary digitization of X-ray films. This technique may not provide as high an image quality as the direct digital approach, but definitely have other advantages inherent to digital images. One of them is the usage of satellite-transmission technique for transferring digital mammograms between a remote image-acquisition site and a central image-reading site. This technique can benefit a large population of women who reside in remote areas where major screening and diagnosing facilities are not available. The NASA-Lewis Research Center (LeRC), in collaboration with the Cleveland Clinic Foundation (CCF), has begun a pilot study to investigate the application of the Advanced Communications Technology Satellite (ACTS) network to telemammography. The bandwidth of the T1 transmission is limited (1.544 Mbps) while the size of a mammographic image is huge. It takes a long time to transmit a single mammogram. For example, a mammogram of 4k by 4k pixels with 16 bits per pixel needs more than 4 minutes to transmit. Four images for a typical screening exam would take more than 16 minutes. This is too long a time period for a convenient screening. Consequently, compression is necessary for making satellite-transmission of mammographic images practically possible. The Wavelet Research Group of the Department of Electrical Engineering at The Ohio State University (OSU) participated in the LeRC-CCF collaboration by providing advanced compression technology using wavelet transform. OSU developed a time-efficient software package with various wavelets to compress a serious of mammographic images. This documents reports the result of the compression activities."
Photographic Volume Estimation of CPAS Main Parachutes,37.512486,jpeg quality,['Aerodynamics'],"Capsule Parachute Assembly System (CPAS) flight tests regularly stage a helicopter to observe inflation of 116 ft D o ringsail Main parachutes. These side views can be used to generate 3-D models of inflating canopies to estimate enclosed volume. Assuming a surface of revolution is inadequate because reefed canopies in a cluster are elongated due to mutual aerodynamic interference. A method was developed to combine the side views with upward looking HD video to account for non-circular cross sections. Approximating the cross sections as elliptical greatly improves accuracy. But since that correction requires manually tracing projected outlines, the actual irregular shapes can be used to generate high fidelity models. Compensation is also made for apparent tilt angle. Validation was accomplished by comparing perimeter and projected area with known line lengths and/or high quality photogrammetry. "
"Data compression for data archival, browse or quick-look",35.636105,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Soon after space and Earth science data is collected, it is stored in one or more archival facilities for later retrieval and analysis. Since the purpose of the archival process is to keep an accurate and complete record of data, any data compression used in an archival system must be lossless, and protect against propagation of error in the storage media. A browse capability for space and Earth science data is needed to enable scientists to check the appropriateness and quality of particular data sets before obtaining the full data set(s) for detailed analysis. Browse data produced for these purposes could be used to facilitate the retrieval of data from an archival facility. Quick-look data is data obtained directly from the sensor for either previewing the data or for an application that requires very timely analysis of the space or Earth science data. Two main differences between data compression techniques appropriate to browse and quick-look cases, are that quick-look can be more specifically tailored, and it must be limited in complexity by the relatively limited computational power available on space platforms."
Intelligent Integrated Health Management for a System of Systems,35.478966,jpeg quality,['Systems Analysis and Operations Research'],"An intelligent integrated health management system (IIHMS) incorporates major improvements over prior such systems. The particular IIHMS is implemented for any system defined as a hierarchical distributed network of intelligent elements (HDNIE), comprising primarily: (1) an architecture (Figure 1), (2) intelligent elements, (3) a conceptual framework and taxonomy (Figure 2), and (4) and ontology that defines standards and protocols. Some definitions of terms are prerequisite to a further brief description of this innovation: A system-of-systems (SoS) is an engineering system that comprises multiple subsystems (e.g., a system of multiple possibly interacting flow subsystems that include pumps, valves, tanks, ducts, sensors, and the like); 'Intelligent' is used here in the sense of artificial intelligence. An intelligent element may be physical or virtual, it is network enabled, and it is able to manage data, information, and knowledge (DIaK) focused on determining its condition in the context of the entire SoS; As used here, 'health' signifies the functionality and/or structural integrity of an engineering system, subsystem, or process (leading to determination of the health of components); 'Process' can signify either a physical process in the usual sense of the word or an element into which functionally related sensors are grouped; 'Element' can signify a component (e.g., an actuator, a valve), a process, a controller, an actuator, a subsystem, or a system; The term Integrated System Health Management (ISHM) is used to describe a capability that focuses on determining the condition (health) of every element in a complex system (detect anomalies, diagnose causes, prognosis of future anomalies), and provide data, information, and knowledge (DIaK) not just data to control systems for safe and effective operation. A major novel aspect of the present development is the concept of intelligent integration. The purpose of intelligent integration, as defined and implemented in the present IIHMS, is to enable automated analysis of physical phenomena in imitation of human reasoning, including the use of qualitative methods. Intelligent integration is said to occur in a system in which all elements are intelligent and can acquire, maintain, and share knowledge and information. In the HDNIE of the present IIHMS, an SoS is represented as being operationally organized in a hierarchical-distributed format. The elements of the SoS are considered to be intelligent in that they determine their own conditions within an integrated scheme that involves consideration of data, information, knowledge bases, and methods that reside in all elements of the system. The conceptual framework of the HDNIE and the methodologies of implementing it enable the flow of information and knowledge among the elements so as to make possible the determination of the condition of each element. The necessary information and knowledge is made available to each affected element at the desired time, satisfying a need to prevent information overload while providing context-sensitive information at the proper level of detail. Provision of high-quality data is a central goal in designing this or any IIHMS. In pursuit of this goal, functionally related sensors are logically assigned to groups denoted processes. An aggregate of processes is considered to form a system. Alternatively or in addition to what has been said thus far, the HDNIE of this IIHMS can be regarded as consisting of a framework containing object models that encapsulate all elements of the system, their individual and relational knowledge bases, generic methods and procedures based on models of the applicable physics, and communication processes (Figure 2). The framework enables implementation of a paradigm inspired by how expert operators monitor the health of systems with the help of (1) DIaK from various sources, (2) software tools that assist in rapid visualization of the condition of the system, (3) analical software tools that assist in reasoning about the condition, (4) sharing of information via network communication hardware and software, and (5) software tools that aid in making decisions to remedy unacceptable conditions or improve performance."
Aerospace Technology Innovation,34.035168,jpeg quality,['Aerospace Medicine'],"Whether finding new applications for existing NASA technologies or developing unique marketing strategies to demonstrate them, NASA's offices are committed to identifying unique partnering opportunities. Through their efforts NASA leverages resources through joint research and development, and gains new insight into the core areas relevant to all NASA field centers. One of the most satisfying aspects of my job comes when I learn of a mission-driven technology that can be spun-off to touch the lives of everyday people. NASA's New Partnerships in Medical Diagnostic Imaging is one such initiative. Not only does it promise to provide greater dividends for the country's investment in aerospace research, but also to enhance the American quality of life. This issue of Innovation highlights the new NASA-sponsored initiative in medical imaging. Early in 2001, NASA announced the launch of the New Partnerships in Medical Diagnostic Imaging initiative to promote the partnership and commercialization of NASA technologies in the medical imaging industry. NASA and the medical imaging industry share a number of crosscutting technologies in areas such as high-performance detectors and image-processing tools. Many of the opportunities for joint development and technology transfer to the medical imaging market also hold the promise for future spin back to NASA."
1994 Science Information Management and Data Compression Workshop,33.094456,jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Video transmission on ATM networks,32.915398,jpeg quality,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
Distant Operational Care Centre: Design Project Report,32.345963,jpeg quality,['Astronautics (General)'],"The goal of this project is to outline the design of the Distant Operational Care Centre (DOCC), a modular medical facility to maintain human health and performance in space, that is adaptable to a range of remote human habitats. The purpose of this project is to outline a design, not to go into a complete technical specification of a medical facility for space. This project involves a process to produce a concise set of requirements, addressing the fundamental problems and issues regarding all aspects of a space medical facility for the future. The ideas presented here are at a high level, based on existing, researched, and hypothetical technologies. Given the long development times for space exploration, the outlined concepts from this project embodies a collection of identified problems, and corresponding proposed solutions and ideas, ready to contribute to future space exploration efforts. In order to provide a solid extrapolation and speculation in the context of the future of space medicine, the extent of this project's vision is roughly within the next two decades. The Distant Operational Care Centre (DOCC) is a modular medical facility for space. That is, its function is to maintain human health and performance in space environments, through prevention, diagnosis, and treatment. Furthermore, the DOCC must be adaptable to meet the environmental requirements of different remote human habitats, and support a high quality of human performance. To meet a diverse range of remote human habitats, the DOCC concentrates on a core medical capability that can then be adapted. Adaptation would make use of the DOCC's functional modularity, providing the ability to replace, add, and modify core functions of the DOCC by updating hardware, operations, and procedures. Some of the challenges to be addressed by this project include what constitutes the core medical capability in terms of hardware, operations, and procedures, and how DOCC can be adapted to different remote habitats."
The 1993 Space and Earth Science Data Compression Workshop,31.631958,jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
Toward Better Modeling of Supercritical Turbulent Mixing,31.289448,jpeg quality,['Man/System Technology and Life Support'],"study was done as part of an effort to develop computational models representing turbulent mixing under thermodynamic supercritical (here, high pressure) conditions. The question was whether the large-eddy simulation (LES) approach, developed previously for atmospheric-pressure compressible-perfect-gas and incompressible flows, can be extended to real-gas non-ideal (including supercritical) fluid mixtures. [In LES, the governing equations are approximated such that the flow field is spatially filtered and subgrid-scale (SGS) phenomena are represented by models.] The study included analyses of results from direct numerical simulation (DNS) of several such mixing layers based on the Navier-Stokes, total-energy, and conservation- of-chemical-species governing equations. Comparison of LES and DNS results revealed the need to augment the atmospheric- pressure LES equations with additional SGS momentum and energy terms. These new terms are the direct result of high-density-gradient-magnitude regions found in the DNS and observed experimentally under fully turbulent flow conditions. A model has been derived for the new term in the momentum equation and was found to perform well at small filter size but to deteriorate with increasing filter size. Several alternative models were derived for the new SGS term in the energy equation that would need further investigations to determine if they are too computationally intensive in LES."
The effects of video compression on acceptability of images for monitoring life sciences' experiments,30.982002,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Current plans indicate that there will be a large number of life science experiments carried out during the thirty year-long mission of the Biological Flight Research Laboratory (BFRL) on board Space Station Freedom (SSF). Non-human life science experiments will be performed in the BFRL. Two distinct types of activities have already been identified for this facility: (1) collect, store, distribute, analyze and manage engineering and science data from the Habitats, Glovebox and Centrifuge, (2) perform a broad range of remote science activities in the Glovebox and Habitat chambers in conjunction with the remotely located principal investigator (PI). These activities require extensive video coverage, viewing and/or recording and distribution to video displays on board SSF and to the ground. This paper concentrates mainly on the second type of activity. Each of the two BFRL habitat racks are designed to be configurable for either six rodent habitats per rack, four plant habitats per rack, or a combination of the above. Two video cameras will be installed in each habitat with a spare attachment for a third camera when needed. Therefore, a video system that can accommodate up to 12-18 camera inputs per habitat rack must be considered."
Fractal image compression: A resolution independent representation for imagery,30.981562,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"A deterministic fractal is an image which has low information content and no inherent scale. Because of their low information content, deterministic fractals can be described with small data sets. They can be displayed at high resolution since they are not bound by an inherent scale. A remarkable consequence follows. Fractal images can be encoded at very high compression ratios. This fern, for example is encoded in less than 50 bytes and yet can be displayed at resolutions with increasing levels of detail appearing. The Fractal Transform was discovered in 1988 by Michael F. Barnsley. It is the basis for a new image compression scheme which was initially developed by myself and Michael Barnsley at Iterated Systems. The Fractal Transform effectively solves the problem of finding a fractal which approximates a digital 'real world image'."
Concept report: Experimental vector magnetograph (EXVM) operational configuration balloon flight assembly,30.908962,jpeg quality,['AERODYNAMICS'],"The observational limitations of earth bound solar studies has prompted a great deal of interest in recent months in being able to gain new scientific perspectives through, what should prove to be, relatively low cost flight of the magnetograph system. The ground work done by TBE for the solar balloon missions (originally planned for SOUP and GRID) as well as the rather advanced state of assembly of the EXVM has allowed the quick formulation of a mission concept for the 30 cm system currently being assembled. The flight system operational configuration will be discussed as it is proposed for short duration flight (on the order of one day) over the continental United States. Balloon hardware design requirements used in formulation of the concept are those set by the National Science Balloon Facility (NSBF), the support agency under NASA contract for flight services. The concept assumes that the flight hardware assembly would come together from three development sources: the scientific investigator package, the integration contractor package, and the NSBF support system. The majority of these three separate packages can be independently developed; however, the computer control interfaces and telemetry links would require extensive preplanning and coordination. A special section of this study deals with definition of a dedicated telemetry link to be provided by the integration contractor for video image data for pointing system performance verification. In this study the approach has been to capitalize to the maximum extent possible on existing hardware and system design. This is the most prudent step that can be taken to reduce eventual program cost for long duration flights. By fielding the existing EXVM as quickly as possible, experience could be gained from several short duration flight tests before it became necessary to commit to major upgrades for long duration flights of this system or of the larger 60 cm version being considered for eventual development."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,30.79623,jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
Compression of color-mapped images,30.73765,jpeg quality,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
The Space and Earth Science Data Compression Workshop,30.634165,jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",30.547379,jpeg quality,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
Resiliency of the Multiscale Retinex Image Enhancement Algorithm,30.477976,jpeg quality,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) continues to prove itself in extensive testing to be very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition, However, issues remain with regard to the resiliency of the MSRCR to different image sources and arbitrary image manipulations which may have been applied prior to retinex processing. In this paper we define these areas of concern, provide experimental results, and, examine the effects of commonly occurring image manipulation on retinex performance. In virtually all cases the MSRCR is highly resilient to the effects of both the image source variations and commonly encountered prior image-processing. Significant artifacts are primarily observed for the case of selective color channel clipping in large dark zones in a image. These issues are of concerning the processing of digital image archives and other applications where there is neither control over the image acquisition process, nor knowledge about any processing done on th data beforehand."
"Model-based VQ for image data archival, retrieval and distribution",30.421074,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"An ideal image compression technique for image data archival, retrieval and distribution would be one with the asymmetrical computational requirements of Vector Quantization (VQ), but without the complications arising from VQ codebooks. Codebook generation and maintenance are stumbling blocks which have limited the use of VQ as a practical image compression algorithm. Model-based VQ (MVQ), a variant of VQ described here, has the computational properties of VQ but does not require explicit codebooks. The codebooks are internally generated using mean removed error and Human Visual System (HVS) models. The error model assumed is the Laplacian distribution with mean, lambda-computed from a sample of the input image. A Laplacian distribution with mean, lambda, is generated with uniform random number generator. These random numbers are grouped into vectors. These vectors are further conditioned to make them perceptually meaningful by filtering the DCT coefficients from each vector. The DCT coefficients are filtered by multiplying by a weight matrix that is found to be optimal for human perception. The inverse DCT is performed to produce the conditioned vectors for the codebook. The only image dependent parameter used in the generation of codebook is the mean, lambda, that is included in the coded file to repeat the codebook generation process for decoding."
The NASA Electronic Parts and Packaging (NEPP) Program: Memory and Advanced Processor Plans,30.344046,jpeg quality,"['Quality Assurance and Reliability', 'Electronics and Electrical Engineering']",No abstract available
Image Registration Workshop Proceedings,30.246475,jpeg quality,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Conditional Entropy-Constrained Residual VQ with Application to Image Coding,30.009403,jpeg quality,['Computer Programming and Software'],"This paper introduces an extension of entropy-constrained residual vector quantization (VQ) where intervector dependencies are exploited. The method, which we call conditional entropy-constrained residual VQ, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. When applied to coding images, the proposed method is shown to achieve better rate-distortion performance than that of entropy-constrained residual vector quantization with less computational complexity and lower memory requirements. Moreover, it can be designed to support progressive transmission in a natural way. It is also shown to outperform some of the best predictive and finite-state VQ techniques reported in the literature. This is due partly to the joint optimization between the residual vector quantizer and a high-order conditional entropy coder as well as the efficiency of the multistage residual VQ structure and the dynamic nature of the prediction."
Development of interactive multimedia applications,29.962183,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Multimedia is making an increasingly significant contribution to our informational society. The usefulness of this technology is already evident in education, business presentations, informational kiosks (e.g., in museums), training and the entertainment environment. Institutions, from grade schools to medical schools, are exploring the use of multifaceted electronic text books and teaching aids to enhance course materials. Through multimedia, teachers and students can take full advantage of the cognitive value of animation, audio, video and other types in a seamless application. The Software Technology Branch at NASA Johnson Space Center (NASA/JSC) is taking similar approaches to apply the state-of-the-art technology to space training, mission operations and other applications. This paper discusses the characteristics and development of multimedia applications at the NASA/JSC."
Studies on image compression and image reconstruction,29.937298,jpeg quality,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Introduction to Image Processing,29.58886,jpeg quality,['Instrumentation and Photography'],No abstract available
SEDSAT-1 Technology Development,29.535568,jpeg quality,"['Spacecraft Design, Testing and Performance']","The Students for the Exploration and Development of Space Satellite (SEDSAT-1) is an ambitious project to design, build, and fly a generally-accessible low-cost satellite which will 1) act as a technology demonstration to verify the suitability of novel optical, battery, microprocessor, and memory hardware for space flight environments, (2) to advance the understanding of tether dynamics and environmental science through the development of advanced imaging experiments, (3) to act as a communication link for radio amateurs, and (4) to provide graduate and undergraduate students with a unique multi-disciplinary experience in designing complex real-world hardware/software. This report highlights the progress made on this project during the time period from January 2, 1996 to June 1, 1996 at the end of which time the SEASIS 0.7 version software was completed and integrated on the SEASIS breadboard, a functional prototype of the Panoramic Annual Lenses (PAL) camera was developed, the preferred image compression technique was selected, the layout of the SEASIS board was begun, porting of the SCOS operating system to the command data system (CDS) board was begun, a new design for a tether release mechanism was developed, safety circuitry to inhibit tether cutting was developed and prototyped, material was prepared to support a comprehensive safety review of the project which was held at Johnson Space Center (JSC) (which was personally attended by one of the Principal Investigators), and prototype ground software was developed."
A High Performance Image Data Compression Technique for Space Applications,29.521347,jpeg quality,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
The 1995 Science Information Management and Data Compression Workshop,29.201178,jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Interactive archives of scientific data,29.175777,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"A focus on qualitative methods of presenting data shows that visualization provides a mechanism for browsing independent of the source of data and is an effective alternative to traditional image-based browsing of image data. To be generally applicable, such visualization methods, however, must be based upon an underlying data model with support for a broad class of data types and structures. Interactive, near-real-time browsing for data sets of interesting size today requires a browse server of considerable power. A symmetric multi-processor with very high internal and external bandwidth demonstrates the feasibility of this concept. Although this technology is likely to be available on the desktop within a few years, the increase in the size and complexity of achieved data will continue to exceed the capacity of 'worksation' systems. Hence, a higher class of performance, especially in bandwidth, will generally be required for on-demand browsing. A few experiments with differing digital compression techniques indicates that a MPEG-1 implementation within the context of a high-performance browse server (i.e., parallized) is a practical method of converting a browse product to a form suitable for network or CD-ROM distribution."
Landsat Pathfinder tropical forest information management system,29.099045,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"A Tropical Forest Information Management System_(TFIMS) has been designed to fulfill the needs of HTFIP in such a way that it tracks all aspects of the generation and analysis of the raw satellite data and the derived deforestation dataset. The system is broken down into four components: satellite image selection, processing, data management and archive management. However, as we began to think of how the TFIMS could also be used to make the data readily accessible to all user communities we realized that the initial system was too project oriented and could only be accessed locally. The new system needed development in the areas of data ingest and storage, while at the same time being implemented on a server environment with a network interface accessible via Internet. This paper summarizes the overall design of the existing prototype (version 0) information management system and then presents the design of the new system (version 1). The development of version 1 of the TFIMS is ongoing. There are no current plans for a gradual transition from version 0 to version 1 because the significant changes are in how the data within the HTFIP will be made accessible to the extended community of scientists, policy makers, educators, and students and not in the functionality of the basic system."
Spinoff 2013,29.086874,jpeg quality,['Man/System Technology and Life Support'],"Topics covered include: Innovative Software Tools Measure Behavioral Alertness; Miniaturized, Portable Sensors Monitor Metabolic Health; Patient Simulators Train Emergency Caregivers; Solar Refrigerators Store Life-Saving Vaccines; Monitors Enable Medication Management in Patients' Homes; Handheld Diagnostic Device Delivers Quick Medical Readings; Experiments Result in Safer, Spin-Resistant Aircraft; Interfaces Visualize Data for Airline Safety, Efficiency; Data Mining Tools Make Flights Safer, More Efficient; NASA Standards Inform Comfortable Car Seats; Heat Shield Paves the Way for Commercial Space; Air Systems Provide Life Support to Miners; Coatings Preserve Metal, Stone, Tile, and Concrete; Robots Spur Software That Lends a Hand; Cloud-Based Data Sharing Connects Emergency Managers; Catalytic Converters Maintain Air Quality in Mines; NASA-Enhanced Water Bottles Filter Water on the Go; Brainwave Monitoring Software Improves Distracted Minds; Thermal Materials Protect Priceless, Personal Keepsakes; Home Air Purifiers Eradicate Harmful Pathogens; Thermal Materials Drive Professional Apparel Line; Radiant Barriers Save Energy in Buildings; Open Source Initiative Powers Real-Time Data Streams; Shuttle Engine Designs Revolutionize Solar Power; Procedure-Authoring Tool Improves Safety on Oil Rigs; Satellite Data Aid Monitoring of Nation's Forests; Mars Technologies Spawn Durable Wind Turbines; Programs Visualize Earth and Space for Interactive Education; Processor Units Reduce Satellite Construction Costs; Software Accelerates Computing Time for Complex Math; Simulation Tools Prevent Signal Interference on Spacecraft; Software Simplifies the Sharing of Numerical Models; Virtual Machine Language Controls Remote Devices; Micro-Accelerometers Monitor Equipment Health; Reactors Save Energy, Costs for Hydrogen Production; Cameras Monitor Spacecraft Integrity to Prevent Failures; Testing Devices Garner Data on Insulation Performance; Smart Sensors Gather Information for Machine Diagnostics; Oxygen Sensors Monitor Bioreactors and Ensure Health and Safety; Vision Algorithms Catch Defects in Screen Displays; and Deformable Mirrors Capture Exoplanet Data, Reflect Lasers. "
Restoration and PDS Archive of Apollo Lunar Rock Sample Data,29.011126,jpeg quality,['Lunar and Planetary Science and Exploration'],"In 2008, scientists at the Johnson Space Center (JSC) Lunar Sample Laboratory and Image Science & Analysis Laboratory (under the auspices of the Astromaterials Research and Exploration Science Directorate or ARES) began work on a 4-year project to digitize the original film negatives of Apollo Lunar Rock Sample photographs. These rock samples together with lunar regolith and core samples were collected as part of the lander missions for Apollos 11, 12, 14, 15, 16 and 17. The original film negatives are stored at JSC under cryogenic conditions. This effort is data restoration in the truest sense. The images represent the only record available to scientists which allows them to view the rock samples when making a sample request. As the negatives are being scanned, they are also being formatted and documented for permanent archive in the NASA Planetary Data System (PDS) archive. The ARES group is working collaboratively with the Imaging Node of the PDS on the archiving."
NASA Image eXchange (NIX),28.817488,jpeg quality,['Mathematical and Computer Sciences (General)'],"This paper discusses the technical aspects of and the project background for the NASA Image exchange (NIX). NIX, which provides a single entry point to search selected image databases at the NASA Centers, is a meta-search engine (i.e., a search engine that communicates with other search engines). It uses these distributed digital image databases to access photographs, animations, and their associated descriptive information (meta-data). NIX is available for use at the following URL: http://nix.nasa.gov./NIX, which was sponsored by NASAs Scientific and Technical Information (STI) Program, currently serves images from seven NASA Centers. Plans are under way to link image databases from three additional NASA Centers. images and their associated meta-data, which are accessible by NIX, reside at the originating Centers, and NIX utilizes a virtual central site that communicates with each of these sites. Incorporated into the virtual central site are several protocols to support searches from a diverse collection of database engines. The searches are performed in parallel to ensure optimization of response times. To augment the search capability, browse functionality with pre-defined categories has been built into NIX, thereby ensuring dissemination of 'best-of-breed' imagery. As a final recourse, NIX offers access to a help desk via an on-line form to help locate images and information either within the scope of NIX or from available external sources."
Subband Image Coding with Jointly Optimized Quantizers,28.347385,jpeg quality,['Computer Programming and Software'],"An iterative design algorithm for the joint design of complexity- and entropy-constrained subband quantizers and associated entropy coders is proposed. Unlike conventional subband design algorithms, the proposed algorithm does not require the use of various bit allocation algorithms. Multistage residual quantizers are employed here because they provide greater control of the complexity-performance tradeoffs, and also because they allow efficient and effective high-order statistical modeling. The resulting subband coder exploits statistical dependencies within subbands, across subbands, and across stages, mainly through complexity-constrained high-order entropy coding. Experimental results demonstrate that the complexity-rate-distortion performance of the new subband coder is exceptional."
Apollo Lunar Sample Photographs: Digitizing the Moon Rock Collection,28.308561,jpeg quality,['Instrumentation and Photography'],"The Acquisition and Curation Office at JSC has undertaken a 4-year data restoration project effort for the lunar science community funded by the LASER program (Lunar Advanced Science and Exploration Research) to digitize photographs of the Apollo lunar rock samples and create high resolution digital images. These sample photographs are not easily accessible outside of JSC, and currently exist only on degradable film in the Curation Data Storage Facility"
The NASA Electronic Parts and Packaging (NEPP) Program - NASA Items of Interest,28.285595,jpeg quality,"['Quality Assurance and Reliability', 'Electronics and Electrical Engineering']","This presentation provides a background summary of the NEPP Program, its origins and operating principles followed by examples of issues and opportunities that NEPP is currently pursuing. These examples include Electrostatic Discharge protection procedures that are not being properly applied, one reason for which is the confusion caused by the numerous, different standards covering this topic. Updates are provided for key activities in radiation hardness assurance, and the evaluation of automotive grade electronic parts for use in space applications. Some recent examples of part problems experienced by NASA are briefly described and the latest trending of incidences of counterfeit electronic parts is shown graphically. Finally some forward actions are identified and the time, place and typical topics is provided for the next NEPP Electronic Technology Workshop (ETW)."
The importance of robust error control in data compression applications,28.271046,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
EDOS Data Capture for ALOS,28.067255,jpeg quality,['Computer Systems'],"In 2008, NASA's Earth Sciences Missions Operations (ESMO) at Goddard Space Flight Center (GSFC) directed the Earth Observing System Data Operations System (EDOS) project to provide a prototype system to assess the feasibility of high rate data capture for the Japan Aerospace Exploration Agency's (JAXA) Advanced Land Observing Satellite (ALOS) spacecraft via NASA's Tracking and Data Relay Satellite System (TDRSS). The key objective of this collaborative effort between NASA and JAXA was to share science data collected over North and South America previously unavailable due to limitations in ALOS downlink capacity. EDOS provided a single system proof-of-concept in 4 months at White Sands TDRS Ground Terminal The system captured 6 ALOS events error-free at 277 Mbps and delivered the data to the Alaska Satellite Facility (ASF) within 3 hours (May/June '08). This paper describes the successful rapid prototyping approach which led to a successful demonstration and agreement between NASA and JAXA for operational support. The design of the operational system will be discussed with emphasis on concurrent high-rate data capture, Level-O processing, real-time display and high-rate delivery with stringent latency requirements. A similar solution was successfully deployed at Svalbard, Norway to support the Suomi NPP launch (October 2011) and capture all X-band data and provide a 30-day backup archive."
Electronic Photography at the NASA Langley Research Center,28.026047,jpeg quality,['Documentation and Information Science'],"An electronic photography facility has been established in the Imaging & Photographic Technology Section, Visual Imaging Branch, at the NASA Langley Research Center (LaRC). The purpose of this facility is to provide the LaRC community with access to digital imaging technology. In particular, capabilities have been established for image scanning, direct image capture, optimized image processing for storage, image enhancement, and optimized device dependent image processing for output. Unique approaches include: evaluation and extraction of the entire film information content through scanning; standardization of image file tone reproduction characteristics for optimal bit utilization and viewing; education of digital imaging personnel on the effects of sampling and quantization to minimize image processing related information loss; investigation of the use of small kernel optimal filters for image restoration; characterization of a large array of output devices and development of image processing protocols for standardized output. Currently, the laboratory has a large collection of digital image files which contain essentially all the information present on the original films. These files are stored at 8-bits per color, but the initial image processing was done at higher bit depths and/or resolutions so that the full 8-bits are used in the stored files. The tone reproduction of these files has also been optimized so the available levels are distributed according to visual perceptibility. Look up tables are available which modify these files for standardized output on various devices, although color reproduction has been allowed to float to some extent to allow for full utilization of output device gamut."
Aeronautics and Aviation Science: Careers and Opportunities Project,27.97369,jpeg quality,['Social Sciences (General)'],"The National Aeronautics and Space Administration funded project, Aeronautics and Aviation Science: Careers and Opportunities has been in operation since July, 1995. This project operated as a collaboration with Massachusetts Corporation for Educational Telecommunications, the Federal Aviation Administration, Bridgewater State College and four targeted ""core sites"" in the greater Boston area. In its first and second years, a video series on aeronautics and aviation science was developed and broadcast via ""live, interactive"" satellite feed. Accompanying teacher and student supplementary instructional materials for grades 6-9 were produced and disseminated by the Massachusetts Corporation for Educational Telecommunications (MCET). In the MCET grant application it states that project Take Off! in its initial phase would recruit and train teachers at ""core"" sites in the greater Boston area, as well as opening participation to other on-line users of MCET's satellite feeds. ""Core site"" classrooms would become equipped so that teachers and students might become engaged in an interactive format which aimed at not only involving the students during the ""live"" broadcast of the instructional video series, but which would encourage participation in electronic information gathering and sharing among participants. As a Take Off! project goal, four schools with a higher than average proportion of minority and underrepresented youth were invited to become involved with the project to give these students the opportunity to consider career exploration and development in the field of science aviation and aeronautics. The four sites chosen to participate in this project were: East Boston High School, Dorchester High School, Randolph Junior-Senior High School and Malden High School. In year 3 Dorchester was unable to continue to fully participate and exited out. Danvers was added to the ""core site"" list in year 3. In consideration of Goals 2000, the National Science Foundation standards for quality of teaching, and an educational agenda that promotes high standards for all students, Aeronautics and Aviation Science: Careers and Opportunities had as its aim to deliver products to schools, both in and outside the project sites, which attempt to incorporate multi-disciplined approaches in the presentation of a curriculum which would be appropriate in any classroom, while also aiming to appeal to young women and minorities. The curriculum was developed to provide students with fundamentals of aeronautics and aviation science. The curriculum also encouraged involving students and teachers in research projects, and further information gathering via electronic bulletin boards and internet capabilities. Though not entirely prescriptive, the curriculum was designed to guide teachers through recommended activities to supplement MCET's live telecast video presentations. Classroom teachers were encouraged to invite local pilots, meteorologists, and others from the field of aviation and aeronautics, particularly women and minorities to visit schools and to field questions from the students."
Searching for patterns in remote sensing image databases using neural networks,27.857664,jpeg quality,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
Technology Transfer Report,27.851198,jpeg quality,['Technology Utilization and Surface Transportation'],"Since its inception, Goddard has pursued a commitment to technology transfer and commercialization. For every space technology developed, Goddard strives to identify secondary applications. Goddard then provides the technologies, as well as NASA expertise and facilities, to U.S. companies, universities, and government agencies. These efforts are based in Goddard's Technology Commercialization Office. This report presents new technologies, commercialization success stories, and other Technology Commercialization Office activities in 1999."
Voice and video transmission using XTP and FDDI,27.802359,jpeg quality,['COMMUNICATIONS AND RADAR'],"The use of Xpress Transfer Protocol (XTP) and Fiber Distributed Data Interface (FDDI) provides a high speed and high performance network solution to multimedia transmission that requires high bandwidth. FDDI is an ANSI and ISO standard for a MAC and physical layer protocol that provides a signaling rate of 100 Mbits/sec and fault tolerance. XTP is a transport and network layer protocol designed for high performance and efficiency and is the heart of the SAFENET Lightweight Suite for systems that require performance or realtime communications. The testbed consists of several commercially available Intel based i486 PC's containing off-the-shelf FDDI cards, audio analog-digital converter cards, video interface cards, and XTP software. Unicast, multicast, and duplex audio transmission experiments have been performed using XTP software. Unicast and multicast video transmission is in progress. Several potential commercial applications are described."
Fast image decompression for telebrowsing of images,27.788288,jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Progressive image transmission (PIT) is often used to reduce the transmission time of an image telebrowsing system. A side effect of the PIT is the increase of computational complexity at the viewer's site. This effect is more serious in transform domain techniques than in other techniques. Recent attempts to reduce the side effect are futile as they create another side effect, namely, the discontinuous and unpleasant image build-up. Based on a practical assumption that image blocks to be inverse transformed are generally sparse, this paper presents a method to minimize both side effects simultaneously."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,27.778164,jpeg quality,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
Lunar Robotics Update,27.367334,jpeg quality,"['Cybernetics, Artificial Intelligence and Robotics']",Robotics project status update for the Network for Exploration and Space Science (NESS).
Generation of Precision Stimuli for Web-based and At-home Psychophysics,27.307348,jpeg quality,['Air Transportation and Safety'],"This oral presentation will present methods for linearizing a display to enable the delivery of precisely controlled stimuli in vision experiments, performing colorimetric calibrations using common objects, and testing for spatial and temporal nonlinearities."
Asynchronous transfer mode link performance over ground networks,27.259851,jpeg quality,['COMMUNICATIONS AND RADAR'],"The results of an experiment to determine the feasibility of using asynchronous transfer mode (ATM) technology to support advanced spacecraft missions that require high-rate ground communications and, in particular, full-motion video are reported. Potential nodes in such a ground network include Deep Space Network (DSN) antenna stations, the Jet Propulsion Laboratory, and a set of national and international end users. The experiment simulated a lunar microrover, lunar lander, the DSN ground communications system, and distributed science users. The users were equipped with video-capable workstations. A key feature was an optical fiber link between two high-performance workstations equipped with ATM interfaces. Video was also transmitted through JPL's institutional network to a user 8 km from the experiment. Variations in video depending on the networks and computers were observed, the results are reported."
"PDS Archive Release of Apollo 11, Apollo 12, and Apollo 17 Lunar Rock Sample Images",27.199497,jpeg quality,['Lunar and Planetary Science and Exploration'],"Scientists at the Johnson Space Center (JSC) Lunar Sample Laboratory, Information Resources Directorate, and Image Science & Analysis Laboratory have been working to digitize (scan) the original film negatives of Apollo Lunar Rock Sample photographs [1, 2]. The rock samples, and associated regolith and lunar core samples, were obtained during the Apollo 11, 12, 14, 15, 16 and 17 missions. The images allow scientists to view the individual rock samples in their original or subdivided state prior to requesting physical samples for their research. In cases where access to the actual physical samples is not practical, the images provide an alternate mechanism for study of the subject samples. As the negatives are being scanned, they have been formatted and documented for permanent archive in the NASA Planetary Data System (PDS). The Astromaterials Research and Exploration Science Directorate (which includes the Lunar Sample Laboratory and Image Science & Analysis Laboratory) at JSC is working collaboratively with the Imaging Node of the PDS on the archiving of these valuable data. The PDS Imaging Node is now pleased to announce the release of the image archives for Apollo missions 11, 12, and 17."
"Robo-line storage: Low latency, high capacity storage systems over geographically distributed networks",27.13593,jpeg quality,['COMPUTER OPERATIONS AND HARDWARE'],"Rapid advances in high performance computing are making possible more complete and accurate computer-based modeling of complex physical phenomena, such as weather front interactions, dynamics of chemical reactions, numerical aerodynamic analysis of airframes, and ocean-land-atmosphere interactions. Many of these 'grand challenge' applications are as demanding of the underlying storage system, in terms of their capacity and bandwidth requirements, as they are on the computational power of the processor. A global view of the Earth's ocean chlorophyll and land vegetation requires over 2 terabytes of raw satellite image data. In this paper, we describe our planned research program in high capacity, high bandwidth storage systems. The project has four overall goals. First, we will examine new methods for high capacity storage systems, made possible by low cost, small form factor magnetic and optical tape systems. Second, access to the storage system will be low latency and high bandwidth. To achieve this, we must interleave data transfer at all levels of the storage system, including devices, controllers, servers, and communications links. Latency will be reduced by extensive caching throughout the storage hierarchy. Third, we will provide effective management of a storage hierarchy, extending the techniques already developed for the Log Structured File System. Finally, we will construct a protototype high capacity file server, suitable for use on the National Research and Education Network (NREN). Such research must be a Cornerstone of any coherent program in high performance computing and communications."
Data compression for full motion video transmission,27.011791,jpeg quality,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
The CCDS Data Compression Recommendations: Development and Status,26.930048,jpeg quality,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
"Modeling of video traffic in packet networks, low rate video compression, and the development of a lossy+lossless image compression algorithm",171.73529,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this reporting period we have worked on three somewhat different problems. These are modeling of video traffic in packet networks, low rate video compression, and the development of a lossy + lossless image compression algorithm, which might have some application in browsing algorithms. The lossy + lossless scheme is an extension of work previously done under this grant. It provides a simple technique for incorporating browsing capability. The low rate coding scheme is also a simple variation on the standard discrete cosine transform (DCT) coding approach. In spite of its simplicity, the approach provides surprisingly high quality reconstructions. The modeling approach is borrowed from the speech recognition literature, and seems to be promising in that it provides a simple way of obtaining an idea about the second order behavior of a particular coding scheme. Details about these are presented."
Image Compression Algorithm Altered to Improve Stereo Ranging,150.94383,image lossy algorithm,['Man/System Technology and Life Support'],"A report discusses a modification of the ICER image-data-compression algorithm to increase the accuracy of ranging computations performed on compressed stereoscopic image pairs captured by cameras aboard the Mars Exploration Rovers. (ICER and variants thereof were discussed in several prior NASA Tech Briefs articles.) Like many image compressors, ICER was designed to minimize a mean-square-error measure of distortion in reconstructed images as a function of the compressed data volume. The present modification of ICER was preceded by formulation of an alternative error measure, an image-quality metric that focuses on stereoscopic-ranging quality and takes account of image-processing steps in the stereoscopic-ranging process. This metric was used in empirical evaluation of bit planes of wavelet-transform subbands that are generated in ICER. The present modification, which is a change in a bit-plane prioritization rule in ICER, was adopted on the basis of this evaluation. This modification changes the order in which image data are encoded, such that when ICER is used for lossy compression, better stereoscopic-ranging results are obtained as a function of the compressed data volume."
Subband coding for image data archiving,147.21768,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],The use of subband coding on image data is discussed. An overview of subband coding is given. Advantages of subbanding for browsing and progressive resolution are presented. Implementations for lossless and lossy coding are discussed. Algorithm considerations and simple implementations of subband systems are given.
Subband coding for image data archiving,146.9824,image lossy algorithm,['COMMUNICATIONS AND RADAR'],The use of subband coding on image data is discussed. An overview of subband coding is given. Advantages of subbanding for browsing and progressive resolution are presented. Implementations for lossless and lossy coding are discussed. Algorithm considerations and simple implementations of subband are given.
Resiliency of the Multiscale Retinex Image Enhancement Algorithm,136.93683,image lossy algorithm,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) continues to prove itself in extensive testing to be very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition, However, issues remain with regard to the resiliency of the MSRCR to different image sources and arbitrary image manipulations which may have been applied prior to retinex processing. In this paper we define these areas of concern, provide experimental results, and, examine the effects of commonly occurring image manipulation on retinex performance. In virtually all cases the MSRCR is highly resilient to the effects of both the image source variations and commonly encountered prior image-processing. Significant artifacts are primarily observed for the case of selective color channel clipping in large dark zones in a image. These issues are of concerning the processing of digital image archives and other applications where there is neither control over the image acquisition process, nor knowledge about any processing done on th data beforehand."
Algorithm for Compressing Time-Series Data,136.06857,image lossy algorithm,['Man/System Technology and Life Support'],"An algorithm based on Chebyshev polynomials effects lossy compression of time-series data or other one-dimensional data streams (e.g., spectral data) that are arranged in blocks for sequential transmission. The algorithm was developed for use in transmitting data from spacecraft scientific instruments to Earth stations. In spite of its lossy nature, the algorithm preserves the information needed for scientific analysis. The algorithm is computationally simple, yet compresses data streams by factors much greater than two. The algorithm is not restricted to spacecraft or scientific uses: it is applicable to time-series data in general. The algorithm can also be applied to general multidimensional data that have been converted to time-series data, a typical example being image data acquired by raster scanning. However, unlike most prior image-data-compression algorithms, this algorithm neither depends on nor exploits the two-dimensional spatial correlations that are generally present in images. In order to understand the essence of this compression algorithm, it is necessary to understand that the net effect of this algorithm and the associated decompression algorithm is to approximate the original stream of data as a sequence of finite series of Chebyshev polynomials. For the purpose of this algorithm, a block of data or interval of time for which a Chebyshev polynomial series is fitted to the original data is denoted a fitting interval. Chebyshev approximation has two properties that make it particularly effective for compressing serial data streams with minimal loss of scientific information: The errors associated with a Chebyshev approximation are nearly uniformly distributed over the fitting interval (this is known in the art as the ""equal error property""); and the maximum deviations of the fitted Chebyshev polynomial from the original data have the smallest possible values (this is known in the art as the ""min-max property"")."
The effect of lossy image compression on image classification,134.88861,image lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
The New CCSDS Image Compression Recommendation,132.6488,image lossy algorithm,['Instrumentation and Photography'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-Earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An Application-Specific Integrated Circuit (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images."
Parallel image compression,129.38391,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A parallel compression algorithm for the 16,384 processor MPP machine was developed. The serial version of the algorithm can be viewed as a combination of on-line dynamic lossless test compression techniques (which employ simple learning strategies) and vector quantization. These concepts are described. How these concepts are combined to form a new strategy for performing dynamic on-line lossy compression is discussed. Finally, the implementation of this algorithm in a massively parallel fashion on the MPP is discussed."
Neural Network Repair of Lossy Compression Artifacts in the Sept 2015  March 2016 Duration of the MMS/FPI Dataset,123.09051,image lossy algorithm,['Instrumentation and Photography'],"During the Sept 2015 March 2016 duration (sometimes referred to as Phase 1A) of the Magnetospheric Multiscale Mission (MMS), the Dual Electron Spectrometers (DES) were configured to generously utilize lossy compression. While this maximized the number of velocity distribution functions downlinked, it cameat the expense of lost information content for a fractionof the frames. Following this period of lossy compression, the DES was re-configured in a way that allowed for 95% of the framesto arrive to the ground without loss. Using this high-quality set offrameson-orbit observations, we compressed and decompressed the frameson the ground to create a side-by-side record of the compression effect.  This record was used to drive an optimization method that (a) derived basis functions capable of approximating the lossless sample space and with non-negative coefficients and (b) fitted a function which maps the lossy framesto basis weights that recreate the framewithout compression artifacts. This methodis introduced and evaluated in this paper.Data users should expect a higher level of confidence in the absolute scale of density/temperature measurements andnotice less sinusoidal bias in the velocity X and Y components(GSE)."
Entropy reduction via simplified image contourization,121.751076,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The process of contourization is presented which converts a raster image into a set of plateaux or contours. These contours can be grouped into a hierarchical structure, defining total spatial inclusion, called a contour tree. A contour coder has been developed which fully describes these contours in a compact and efficient manner and is the basis for an image compression method. Simplification of the contour tree has been undertaken by merging contour tree nodes thus lowering the contour tree's entropy. This can be exploited by the contour coder to increase the image compression ratio. By applying general and simple rules derived from physiological experiments on the human vision system, lossy image compression can be achieved which minimizes noticeable artifacts in the simplified image."
Compressing Image Data While Limiting the Effects of Data Losses,121.47366,image lossy algorithm,['Man/System Technology and Life Support'],"ICER is computer software that can perform both lossless and lossy compression and decompression of gray-scale-image data using discrete wavelet transforms. Designed for primary use in transmitting scientific image data from distant spacecraft to Earth, ICER incorporates an error-containment scheme that limits the adverse effects of loss of data and is well suited to the data packets transmitted by deep-space probes. The error-containment scheme includes utilization of the algorithm described in ""Partitioning a Gridded Rectangle Into Smaller Rectangles "" (NPO-30479), NASA Tech Briefs, Vol. 28, No. 7 (July 2004), page 56. ICER has performed well in onboard compression of thousands of images transmitted from the Mars Exploration Rovers."
Planning/scheduling techniques for VQ-based image compression,120.668686,image lossy algorithm,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
Radiometric resolution enhancement by lossy compression as compared to truncation followed by lossless compression,117.81262,image lossy algorithm,['COMPUTER SYSTEMS'],"Recent advances in imaging technology make it possible to obtain imagery data of the Earth at high spatial, spectral and radiometric resolutions from Earth orbiting satellites. The rate at which the data is collected from these satellites can far exceed the channel capacity of the data downlink. Reducing the data rate to within the channel capacity can often require painful trade-offs in which certain scientific returns are sacrificed for the sake of others. In this paper we model the radiometric version of this form of lossy compression by dropping a specified number of least significant bits from each data pixel and compressing the remaining bits using an appropriate lossless compression technique. We call this approach 'truncation followed by lossless compression' or TLLC. We compare the TLLC approach with applying a lossy compression technique to the data for reducing the data rate to the channel capacity, and demonstrate that each of three different lossy compression techniques (JPEG/DCT, VQ and Model-Based VQ) give a better effective radiometric resolution than TLLC for a given channel rate."
The New CCSDS Image Compression Recommendation,115.05443,image lossy algorithm,['Mathematical and Computer Sciences (General)'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists a two dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An ASIC implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm."
Emerging standards for still image compression: A software implementation and simulation study,114.47053,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],The software implementation is described of an emerging standard for the lossy compression of continuous tone still images. This software program can be used to compress planetary images and other 2-D instrument data. It provides a high compression image coding capability that preserves image fidelity at compression rates competitive or superior to most known techniques. This software implementation confirms the usefulness of such data compression and allows its performance to be compared with other schemes used in deep space missions and for data based storage.
ICER-3D Hyperspectral Image Compression Software,112.69887,image lossy algorithm,['Man/System Technology and Life Support'],"Software has been developed to implement the ICER-3D algorithm. ICER-3D effects progressive, three-dimensional (3D), wavelet-based compression of hyperspectral images. If a compressed data stream is truncated, the progressive nature of the algorithm enables reconstruction of hyperspectral data at fidelity commensurate with the given data volume. The ICER-3D software is capable of providing either lossless or lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The compression algorithm, which was derived from the ICER image compression algorithm, includes wavelet-transform, context-modeling, and entropy coding subalgorithms. The 3D wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of sets of hyperspectral image data, while facilitating elimination of spectral ringing artifacts, using a technique summarized in ""Improving 3D Wavelet-Based Compression of Spectral Images"" (NPO-41381), NASA Tech Briefs, Vol. 33, No. 3 (March 2009), page 7a. Correlation is further exploited by a context-modeling subalgorithm, which exploits spectral dependencies in the wavelet-transformed hyperspectral data, using an algorithm that is summarized in ""Context Modeler for Wavelet Compression of Hyperspectral Images"" (NPO-43239), which follows this article. An important feature of ICER-3D is a scheme for limiting the adverse effects of loss of data during transmission. In this scheme, as in the similar scheme used by ICER, the spatial-frequency domain is partitioned into rectangular error-containment regions. In ICER-3D, the partitions extend through all the wavelength bands. The data in each partition are compressed independently of those in the other partitions, so that loss or corruption of data from any partition does not affect the other partitions. Furthermore, because compression is progressive within each partition, when data are lost, any data from that partition received prior to the loss can be used to reconstruct that partition at lower fidelity. By virtue of the compression improvement it achieves relative to previous means of onboard data compression, this software enables (1) increased return of hyperspectral scientific data in the presence of limits on the rates of transmission of data from spacecraft to Earth via radio communication links and/or (2) reduction in spacecraft radio-communication power and/or cost through reduction in the amounts of data required to be downlinked and stored onboard prior to downlink. The software is also suitable for compressing hyperspectral images for ground storage or archival purposes."
Toward an image compression algorithm for the high-resolution electronic still camera,112.3443,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Taking pictures with a camera that uses a digital recording medium instead of film has the advantage of recording and transmitting images without the use of a darkroom or a courier. However, high-resolution images contain an enormous amount of information and strain data-storage systems. Image compression will allow multiple images to be stored in the High-Resolution Electronic Still Camera. The camera is under development at Johnson Space Center. Fidelity of the reproduced image and compression speed are of tantamount importance. Lossless compression algorithms are fast and faithfully reproduce the image, but their compression ratios will be unacceptably low due to noise in the front end of the camera. Future efforts will include exploring methods that will reduce the noise in the image and increase the compression ratio."
"Adjustable lossless image compression based on a natural splitting of an image into drawing, shading, and fine-grained components",111.88964,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The compression, or efficient coding, of single band or multispectral still images is becoming an increasingly important topic. While lossy compression approaches can produce reconstructions that are visually close to the original, many scientific and engineering applications require exact (lossless) reconstructions. However, the most popular and efficient lossless compression techniques do not fully exploit the two-dimensional structural links existing in the image data. We describe here a general approach to lossless data compression that effectively exploits two-dimensional structural links of any length. After describing in detail two main variants on this scheme, we discuss experimental results."
Image Compression Devices,107.27008,image lossy algorithm,['Computer Programming and Software'],"The Rice algorithm is a ""lossless"" compression algorithm; it takes an image or other data that has been broken down into short strings of digital data, then processes each string mathematically to reduce the amount of memory required to store or transmit them. It is particularly useful in medical, scientific or engineering applications where all data must be preserved. Originally developed at Jet Propulsion Laboratory, the technology is marketed by Advanced Hardware Architectures, a company started by a former employee of the NASA Microelectronics Research Center."
Image Registration Workshop Proceedings,106.38079,image lossy algorithm,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Hybrid LZW compression,104.58015,image lossy algorithm,['INSTRUMENTATION AND PHOTOGRAPHY'],"The Science Data Management and Science Payload Operations subpanel reports from the NASA Conference on Scientific Data Compression (Snowbird, Utah in 1988) indicate the need for both lossless and lossy image data compression systems. The ranges developed by the subpanel suggest ratios of 2:1 to 4:1 for lossless coding and 2:1 to 6:1 for lossy predictive coding. For the NASA Freedom Science Video Processing Facility it would be highly desirable to implement one baseline compression system which would meet both of these criteria. Presented here is such a system, utilizing an LZW hybrid coding scheme which is adaptable to either type of compression. Simulation results are presented with the hybrid LZW algorithm operating in each of its modes."
Implementation issues in source coding,101.79195,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],An edge preserving image coding scheme which can be operated in both a lossy and a lossless manner was developed. The technique is an extension of the lossless encoding algorithm developed for the Mars observer spectral data. It can also be viewed as a modification of the DPCM algorithm. A packet video simulator was also developed from an existing modified packet network simulator. The coding scheme for this system is a modification of the mixture block coding (MBC) scheme described in the last report. Coding algorithms for packet video were also investigated.
Information-Adaptive Image Encoding and Restoration,100.719376,image lossy algorithm,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) has shown itself to be a very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition. A number of algorithms exist that provide one or more of these features, but not all. In this paper we compare the performance of the MSRCR with techniques that are widely used for image enhancement. Specifically, we compare the MSRCR with color adjustment methods such as gamma correction and gain/offset application, histogram modification techniques such as histogram equalization and manual histogram adjustment, and other more powerful techniques such as homomorphic filtering and 'burning and dodging'. The comparison is carried out by testing the suite of image enhancement methods on a set of diverse images. We find that though some of these techniques work well for some of these images, only the MSRCR performs universally well oil the test set."
Context Modeler for Wavelet Compression of Spectral Hyperspectral Images,97.49619,image lossy algorithm,['Man/System Technology and Life Support'],"A context-modeling sub-algorithm has been developed as part of an algorithm that effects three-dimensional (3D) wavelet-based compression of hyperspectral image data. The context-modeling subalgorithm, hereafter denoted the context modeler, provides estimates of probability distributions of wavelet-transformed data being encoded. These estimates are utilized by an entropy coding subalgorithm that is another major component of the compression algorithm. The estimates make it possible to compress the image data more effectively than would otherwise be possible. The following background discussion is prerequisite to a meaningful summary of the context modeler. This discussion is presented relative to ICER-3D, which is the name attached to a particular compression algorithm and the software that implements it. The ICER-3D software is summarized briefly in the preceding article, ICER-3D Hyperspectral Image Compression Software (NPO-43238). Some aspects of this algorithm were previously described, in a slightly more general context than the ICER-3D software, in ""Improving 3D Wavelet-Based Compression of Hyperspectral Images"" (NPO-41381), NASA Tech Briefs, Vol. 33, No. 3 (March 2009), page 7a. In turn, ICER-3D is a product of generalization of ICER, another previously reported algorithm and computer program that can perform both lossless and lossy wavelet-based compression and decompression of gray-scale-image data. In ICER-3D, hyperspectral image data are decomposed using a 3D discrete wavelet transform (DWT). Following wavelet decomposition, mean values are subtracted from spatial planes of spatially low-pass subbands prior to encoding. The resulting data are converted to sign-magnitude form and compressed. In ICER-3D, compression is progressive, in that compressed information is ordered so that as more of the compressed data stream is received, successive reconstructions of the hyperspectral image data are of successively higher overall fidelity."
Lossless Astronomical Image Compression and the Effects of Random Noise,97.46939,image lossy algorithm,['Documentation and Information Science'],"In this paper we compare a variety of modern image compression methods on a large sample of astronomical images. We begin by demonstrating from first principles how the amount of noise in the image pixel values sets a theoretical upper limit on the lossless compression ratio of the image. We derive simple procedures for measuring the amount of noise in an image and for quantitatively predicting how much compression will be possible. We then compare the traditional technique of using the GZIP utility to externally compress the image, with a newer technique of dividing the image into tiles, and then compressing and storing each tile in a FITS binary table structure. This tiled-image compression technique offers a choice of other compression algorithms besides GZIP, some of which are much better suited to compressing astronomical images. Our tests on a large sample of images show that the Rice algorithm provides the best combination of speed and compression efficiency. In particular, Rice typically produces 1.5 times greater compression and provides much faster compression speed than GZIP. Floating point images generally contain too much noise to be effectively compressed with any lossless algorithm. We have developed a compression technique which discards some of the useless noise bits by quantizing the pixel values as scaled integers. The integer images can then be compressed by a factor of 4 or more. Our image compression and uncompression utilities (called fpack and funpack) that were used in this study are publicly available from the HEASARC web site.Users may run these stand-alone programs to compress and uncompress their own images."
Compression through decomposition into browse and residual images,97.04849,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Economical archival and retrieval of image data is becoming increasingly important considering the unprecedented data volumes expected from the Earth Observing System (EOS) instruments. For cost effective browsing the image data (possibly from remote site), and retrieving the original image data from the data archive, we suggest an integrated image browse and data archive system employing incremental transmission. We produce our browse image data with the JPEG/DCT lossy compression approach. Image residual data is then obtained by taking the pixel by pixel differences between the original data and the browse image data. We then code the residual data with a form of variable length coding called diagonal coding. In our experiments, the JPEG/DCT is used at different quality factors (Q) to generate the browse and residual data. The algorithm has been tested on band 4 of two Thematic mapper (TM) data sets. The best overall compression ratios (of about 1.7) were obtained when a quality factor of Q=50 was used to produce browse data at a compression ratio of 10 to 11. At this quality factor the browse image data has virtually no visible distortions for the images tested."
Studies on image compression and image reconstruction,96.18113,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Locally adaptive vector quantization: Data compression with feature preservation,96.074265,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
High-performance compression of astronomical images,94.0699,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Astronomical images have some rather unusual characteristics that make many existing image compression techniques either ineffective or inapplicable. A typical image consists of a nearly flat background sprinkled with point sources and occasional extended sources. The images are often noisy, so that lossless compression does not work very well; furthermore, the images are usually subjected to stringent quantitative analysis, so any lossy compression method must be proven not to discard useful information, but must instead discard only the noise. Finally, the images can be extremely large. For example, the Space Telescope Science Institute has digitized photographic plates covering the entire sky, generating 1500 images each having 14000 x 14000 16-bit pixels. Several astronomical groups are now constructing cameras with mosaics of large CCD's (each 2048 x 2048 or larger); these instruments will be used in projects that generate data at a rate exceeding 100 MBytes every 5 minutes for many years. An effective technique for image compression may be based on the H-transform (Fritze et al. 1977). The method that we have developed can be used for either lossless or lossy compression. The digitized sky survey images can be compressed by at least a factor of 10 with no noticeable losses in the astrometric and photometric properties of the compressed images. The method has been designed to be computationally efficient: compression or decompression of a 512 x 512 image requires only 4 seconds on a Sun SPARCstation 1. The algorithm uses only integer arithmetic, so it is completely reversible in its lossless mode, and it could easily be implemented in hardware for space applications."
Searching for patterns in remote sensing image databases using neural networks,91.99991,image lossy algorithm,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
"Model-based VQ for image data archival, retrieval and distribution",91.89021,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"An ideal image compression technique for image data archival, retrieval and distribution would be one with the asymmetrical computational requirements of Vector Quantization (VQ), but without the complications arising from VQ codebooks. Codebook generation and maintenance are stumbling blocks which have limited the use of VQ as a practical image compression algorithm. Model-based VQ (MVQ), a variant of VQ described here, has the computational properties of VQ but does not require explicit codebooks. The codebooks are internally generated using mean removed error and Human Visual System (HVS) models. The error model assumed is the Laplacian distribution with mean, lambda-computed from a sample of the input image. A Laplacian distribution with mean, lambda, is generated with uniform random number generator. These random numbers are grouped into vectors. These vectors are further conditioned to make them perceptually meaningful by filtering the DCT coefficients from each vector. The DCT coefficients are filtered by multiplying by a weight matrix that is found to be optimal for human perception. The inverse DCT is performed to produce the conditioned vectors for the codebook. The only image dependent parameter used in the generation of codebook is the mean, lambda, that is included in the coded file to repeat the codebook generation process for decoding."
A High Performance Image Data Compression Technique for Space Applications,91.45092,image lossy algorithm,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
A Real-Time High Performance Data Compression Technique For Space Applications,90.02106,image lossy algorithm,['Earth Resources and Remote Sensing'],A high performance lossy data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on block-transform combined with bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate. The lossy coder is described. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Hardware implementations are in development; a functional chip set is expected by the end of 2001.
Image compression system and method having optimized quantization tables,89.91849,image lossy algorithm,['Instrumentation and Photography'],"A digital image compression preprocessor for use in a discrete cosine transform-based digital image compression device is provided. The preprocessor includes a gathering mechanism for determining discrete cosine transform statistics from input digital image data. A computing mechanism is operatively coupled to the gathering mechanism to calculate a image distortion array and a rate of image compression array based upon the discrete cosine transform statistics for each possible quantization value. A dynamic programming mechanism is operatively coupled to the computing mechanism to optimize the rate of image compression array against the image distortion array such that a rate-distortion-optimal quantization table is derived. In addition, a discrete cosine transform-based digital image compression device and a discrete cosine transform-based digital image compression and decompression system are provided. Also, a method for generating a rate-distortion-optimal quantization table, using discrete cosine transform-based digital image compression, and operating a discrete cosine transform-based digital image compression and decompression system are provided."
Non-linear Post Processing Image Enhancement,89.760574,image lossy algorithm,"['Cybernetics, Artificial Intelligence and Robotics']","A non-linear filter for image post processing based on the feedforward Neural Network topology is presented. This study was undertaken to investigate the usefulness of ""smart"" filters in image post processing. The filter has shown to be useful in recovering high frequencies, such as those lost during the JPEG compression-decompression process. The filtered images have a higher signal to noise ratio, and a higher perceived image quality. Simulation studies comparing the proposed filter with the optimum mean square non-linear filter, showing examples of the high frequency recovery, and the statistical properties of the filter are given,"
Compression of color-mapped images,87.70378,image lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
The CCDS Data Compression Recommendations: Development and Status,85.140915,image lossy algorithm,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
Lossless compression of image data products on th e FIFE CD-ROM series,84.953316,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"How do you store enough of the key data sets, from a total of 120 gigabytes of data collected for a scientific experiment, on a collection of CD-ROM's, small enough to distribute to a broad scientific community? In such an application where information loss in unacceptable, lossless compression algorithms are the only choice. Although lossy compression algorithms can provide an order of magnitude improvement in compression ratios over lossless algorithms the information that is lost is often part of the key scientific precision of the data. Therefore, lossless compression algorithms are and will continue to be extremely important in minimizing archiving storage requirements and distribution of large earth and space (ESS) data sets while preserving the essential scientific precision of the data."
A high-speed distortionless predictive image-compression scheme,83.12264,image lossy algorithm,['COMMUNICATIONS AND RADAR'],A high-speed distortionless predictive image-compression scheme that is based on differential pulse code modulation output modeling combined with efficient source-code design is introduced. Experimental results show that this scheme achieves compression that is very close to the difference entropy of the source.
Real-time demonstration hardware for enhanced DPCM video compression algorithm,82.861244,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"The lack of available wideband digital links as well as the complexity of implementation of bandwidth efficient digital video CODECs (encoder/decoder) has worked to keep the cost of digital television transmission too high to compete with analog methods. Terrestrial and satellite video service providers, however, are now recognizing the potential gains that digital video compression offers and are proposing to incorporate compression systems to increase the number of available program channels. NASA is similarly recognizing the benefits of and trend toward digital video compression techniques for transmission of high quality video from space and therefore, has developed a digital television bandwidth compression algorithm to process standard National Television Systems Committee (NTSC) composite color television signals. The algorithm is based on differential pulse code modulation (DPCM), but additionally utilizes a non-adaptive predictor, non-uniform quantizer and multilevel Huffman coder to reduce the data rate substantially below that achievable with straight DPCM. The non-adaptive predictor and multilevel Huffman coder combine to set this technique apart from other DPCM encoding algorithms. All processing is done on a intra-field basis to prevent motion degradation and minimize hardware complexity. Computer simulations have shown the algorithm will produce broadcast quality reconstructed video at an average transmission rate of 1.8 bits/pixel. Hardware implementation of the DPCM circuit, non-adaptive predictor and non-uniform quantizer has been completed, providing realtime demonstration of the image quality at full video rates. Video sampling/reconstruction circuits have also been constructed to accomplish the analog video processing necessary for the real-time demonstration. Performance results for the completed hardware compare favorably with simulation results. Hardware implementation of the multilevel Huffman encoder/decoder is currently under development along with implementation of a buffer control algorithm to accommodate the variable data rate output of the multilevel Huffman encoder. A video CODEC of this type could be used to compress NTSC color television signals where high quality reconstruction is desirable (e.g., Space Station video transmission, transmission direct-to-the-home via direct broadcast satellite systems or cable television distribution to system headends and direct-to-the-home)."
Image Edge Extraction via Fuzzy Reasoning,82.82294,image lossy algorithm,['Instrumentation and Photography'],"A computer-based technique for detecting edges in gray level digital images employs fuzzy reasoning to analyze whether each pixel in an image is likely on an edge. The image is analyzed on a pixel-by-pixel basis by analyzing gradient levels of pixels in a square window surrounding the pixel being analyzed. An edge path passing through the pixel having the greatest intensity gradient is used as input to a fuzzy membership function, which employs fuzzy singletons and inference rules to assigns a new gray level value to the pixel that is related to the pixel's edginess degree."
Image edge extraction via fuzzy reasoning,82.44262,image lossy algorithm,['Mathematical and Computer Sciences (General)'],"A computer-based technique for detecting edges in gray level digital images employs fuzzy reasoning to analyze whether each pixel in an image is likely on an edge. The image is analyzed on a pixel-by-pixel basis by analyzing gradient levels of pixels in a square window surrounding the pixel being analyzed. An edge path passing through the pixel having the greatest intensity gradient is used as input to a fuzzy membership function, which employs fuzzy singletons and inference rules to assigns a new gray level value to the pixel that is related to the pixel's edginess degree."
Digital Image Compression Using Artificial Neural Networks,81.723175,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The problem of storing, transmitting, and manipulating digital images is considered. Because of the file sizes involved, large amounts of digitized image information are becoming common in modern projects. Our goal is to described an image compression transform coder based on artificial neural networks techniques (NNCTC). A comparison of the compression results obtained from digital astronomical images by the NNCTC and the method used in the compression of the digitized sky survey from the Space Telescope Science Institute based on the H-transform is performed in order to assess the reliability of the NNCTC."
System considerations for efficient communication and storage of MSTI image data,81.43067,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,80.03715,image lossy algorithm,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
Pre-Processor for Compression of Multispectral Image Data,79.59639,image lossy algorithm,['Computer Programming and Software'],A computer program that preprocesses multispectral image data has been developed to provide the Mars Exploration Rover (MER) mission with a means of exploiting the additional correlation present in such data without appreciably increasing the complexity of compressing the data.
A survey of quality measures for gray-scale image compression,79.49273,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Although a variety of techniques are available today for gray-scale image compression, a complete evaluation of these techniques cannot be made as there is no single reliable objective criterion for measuring the error in compressed images. The traditional subjective criteria are burdensome, and usually inaccurate or inconsistent. On the other hand, being the most common objective criterion, the mean square error (MSE) does not have a good correlation with the viewer's response. It is now understood that in order to have a reliable quality measure, a representative model of the complex human visual system is required. In this paper, we survey and give a classification of the criteria for the evaluation of monochrome image quality."
"Multi-rate, real time image compression for images dominated by point sources",76.38721,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"An image compression system recently developed for compression of digital images dominated by point sources is presented. Encoding consists of minimum-mean removal, vector quantization, adaptive threshold truncation, and modified Huffman encoding. Simulations are presented showing that the peaks corresponding to point sources can be transmitted losslessly for low signal-to-noise ratios (SNR) and high point source densities while maintaining a reduced output bit rate. Encoding and decoding hardware has been built and tested which processes 552,960 12-bit pixels per second at compression rates of 10:1 and 4:1. Simulation results are presented for the 10:1 case only."
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,76.33254,image lossy algorithm,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
Basic research planning in mathematical pattern recognition and image analysis,76.31229,image lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"Fundamental problems encountered while attempting to develop automated techniques for applications of remote sensing are discussed under the following categories: (1) geometric and radiometric preprocessing; (2) spatial, spectral, temporal, syntactic, and ancillary digital image representation; (3) image partitioning, proportion estimation, and error models in object scene interference; (4) parallel processing and image data structures; and (5) continuing studies in polarization; computer architectures and parallel processing; and the applicability of ""expert systems"" to interactive analysis."
Reduction of blocking effects for the JPEG baseline image compression standard,76.12016,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Transform coding has been chosen for still image compression in the Joint Photographic Experts Group (JPEG) standard. Although transform coding performs superior to many other image compression methods and has fast algorithms for implementation, it is limited by a blocking effect at low bit rates. The blocking effect is inherent in all nonoverlapping transforms. This paper presents a technique for reducing blocking while remaining compatible with the JPEG standard. Simulations show that the system results in subjective performance improvements, sacrificing only a marginal increase in bit rate."
Progressive Vector Quantization on a massively parallel SIMD machine with application to multispectral image data,75.15265,image lossy algorithm,['COMPUTER SYSTEMS'],"A progressive vector quantization (VQ) compression approach is discussed which decomposes image data into a number of levels using full search VQ. The final level is losslessly compressed, enabling lossless reconstruction. The computational difficulties are addressed by implementation on a massively parallel SIMD machine. We demonstrate progressive VQ on multispectral imagery obtained from the Advanced Very High Resolution Radiometer instrument and other Earth observation image data, and investigate the trade-offs in selecting the number of decomposition levels and codebook training method."
Optimal Compression of Floating-Point Astronomical Images Without Significant Loss of Information,73.81737,image lossy algorithm,['Documentation and Information Science'],"We describe a compression method for floating-point astronomical images that gives compression ratios of 6 - 10 while still preserving the scientifically important information in the image. The pixel values are first preprocessed by quantizing them into scaled integer intensity levels, which removes some of the uncompressible noise in the image. The integers are then losslessly compressed using the fast and efficient Rice algorithm and stored in a portable FITS format file. Quantizing an image more coarsely gives greater image compression, but it also increases the noise and degrades the precision of the photometric and astrometric measurements in the quantized image. Dithering the pixel values during the quantization process greatly improves the precision of measurements in the more coarsely quantized images. We perform a series of experiments on both synthetic and real astronomical CCD images to quantitatively demonstrate that the magnitudes and positions of stars in the quantized images can be measured with the predicted amount of precision. In order to encourage wider use of these image compression methods, we have made available a pair of general-purpose image compression programs, called fpack and funpack, which can be used to compress any FITS format image."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,72.898315,image lossy algorithm,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Calibration and Image Reconstruction for the Hurricane Imaging Radiometer (HIRAD),72.830086,image lossy algorithm,['Instrumentation and Photography'],"The Hurricane Imaging Radiometer (HIRAD) is a new airborne passive microwave synthetic aperture radiometer designed to provide wide swath images of ocean surface wind speed under heavy precipitation and, in particular, in tropical cyclones. It operates at 4, 5, 6 and 6.6 GHz and uses interferometric signal processing to synthesize a pushbroom imager in software from a low profile planar antenna with no mechanical scanning. HIRAD participated in NASA s Genesis and Rapid Intensification Processes (GRIP) mission during Fall 2010 as its first science field campaign. HIRAD produced images of upwelling brightness temperature over a aprox 70 km swath width with approx 3 km spatial resolution. From this, ocean surface wind speed and column averaged atmospheric liquid water content can be retrieved across the swath. The calibration and image reconstruction algorithms that were used to verify HIRAD functional performance during and immediately after GRIP were only preliminary and used a number of simplifying assumptions and approximations about the instrument design and performance. The development and performance of a more detailed and complete set of algorithms are reported here."
Proposed data compression schemes for the Galileo S-band contingency mission,69.18493,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Galileo spacecraft is currently on its way to Jupiter and its moons. In April 1991, the high gain antenna (HGA) failed to deploy as commanded. In case the current efforts to deploy the HGA fails, communications during the Jupiter encounters will be through one of two low gain antenna (LGA) on an S-band (2.3 GHz) carrier. A lot of effort has been and will be conducted to attempt to open the HGA. Also various options for improving Galileo's telemetry downlink performance are being evaluated in the event that the HGA will not open at Jupiter arrival. Among all viable options the most promising and powerful one is to perform image and non-image data compression in software onboard the spacecraft. This involves in-flight re-programming of the existing flight software of Galileo's Command and Data Subsystem processors and Attitude and Articulation Control System (AACS) processor, which have very limited computational and memory resources. In this article we describe the proposed data compression algorithms and give their respective compression performance. The planned image compression algorithm is a 4 x 4 or an 8 x 8 multiplication-free integer cosine transform (ICT) scheme, which can be viewed as an integer approximation of the popular discrete cosine transform (DCT) scheme. The implementation complexity of the ICT schemes is much lower than the DCT-based schemes, yet the performances of the two algorithms are indistinguishable. The proposed non-image compression algorith is a Lempel-Ziv-Welch (LZW) variant, which is a lossless universal compression algorithm based on a dynamic dictionary lookup table. We developed a simple and efficient hashing function to perform the string search."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,68.30559,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
Introduction to Image Processing,67.30678,image lossy algorithm,['Instrumentation and Photography'],No abstract available
Motion video compression system with neural network having winner-take-all function,65.58855,image lossy algorithm,['Instrumentation and Photography'],"A motion video data system includes a compression system, including an image compressor, an image decompressor correlative to the image compressor having an input connected to an output of the image compressor, a feedback summing node having one input connected to an output of the image decompressor, a picture memory having an input connected to an output of the feedback summing node, apparatus for comparing an image stored in the picture memory with a received input image and deducing therefrom pixels having differences between the stored image and the received image and for retrieving from the picture memory a partial image including the pixels only and applying the partial image to another input of the feedback summing node, whereby to produce at the output of the feedback summing node an updated decompressed image, a subtraction node having one input connected to received the received image and another input connected to receive the partial image so as to generate a difference image, the image compressor having an input connected to receive the difference image whereby to produce a compressed difference image at the output of the image compressor."
Data Compression Techniques for Advanced Space Transportation Systems,64.414085,image lossy algorithm,['Documentation and Information Science'],"Advanced space transportation systems, including vehicle state of health systems, will produce large amounts of data which must be stored on board the vehicle and or transmitted to the ground and stored. The cost of storage or transmission of the data could be reduced if the number of bits required to represent the data is reduced by the use of data compression techniques. Most of the work done in this study was rather generic and could apply to many data compression systems, but the first application area to be considered was launch vehicle state of health telemetry systems. Both lossless and lossy compression techniques were considered in this study."
Study of efficient transmission and  reception of image-type data using millimeter waves,64.2967,image lossy algorithm,['COMMUNICATIONS'],Evaluation of signal processing and modulation techniques for transmission and reception of image type data via millimeter wave relay satellites
Study of Efficient Transmission and Reception of Image-type Data Using Millimeter Waves Interim Engineering Report,64.16211,image lossy algorithm,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites
Technology Directions for the 21st Century,62.246788,image lossy algorithm,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
Data compression experiments with LANDSAT thematic mapper and Nimbus-7 coastal zone color scanner data,61.894608,image lossy algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"A case study is presented where an image segmentation based compression technique is applied to LANDSAT Thematic Mapper (TM) and Nimbus-7 Coastal Zone Color Scanner (CZCS) data. The compression technique, called Spatially Constrained Clustering (SCC), can be regarded as an adaptive vector quantization approach. The SCC can be applied to either single or multiple spectral bands of image data. The segmented image resulting from SCC is encoded in small rectangular blocks, with the codebook varying from block to block. Lossless compression potential (LDP) of sample TM and CZCS images are evaluated. For the TM test image, the LCP is 2.79. For the CZCS test image the LCP is 1.89, even though when only a cloud-free section of the image is considered the LCP increases to 3.48. Examples of compressed images are shown at several compression ratios ranging from 4 to 15. In the case of TM data, the compressed data are classified using the Bayes' classifier. The results show an improvement in the similarity between the classification results and ground truth when compressed data are used, thus showing that compression is, in fact, a useful first step in the analysis."
Automatic commanding of the Mars Observer Camera,61.17749,image lossy algorithm,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Mars Observer, launched in September 1992, was intended to be a 'survey-type' mission that acquired global coverage of Mars from a low, circular, near-polar orbit during an entire Martian year. As such, most of its instruments had fixed data rates, wide fields of view, and relatively low resolution, with fairly limited requirements for commanding. An exception is the Mars Observer Camera, or MOC. The MOC consists of a two-color Wide Angle (WA) system that can acquire both global images at low resolution (7.5 km/pixel) and regional images at commandable resolutions up to 250 m/pixel. Complementing the WA is the Narrow Angle (NA) system, that can acquire images at 8 resolutions from 12 m/pixel to 1.5 m/pixel, with a maximum crosstrack dimension of 3 km. The MOC also provides various forms of data compression (both lossless and lossy), and is designed to work at data rates from 700 bits per second (bps) to over 80k bps. Because of this flexibility, developing MOC command sequences is much more difficult than the routine mode-changing that characterizes other instrument operations. Although the MOC cannot be pointed (the spacecraft is fixed nadir-pointing and has no scan platform), the timing, downlink stream allocation, compression type and parameters, and image dimensions of each image must be commanded from the ground, subject to the constraints inherent in the MOC and the spacecraft. To minimize the need for a large operations staff, the entire command generation process has been automated within the MOC Ground Data System. Following the loss of the Mars Observer spacecraft in August 1993, NASA intends to launch a new spacecraft, Mars Global Surveyor (MGS), in late 1996. This spacecraft will carry the MOC flight spare (MOC 2). The MOC 2 operations plan will be largely identical to that developed for MOC, and all of the algorithms described here are applicable to it."
Data compression using adaptive transform coding. Appendix 1: Item 1,60.8842,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Adaptive low-rate source coders are described in this dissertation. These coders adapt by adjusting the complexity of the coder to match the local coding difficulty of the image. This is accomplished by using a threshold driven maximum distortion criterion to select the specific coder used. The different coders are built using variable blocksized transform techniques, and the threshold criterion selects small transform blocks to code the more difficult regions and larger blocks to code the less complex regions. A theoretical framework is constructed from which the study of these coders can be explored. An algorithm for selecting the optimal bit allocation for the quantization of transform coefficients is developed. The bit allocation algorithm is more fully developed, and can be used to achieve more accurate bit assignments than the algorithms currently used in the literature. Some upper and lower bounds for the bit-allocation distortion-rate function are developed. An obtainable distortion-rate function is developed for a particular scalar quantizer mixing method that can be used to code transform coefficients at any rate."
Pre-coding method and apparatus for multiple source or time-shifted single source data and corresponding inverse post-decoding method and apparatus,59.88886,image lossy algorithm,['Computer Systems'],"A pre-coding method and device for improving data compression performance by removing correlation between a first original data set and a second original data set, each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference data set. The method and device produce a double-difference data set, i.e., an adjacent-delta calculation performed on a cross-delta data set or a cross-delta calculation performed on two adjacent-delta data sets, from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted data sets coming from a single source. The resulting double-difference data set is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original data set having been represented by such a double-difference data set."
"SAR data compression: Application, requirements, and designs",59.76647,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The feasibility of reducing data volume and data rate is evaluated for the Earth Observing System (EOS) Synthetic Aperture Radar (SAR). All elements of data stream from the sensor downlink data stream to electronic delivery of browse data products are explored. The factors influencing design of a data compression system are analyzed, including the signal data characteristics, the image quality requirements, and the throughput requirements. The conclusion is that little or no reduction can be achieved in the raw signal data using traditional data compression techniques (e.g., vector quantization, adaptive discrete cosine transform) due to the induced phase errors in the output image. However, after image formation, a number of techniques are effective for data compression."
Progressive transmission of pseudo-color images. Appendix 1: Item 4,59.56488,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"The transmission of digital images can require considerable channel bandwidth. The cost of obtaining such a channel can be prohibitive, or the channel might simply not be available. In this case, progressive transmission (PT) can be useful. PT presents the user with a coarse initial image approximation, and then proceeds to refine it. In this way, the user tends to receive information about the content of the image sooner than if a sequential transmission method is used. PT finds application in image data base browsing, teleconferencing, medical and other applications. A PT scheme is developed for use with a particular type of image data, the pseudo-color or color mapped image. Such images consist of a table of colors called a colormap, plus a 2-D array of index values which indicate which colormap entry is to be used to display a given pixel. This type of image presents some unique problems for a PT coder, and techniques for overcoming these problems are developed. A computer simulation of the color mapped PT scheme is developed to evaluate its performance. Results of simulation using several test images are presented."
Effects of Tunable Data Compression on Geophysical Products Retrieved from Surface Radar Observations with Applications to Spaceborne Meteorological Radars,59.13627,image lossy algorithm,['Earth Resources and Remote Sensing'],"This paper presents results and analyses of applying an international space data compression standard to weather radar measurements that can easily span 8 orders of magnitude and typically require a large storage capacity as well as significant bandwidth for transmission. By varying the degree of the data compression, we analyzed the non-linear response of models that relate measured radar reflectivity and/or Doppler spectra to the moments and properties of the particle size distribution characterizing clouds and precipitation. Preliminary results for the meteorologically important phenomena of clouds and light rain indicate that for a 0.5 dB calibration uncertainty, typical for the ground-based pulsed-Doppler 94 GHz (or 3.2 mm, W-band) weather radar used as a proxy for spaceborne radar in this study, a lossless compression ratio of only 1.2 is achievable. However, further analyses of the non-linear response of various models of rainfall rate, liquid water content and median volume diameter show that a lossy data compression ratio exceeding 15 is realizable. The exploratory analyses presented are relevant to future satellite missions, where the transmission bandwidth is premium and storage requirements of vast volumes of data, potentially problematic."
A browse facility for Earth science remote sensing data: Center director's discretionary fund final report,59.079674,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"An image data visual browse facility is developed for a UNIX platform using the X Windows 11 system. It allows one to visually examine reduced resolution image data to determine which data are applicable for further research. Links with a relational data base manager then allow one to extract not only the full resolution image data, but any other ancillary data related to the case study. Various techniques are examined for compression of the image data in order to reduce data storage requirements and time necessary to transmit the data on the internet. Data used were from the WetNet project."
Pre-coding method and apparatus for multiple source or time-shifted single source data and corresponding inverse post-decoding method and apparatus,58.45573,image lossy algorithm,['Computer Operations and Hardware'],"A pre-coding method and device for improving data compression performance by removing correlation between a first original data set and a second original data set, each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference data set. The method and device produce a double-difference data set, i.e., an adjacent-delta calculation performed on a cross-delta data set or a cross-delta calculation performed on two adjacent-delta data sets, from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted data sets coming from a single source. The resulting double-difference data set is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original data set having been represented by such a double-difference data set."
"NASA Tech Briefs, March 2008",58.091393,image lossy algorithm,['Man/System Technology and Life Support'],"Topics covered include: WRATS Integrated Data Acquisition System; Breadboard Signal Processor for Arraying DSN Antennas; Digital Receiver Phase Meter; Split-Block Waveguide Polarization Twist for 220 to 325 GHz; Nano-Multiplication-Region Avalanche Photodiodes and Arrays; Tailored Asymmetry for Enhanced Coupling to WGM Resonators; Disabling CNT Electronic Devices by Use of Electron Beams; Conical Bearingless Motor/Generators; Integrated Force Method for Indeterminate Structures; Carbon-Nanotube-Based Electrodes for Biomedical Applications; Compact Directional Microwave Antenna for Localized Heating; Using Hyperspectral Imagery to Identify Turfgrass Stresses; Shaping Diffraction-Grating Grooves to Optimize Efficiency; Low-Light-Shift Cesium Fountain without Mechanical Shutters; Magnetic Compensation for Second-Order Doppler Shift in LITS; Nanostructures Exploit Hybrid-Polariton Resonances; Microfluidics, Chromatography, and Atomic-Force Microscopy; Model of Image Artifacts from Dust Particles; Pattern-Recognition System for Approaching a Known Target; Orchestrator Telemetry Processing Pipeline; Scheme for Quantum Computing Immune to Decoherence; Spin-Stabilized Microsatellites with Solar Concentrators; Phase Calibration of Antenna Arrays Aimed at Spacecraft; Ring Bus Architecture for a Solid-State Recorder; and Image Compression Algorithm Altered to Improve Stereo Ranging."
Video transmission on ATM networks,57.866653,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
High Performance Compression of Science Data,57.61773,image lossy algorithm,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
Study and simulation of low rate video coding schemes,57.580177,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
Optimal Compression Methods for Floating-point Format Images,57.306213,image lossy algorithm,['Astronomy'],"We report on the results of a comparison study of different techniques for compressing FITS images that have floating-point (real*4) pixel values. Standard file compression methods like GZIP are generally ineffective in this case (with compression ratios only in the range 1.2 - 1.6), so instead we use a technique of converting the floating-point values into quantized scaled integers which are compressed using the Rice algorithm. The compressed data stream is stored in FITS format using the tiled-image compression convention. This is technically a lossy compression method, since the pixel values are not exactly reproduced, however all the significant photometric and astrometric information content of the image can be preserved while still achieving file compression ratios in the range of 4 to 8. We also show that introducing dithering, or randomization, when assigning the quantized pixel-values can significantly improve the photometric and astrometric precision in the stellar images in the compressed file without adding additional noise. We quantify our results by comparing the stellar magnitudes and positions as measured in the original uncompressed image to those derived from the same image after applying successively greater amounts of compression."
Data compression for full motion video transmission,57.267414,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
Millimeter-wave passive ultra-compact imaging technology for synthetic vision & mobile platforms,56.797882,image lossy algorithm,['Communications and Radar'],"Substantial technical progress was made on all of the three high-risk subsystems of this program. The subsystems include dielectric antenna, G-band receiver, and electro-optic image processor. Progress is approximately on-schedule for both the receiver and the electro-optic processor development, while greater than anticipated challenges have been discovered in the dielectric antenna development. Much of the information in this report was covered in greater detail in the One-Year Review Meeting held at TTC on 22 February 1996. The performance goals of the dielectric antenna project are: Scan Angle -- 20 deg. desired; Loss -- 6 dB end to end (3 dB average); Frequency -- 206-218 GHz (6% bandwidth); Beam width -- 0.25 deg.; and Length -- 12 inches. The scan angle requirement was chosen to satisfy the needs of aircraft pilots. This requirement, coupled with the presently limited bandwidth processors (1 GHz state-of-the-art and 12 GHz in development in this program) forces the antenna to be dielectric (high scan angle air-filled waveguide-based antennas would be too lossy and their performance would vary too much as a function of frequency). A high dielectric constant (e.g., 10) was initially chosen for the dielectric material. This choice lead to the following fabrication challenges: total thickness variation (TTV) tolerance is 1 micrometer; coupler spacing tolerance is 1 micrometer; width tolerance is larger, but unknown, and the surfaces must have mirror finish. Also of importance is the difficulty in obtaining raw materials that satisfy the overall length requirement of 12 inches while simultaneously satisfying the above specifications."
Lossless compression of NOAA-AVHRR satellite data,56.22067,image lossy algorithm,['COMPUTER SYSTEMS'],"A high-performance lossless compression system for satellite NOAA data is developed. The data is called 'high resolution picture transmission' (HRPT) data, and consists of around 93 percent advanced very high resolution radiometer (AVHRR) multi-channel image data and 7 percent of miscellaneous data. In compressing the image portion, we classify each pixel into 10 different groups and apply a multi-channel prediction and a non-linear error conversion. The entropy coder is an arithmetic coder which is adaptive and regenerates the approximation of the statistical properties of the source as an initial probability table. To compress the non-image part, we used the general compressor (gzip). From experimental results, the original information is compressed down to 25 percent to approx. 40 percent."
In search of random noise,54.921394,image lossy algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"In order to make the best high resolution images of IRAS data it is necessary to incorporate any knowledge about the instrument into a model: the IRAS model. This is necessary since every remaining systematic effect will be amplified by any high resolution technique into spurious artifacts in the images. The search for random noise is in fact the never-ending quest for better quality results, and can only be obtained by better models. The Dutch high-resolution effort has resulted in HIRAS which drives the MEMSYS5 algorithm. It is specifically designed for IRAS image construction. A detailed description of HIRAS with many results is in preparation. In this paper we emphasize many of the instrumental effects incorporated in the IRAS model, including our improved 100 micron IRAS response functions."
Unsteady Analysis of Inlet-Compressor Acoustic Interactions Using Coupled 3-D and 1-D CFD Codes,54.06503,image lossy algorithm,['Aircraft Propulsion and Power'],"It is well known that the dynamic response of a mixed compression supersonic inlet is very sensitive to the boundary condition imposed at the subsonic exit (engine face) of the inlet. In previous work, a 3-D computational fluid dynamics (CFD) inlet code (NPARC) was coupled at the engine face to a 3-D turbomachinery code (ADPAC) simulating an isolated rotor and the coupled simulation used to study the unsteady response of the inlet. The main problem with this approach is that the high fidelity turbomachinery simulation becomes prohibitively expensive as more stages are included in the simulation. In this paper, an alternative approach is explored, wherein the inlet code is coupled to a lesser fidelity 1-D transient compressor code (DYNTECC) which simulates the whole compressor. The specific application chosen for this evaluation is the collapsing bump experiment performed at the University of Cincinnati, wherein reflections of a large-amplitude acoustic pulse from a compressor were measured. The metrics for comparison are the pulse strength (time integral of the pulse amplitude) and wave form (shape). When the compressor is modeled by stage characteristics the computed strength is about ten percent greater than that for the experiment, but the wave shapes are in poor agreement. An alternate approach that uses a fixed rise in duct total pressure and temperature (so-called 'lossy' duct) to simulate a compressor gives good pulse shapes but the strength is about 30 percent low."
The New CCSDS Standard for Low-Complexity Lossless and Near-Lossless Multispectral and Hyperspectral Image Compression,53.905487,image lossy algorithm,['Computer Programming and Software'],"This paper describes the emerging Issue 2 of the CCSDS-123.0-B standard for low-complexity compression of multispectral and hyperspectral imagery, focusing on its new features and capabilities. Most significantly, this new issue incorporates a closed-loop quantization scheme to provide near-lossless compression capability while still supporting lossless compression, and introduces a new entropy coding option that provides better compression of low-entropy data."
Subjective evaluations of integer cosine transform compressed Galileo solid state imagery,53.76452,image lossy algorithm,['ASTRONOMY'],"This paper describes a study conducted for the Jet Propulsion Laboratory, Pasadena, California, using 15 evaluators from 12 institutions involved in the Galileo Solid State Imaging (SSI) experiment. The objective of the study was to determine the impact of integer cosine transform (ICT) compression using specially formulated quantization (q) tables and compression ratios on acceptability of the 800 x 800 x 8 monochromatic astronomical images as evaluated visually by Galileo SSI mission scientists. Fourteen different images in seven image groups were evaluated. Each evaluator viewed two versions of the same image side by side on a high-resolution monitor; each was compressed using a different q level. First the evaluators selected the image with the highest overall quality to support them in their visual evaluations of image content. Next they rated each image using a scale from one to five indicating its judged degree of usefulness. Up to four preselected types of images with and without noise were presented to each evaluator."
"User's manual for CBS3DS, version 1.0",53.328316,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"CBS3DS is a computer code written in FORTRAN 77 to compute the backscattering radar cross section of cavity backed apertures in infinite ground plane and slots in thick infinite ground plane. CBS3DS implements the hybrid Finite Element Method (FEM) and Method of Moments (MoM) techniques. This code uses the tetrahedral elements, with vector edge basis functions for FEM in the volume of the cavity/slot and the triangular elements with the basis functions for MoM at the apertures. By virtue of FEM, this code can handle any arbitrarily shaped three-dimensional cavities filled with inhomogeneous lossy materials; due to MoM, the apertures can be of any arbitrary shape. The User's Manual is written to make the user acquainted with the operation of the code. The user is assumed to be familiar with the FORTRAN 77 language and the operating environment of the computer the code is intended to run."
Remote driving with reduced bandwidth communication,52.942406,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"Oak Ridge National Laboratory has developed a real-time video transmission system for low bandwidth remote operations. The system supports both continuous transmission of video for remote driving and progressive transmission of still images. Inherent in the system design is a spatiotemporal limitation to the effects of channel errors. The average data rate of the system is 64,000 bits/s, a compression of approximately 1000:1 for the black and white National Television Standard Code video. The image quality of the transmissions is maintained at a level that supports teleoperation of a high mobility multipurpose wheeled vehicle at speeds up to 15 mph on a moguled dirt track. Video compression is achieved by using Laplacian image pyramids and a combination of classical techniques. Certain subbands of the image pyramid are transmitted by using interframe differencing with a periodic refresh to aid in bandwidth reduction. Images are also foveated to concentrate image detail in a steerable region. The system supports dynamic video quality adjustments between frame rate, image detail, and foveation rate. A typical configuration for the system used during driving has a frame rate of 4 Hz, a compression per frame of 125:1, and a resulting latency of less than 1s."
Visual Information Processing for Television and Telerobotics,52.923103,image lossy algorithm,['INSTRUMENTATION AND PHOTOGRAPHY'],"This publication is a compilation of the papers presented at the NASA conference on Visual Information Processing for Television and Telerobotics. The conference was held at the Williamsburg Hilton, Williamsburg, Virginia on May  10 to 12, 1989. The conference was sponsored jointly by NASA Offices of Aeronautics and Space Technology (OAST) and Space Science and Applications (OSSA) and the NASA Langley Research Center. The presentations were grouped into three sessions: Image Gathering, Coding, and Advanced Concepts; Systems; and Technologies. The program was organized to provide a forum in which researchers from industry, universities, and government could be brought together to discuss the state of knowledge in image gathering, coding,  and processing methods."
Space and Earth Science Data Compression Workshop,52.412075,image lossy algorithm,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The focus was on scientists' data requirements, as well as constraints imposed by the data collection, transmission, distribution, and archival systems. The workshop consisted of several invited papers; two described information systems for space and Earth science data, four depicted analysis scenarios for extracting information of scientific interest from data collected by Earth orbiting and deep space platforms, and a final one was a general tutorial on image data compression."
A visual detection model for DCT coefficient quantization,51.574028,image lossy algorithm,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
Low-Complexity Lossless Compression of Hyperspectral Imagery via Adaptive Filtering,51.507694,image lossy algorithm,['Optics'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral data is presented. The technique relies on the sign algorithm from the repertoire of adaptive filtering. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
Non-orthogonal subband/transform coder,51.477608,image lossy algorithm,['COMMUNICATIONS AND RADAR'],The present invention is directed to a simplified digital subband coder/decoder. In the present invention a signal is fed into a coder. The coder uses a non-orthogonal algorithm that is simply implemented in the coder hardware. The simple non-orthogonal design is then used in the implementation of the decoder to decode the signal.
Digital storage and analysis of color Doppler echocardiograms,50.76728,image lossy algorithm,['Life Sciences (General)'],"Color Doppler flow mapping has played an important role in clinical echocardiography. Most of the clinical work, however, has been primarily qualitative. Although qualitative information is very valuable, there is considerable quantitative information stored within the velocity map that has not been extensively exploited so far. Recently, many researchers have shown interest in using the encoded velocities to address the clinical problems such as quantification of valvular regurgitation, calculation of cardiac output, and characterization of ventricular filling. In this article, we review some basic physics and engineering aspects of color Doppler echocardiography, as well as drawbacks of trying to retrieve velocities from video tape data. Digital storage, which plays a critical role in performing quantitative analysis, is discussed in some detail with special attention to velocity encoding in DICOM 3.0 (medical image storage standard) and the use of digital compression. Lossy compression can considerably reduce file size with minimal loss of information (mostly redundant); this is critical for digital storage because of the enormous amount of data generated (a 10 minute study could require 18 Gigabytes of storage capacity). Lossy JPEG compression and its impact on quantitative analysis has been studied, showing that images compressed at 27:1 using the JPEG algorithm compares favorably with directly digitized video images, the current goldstandard. Some potential applications of these velocities in analyzing the proximal convergence zones, mitral inflow, and some areas of future development are also discussed in the article."
Progressive transmission and compression images,50.628696,image lossy algorithm,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Communications and information research: Improved space link performance via concatenated forward error correction coding,50.62863,image lossy algorithm,['Earth Resources and Remote Sensing'],"With the development of new advanced instruments for remote sensing applications, sensor data will be generated at a rate that not only requires increased onboard processing and storage capability, but imposes demands on the space to ground communication link and ground data management-communication system. Data compression and error control codes provide viable means to alleviate these demands. Two types of data compression have been studied by many researchers in the area of information theory: a lossless technique that guarantees full reconstruction of the data, and a lossy technique which generally gives higher data compaction ratio but incurs some distortion in the reconstructed data. To satisfy the many science disciplines which NASA supports, lossless data compression becomes a primary focus for the technology development. While transmitting the data obtained by any lossless data compression, it is very important to use some error-control code. For a long time, convolutional codes have been widely used in satellite telecommunications. To more efficiently transform the data obtained by the Rice algorithm, it is required to meet the a posteriori probability (APP) for each decoded bit. A relevant algorithm for this purpose has been proposed which minimizes the bit error probability in the decoding linear block and convolutional codes and meets the APP for each decoded bit. However, recent results on iterative decoding of 'Turbo codes', turn conventional wisdom on its head and suggest fundamentally new techniques. During the past several months of this research, the following approaches have been developed: (1) a new lossless data compression algorithm, which is much better than the extended Rice algorithm for various types of sensor data, (2) a new approach to determine the generalized Hamming weights of the algebraic-geometric codes defined by a large class of curves in high-dimensional spaces, (3) some efficient improved geometric Goppa codes for disk memory systems and high-speed mass memory systems, and (4) a tree based approach for data compression using dynamic programming."
Visually Lossless Data Compression for Real-Time Frame/Pushbroom Space Science Imagers,50.49717,image lossy algorithm,['Computer Programming and Software'],"A visually lossless data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also applicable to frame based imaging and is error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on a block transform of a hybrid of modulated lapped transform (MLT) and discrete cosine transform (DCT), or a 2-dimensional lapped transform, followed by bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate as desired by the user. The approach requires no unique table to maximize its performance. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Flight qualified hardware implementations are in development; a functional chip set is expected by the end of 2001. The chip set is being designed to compress data in excess of 20 Msamples/sec and support quantizations from 2 to 16 bits."
Mixture block coding with progressive transmission in packet video. Appendix 1: Item 2,50.301384,image lossy algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Video transmission will become an important part of future multimedia communication because of dramatically increasing user demand for video, and rapid evolution of coding algorithm and VLSI technology. Video transmission will be part of the broadband-integrated services digital network (B-ISDN). Asynchronous transfer mode (ATM) is a viable candidate for implementation of B-ISDN due to its inherent flexibility, service independency, and high performance. According to the characteristics of ATM, the information has to be coded into discrete cells which travel independently in the packet switching network. A practical realization of an ATM video codec called Mixture Block Coding with Progressive Transmission (MBCPT) is presented. This variable bit rate coding algorithm shows how a constant quality performance can be obtained according to user demand. Interactions between codec and network are emphasized including packetization, service synchronization, flow control, and error recovery. Finally, some simulation results based on MBCPT coding with error recovery are presented."
Progressive Transmission and Compression of Images,50.301014,image lossy algorithm,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Vector quantization,49.735485,image lossy algorithm,['COMMUNICATIONS AND RADAR'],"During the past ten years Vector Quantization (VQ) has developed from a theoretical possibility promised by Shannon's source coding theorems into a powerful and competitive technique for speech and image coding and compression at medium to low bit rates. In this survey, the basic ideas behind the design of vector quantizers are sketched and some comments made on the state-of-the-art and current research efforts."
The development of lossless data compression technology for remote sensing applications,49.423843,image lossy algorithm,['COMPUTER SYSTEMS'],"Lossless data compression has been studied for many NASA missions to achieve the benefit of increased science return, reduced onboard memory requirement, station contact time and communication bandwidth. This paper first addresses the requirement for onboard applications and provides rational for the selection of the Rice algorithm among other available techniques. A top-level description of the Rice algorithm will be given, along with some new capabilities already implemented in both software and hardware VLSI forms. The paper then addresses systems issues important for onboard implementation including sensor calibration, error propagation and data packetization. The latter part of the paper provides several case study examples drawn from a broad spectrum of science instruments including the thematic mapper, x-ray telescope, gamma-ray spectrometer, and acousto-optical spectrometer."
Neural Network Repair of Lossy Compression Artifacts in the Sept 2015  March 2016 Duration of the MMS/FPI Dataset,103.36382,lossy format estimate,['Instrumentation and Photography'],"During the Sept 2015 March 2016 duration (sometimes referred to as Phase 1A) of the Magnetospheric Multiscale Mission (MMS), the Dual Electron Spectrometers (DES) were configured to generously utilize lossy compression. While this maximized the number of velocity distribution functions downlinked, it cameat the expense of lost information content for a fractionof the frames. Following this period of lossy compression, the DES was re-configured in a way that allowed for 95% of the framesto arrive to the ground without loss. Using this high-quality set offrameson-orbit observations, we compressed and decompressed the frameson the ground to create a side-by-side record of the compression effect.  This record was used to drive an optimization method that (a) derived basis functions capable of approximating the lossless sample space and with non-negative coefficients and (b) fitted a function which maps the lossy framesto basis weights that recreate the framewithout compression artifacts. This methodis introduced and evaluated in this paper.Data users should expect a higher level of confidence in the absolute scale of density/temperature measurements andnotice less sinusoidal bias in the velocity X and Y components(GSE)."
Current distribution on a cylindrical antenna with parallel orientation in a lossy magnetoplasma,100.512665,lossy format estimate,['COMMUNICATIONS'],The current distribution and impedance of a thin cylindrical antenna with parallel orientation to the static magnetic field of a lossy magnetoplasma is calculated with the method of moments. The electric field produced by an infinitesimal current source is first derived. Results are presented for a wide range of plasma parameters. Reasonable answers are obtained for all cases except for the overdense hyperbolic case. A discussion of the numerical stability is included which not only applies to this problem but other applications of the method of moments.
Finite difference time domain electromagnetic scattering from frequency-dependent lossy materials,72.26102,lossy format estimate,['COMMUNICATIONS AND RADAR'],"Four different FDTD computer codes and companion Radar Cross Section (RCS) conversion codes on magnetic media are submitted. A single three dimensional dispersive FDTD code for both dispersive dielectric and magnetic materials was developed, along with a user's manual. The extension of FDTD to more complicated materials was made. The code is efficient and is capable of modeling interesting radar targets using a modest computer workstation platform. RCS results for two different plate geometries are reported. The FDTD method was also extended to computing far zone time domain results in two dimensions. Also the capability to model nonlinear materials was incorporated into FDTD and validated."
Planning/scheduling techniques for VQ-based image compression,64.010345,lossy format estimate,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,58.94477,lossy format estimate,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Optimal Compression of Floating-Point Astronomical Images Without Significant Loss of Information,58.325382,lossy format estimate,['Documentation and Information Science'],"We describe a compression method for floating-point astronomical images that gives compression ratios of 6 - 10 while still preserving the scientifically important information in the image. The pixel values are first preprocessed by quantizing them into scaled integer intensity levels, which removes some of the uncompressible noise in the image. The integers are then losslessly compressed using the fast and efficient Rice algorithm and stored in a portable FITS format file. Quantizing an image more coarsely gives greater image compression, but it also increases the noise and degrades the precision of the photometric and astrometric measurements in the quantized image. Dithering the pixel values during the quantization process greatly improves the precision of measurements in the more coarsely quantized images. We perform a series of experiments on both synthetic and real astronomical CCD images to quantitatively demonstrate that the magnitudes and positions of stars in the quantized images can be measured with the predicted amount of precision. In order to encourage wider use of these image compression methods, we have made available a pair of general-purpose image compression programs, called fpack and funpack, which can be used to compress any FITS format image."
Technology Directions for the 21st Century,56.01423,lossy format estimate,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
The Nimbus 5 user's guide,52.619297,lossy format estimate,['SPACE VEHICLES'],"Background information on the Nimbus 5 spacecraft and experiments is presented as a basis for selecting, obtaining, and utilizing the data in research studies. The basic spacecraft system operation and the objectives of the Nimbus 5 flight are outlined, followed by a detailed discussion of each of the experiments. The format, archiving, and access to the data, and the contents and format of the Nimbus 5 Data Catalogs are described."
"A Novel, Real-Valued Genetic Algorithm for Optimizing Radar Absorbing Materials",51.928875,lossy format estimate,['Electronics and Electrical Engineering'],"A novel, real-valued Genetic Algorithm (GA) was designed and implemented to minimize the reflectivity and/or transmissivity of an arbitrary number of homogeneous, lossy dielectric or magnetic layers of arbitrary thickness positioned at either the center of an infinitely long rectangular waveguide, or adjacent to the perfectly conducting backplate of a semi-infinite, shorted-out rectangular waveguide. Evolutionary processes extract the optimal physioelectric constants falling within specified constraints which minimize reflection and/or transmission over the frequency band of interest. This GA extracted the unphysical dielectric and magnetic constants of three layers of fictitious material placed adjacent to the conducting backplate of a shorted-out waveguide such that the reflectivity of the configuration was 55 dB or less over the entire X-band. Examples of the optimization of realistic multi-layer absorbers are also presented. Although typical Genetic Algorithms require populations of many thousands in order to function properly and obtain correct results, verified correct results were obtained for all test cases using this GA with a population of only four."
User's Manual for FEMOM3DR,51.74652,lossy format estimate,['Communications and Radar'],"FEMoM3DR is a computer code written in FORTRAN 77 to compute radiation characteristics of antennas on 3D body using combined Finite Element Method (FEM)/Method of Moments (MoM) technique. The code is written to handle different feeding structures like coaxial line, rectangular waveguide, and circular waveguide. This code uses the tetrahedral elements, with vector edge basis functions for FEM and triangular elements with roof-top basis functions for MoM. By virtue of FEM, this code can handle any arbitrary shaped three dimensional bodies with inhomogeneous lossy materials; and due to MoM the computational domain can be terminated in any arbitrary shape. The User's Manual is written to make the user acquainted with the operation of the code. The user is assumed to be familiar with the FORTRAN 77 language and the operating environment of the computers on which the code is intended to run."
System considerations for efficient communication and storage of MSTI image data,50.67547,lossy format estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
Transform coding for space applications,50.639664,lossy format estimate,['COMMUNICATIONS AND RADAR'],"Data compression coding requirements for aerospace applications differ somewhat from the compression requirements for entertainment systems. On the one hand, entertainment applications are bit rate driven with the goal of getting the best quality possible with a given bandwidth. Science applications are quality driven with the goal of getting the lowest bit rate for a given level of reconstruction quality. In the past, the required quality level has been nothing less than perfect allowing only the use of lossless compression methods (if that). With the advent of better, faster, cheaper missions, an opportunity has arisen for lossy data compression methods to find a use in science applications as requirements for perfect quality reconstruction runs into cost constraints. This paper presents a review of the data compression problem from the space application perspective. Transform coding techniques are described and some simple, integer transforms are presented. The application of these transforms to space-based data compression problems is discussed. Integer transforms have an advantage over conventional transforms in computational complexity. Space applications are different from broadcast or entertainment in that it is desirable to have a simple encoder (in space) and tolerate a more complicated decoder (on the ground) rather than vice versa. Energy compaction with new transforms are compared with the Walsh-Hadamard (WHT), Discrete Cosine (DCT), and Integer Cosine (ICT) transforms."
Locally adaptive vector quantization: Data compression with feature preservation,49.051186,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
The New CCSDS Image Compression Recommendation,48.590504,lossy format estimate,['Instrumentation and Photography'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-Earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An Application-Specific Integrated Circuit (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images."
Rectenna Technology Program: Ultra light 2.45 GHz rectenna 20 GHz rectenna,48.295948,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The program had two general objectives. The first objective was to develop the two plane rectenna format for space application at 2.45 GHz. The resultant foreplane was a thin-film, etched-circuit format fabricated from a laminate composed of 2 mil Kapton F sandwiched between sheets of 1 oz copper. The thin-film foreplane contains half wave dipoles, filter circuits, rectifying Schottky diode, and dc bussing lead. It weighs 160 grams per square meter. Efficiency and dc power output density were measured at 85% and 1 kw/sq m, respectively. Special testing techniques to measure temperature of circuit and diode without perturbing microwave operation using the fluoroptic thermometer were developed. A second objective was to investigate rectenna technology for use at 20 GHz and higher frequencies. Several fabrication formats including the thin-film scaled from 2.45 GHz, ceramic substrate and silk-screening, and monolithic were investigated, with the conclusion that the monolithic approach was the best. A preliminary design of the monolithic rectenna structure and the integrated Schottky diode were made."
Program on application of communications satellites to educational development:  Design of a 12 channel FM microwave receiver,47.74623,lossy format estimate,['ELECTRONIC EQUIPMENT'],"The design, fabrication, and performance of elements of a low cost FM microwave satellite ground station receiver is described. It is capable of accepting 12 contiguous color television equivalent bandwidth channels in the 11.72 to 12.2 GHz band. Each channel is 40 MHz wide and incorporates a 4 MHz guard band. The modulation format is wideband FM and the channels are frequency division multiplexed. Twelve independent CATV compatible baseband outputs are provided. The overall system specifications are first discussed, then consideration is given to the receiver subsystems and the signal branching network."
Design of a 12 channel fm microwave receiver,47.49418,lossy format estimate,['COMMUNICATIONS'],"The design, fabrication, and performance of elements of a low cost FM microwave satellite ground station receiver is described. It is capable of accepting 12 contiguous color television equivalent bandwidth channels in the 11.72 to 12.2 GHz band. Each channel is 40 MHz wide and incorporates a 4 MHz guard band. The modulation format is wideband FM and the channels are frequency division multiplexed. Twelve independent CATV compatible baseband outputs are provided. The overall system specifications are first discussed, then consideration is given to the receiver subsystems and the signal branching network."
"NASA Tech Briefs, June 2012",40.24772,lossy format estimate,['Man/System Technology and Life Support'],"Topics covered include: iGlobe Interactive Visualization and Analysis of Spatial Data; Broad-Bandwidth FPGA-Based Digital Polyphase Spectrometer; Small Aircraft Data Distribution System; Earth Science Datacasting v2.0; Algorithm for Compressing Time-Series Data; Onboard Science and Applications Algorithm for Hyperspectral Data Reduction; Sampling Technique for Robust Odorant Detection Based on MIT RealNose Data; Security Data Warehouse Application; Integrated Laser Characterization, Data Acquisition, and Command and Control Test System; Radiation-Hard SpaceWire/Gigabit Ethernet-Compatible Transponder; Hardware Implementation of Lossless Adaptive Compression of Data From a Hyperspectral Imager; High-Voltage, Low-Power BNC Feedthrough Terminator; SpaceCube Mini; Dichroic Filter for Separating W-Band and Ka-Band; Active Mirror Predictive and Requirement Verification Software (AMP-ReVS); Navigation/Prop Software Suite; Personal Computer Transport Analysis Program; Pressure Ratio to Thermal Environments; Probabilistic Fatigue Damage Program (FATIG); ASCENT Program; JPL Genesis and Rapid Intensification Processes (GRIP) Portal; Data::Downloader; Fault Tolerance Middleware for a Multi-Core System; DspaceOgreTerrain 3D Terrain Visualization Tool; Trick Simulation Environment 07; Geometric Reasoning for Automated Planning; Water Detection Based on Color Variation; Single-Layer, All-Metal Patch Antenna Element with Wide Bandwidth; Scanning Laser Infrared Molecular Spectrometer (SLIMS); Next-Generation Microshutter Arrays for Large-Format Imaging and Spectroscopy; Detection of Carbon Monoxide Using Polymer-Composite Films with a Porphyrin-Functionalized Polypyrrole; Enhanced-Adhesion Multiwalled Carbon Nanotubes on Titanium Substrates for Stray Light Control; Three-Dimensional Porous Particles Composed of Curved, Two-Dimensional, Nano-Sized Layers for Li-Ion Batteries 23 Ultra-Lightweight; and Ultra-Lightweight Nanocomposite Foams and Sandwich Structures for Space Structure Applications."
Ku-band field-effect power transistors,39.8788,lossy format estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"A single stage amplifier was developed using an 8 gate, 1200 micrometer width device to give a gain of 3.3 + or - 0.1 dB over the 14.4 to 15.4 GHz band with an output power of 0.48 W and 15% minimum efficiency with 0.255 W of input power. With two 8 gate devices combined and matched on the device carrier, using a lumped element format, a gain of 3 dB was attained over the 14.5 to 15.5 GHz band with a maximum efficiency of 9.9% for an output power of 0.8 W."
Processing CCD Images to Detect Transits of Earth-Sized Planets: Maximizing Sensitivity While Achieving Reasonable Downlink Requirements,39.69026,lossy format estimate,"['Instrumentation and Photography', 'Astronomy']","We have performed end-to-end laboratory and numerical simulations to demonstrate the capability of differential photometry under realistic operating conditions to detect transits of Earth-sized planets orbiting solar-like stars. Data acquisition and processing were conducted using the same methods planned for the proposed Kepler Mission. These included performing aperture photometry on large-format CCD images of an artificial star fields obtained without a shutter at a readout rate of 1 megapixel/sec, detecting and removing cosmic rays from individual exposures and making the necessary corrections for nonlinearity and shutterless operation in the absence of darks. We will discuss the image processing tasks performed `on-board' the simulated spacecraft, which yielded raw photometry and ancillary data used to monitor and correct for systematic effects, and the data processing and analysis tasks conducted to obtain lightcurves from the raw data and characterize the detectability of transits. The laboratory results are discussed along with the results of a numerical simulation carried out in parallel with the laboratory simulation. These two simulations demonstrate that a system-level differential photometric precision of 10-5 on five- hour intervals can be achieved under realistic conditions."
Reefing Line Tension in CPAS Main Parachute Clusters,38.762787,lossy format estimate,"['Spacecraft Design, Testing and Performance']","Reefing lines are an essential feature to manage inflation loads. During each Engineering Development Unit (EDU) test of the Capsule Parachute Assembly System (CPAS), a chase aircraft is staged to be level with the cluster of Main ringsail parachutes during the initial inflation and reefed stages. This allows for capturing high-quality still photographs of the reefed skirt, suspension line, and canopy geometry. The over-inflation angles are synchronized with measured loads data in order to compute the tension force in the reefing line. The traditional reefing tension equation assumes radial symmetry, but cluster effects cause the reefed skirt of each parachute to elongate to a more elliptical shape. This effect was considered in evaluating multiple parachutes to estimate the semi-major and semi-minor axes. Three flight tests are assessed, including one with a skipped first stage, which had peak reefing line tension over three times higher than the nominal parachute disreef sequence."
"Small business innovation research program solicitation: Closing date July 16, 1990",38.307274,lossy format estimate,['ADMINISTRATION AND MANAGEMENT'],"This is the eighth annual solicitation by NASA addressed to small business firms, inviting them to submit proposals for research, or research and development, activities in some of the science and engineering areas of interest to NASA. The solicitation describes the Small Business Innovative Research (SBIR) program, identifies eligibility requirements, outlines the required proposal format and content, states proposal preparation and submission requirements, describes the proposal evaluation and award selection process, and provides other information to assist those interested in participating in NASA's SBIR program. It also identifies the technical topics and subtopics for which SBIR proposals are solicited. These cover a broad range of current NASA interests, but do not necessarily include all areas in which NASA plans or currently conducts research. High-risk high pay-off innovations are desired."
"A users manual for the method of moments Aircraft Modeling Code (AMC), version 2",37.85445,lossy format estimate,['COMMUNICATIONS AND RADAR'],"This report serves as a user's manual for Version 2 of the 'Aircraft Modeling Code' or AMC. AMC is a user-oriented computer code, based on the method of moments (MM), for the analysis of the radiation and/or scattering from geometries consisting of a main body or fuselage shape with attached wings and fins. The shape of the main body is described by defining its cross section at several stations along its length. Wings, fins, rotor blades, and radiating monopoles can then be attached to the main body. Although AMC was specifically designed for aircraft or helicopter shapes, it can also be applied to missiles, ships, submarines, jet inlets, automobiles, spacecraft, etc. The problem geometry and run control parameters are specified via a two character command language input format. This report describes the input command language and also includes several examples which illustrate typical code inputs and outputs."
Superboom Caustic Analysis and Measurement Program (SCAMP) Final Report,37.803032,lossy format estimate,"['Acoustics', 'Fluid Mechanics and Thermodynamics', 'Aircraft Design, Testing and Performance']","The objectives of the Superboom Caustic Analysis and Measurement (SCAMP) Program were to develop and validate, via flight-test measurements, analytical models for sonic boom signatures in and around focal zones as they are expected to occur during commercial aircraft transition from subsonic to supersonic flight, and to apply these models to focus boom prediction of low-boom aircraft designs. The SCAMP program has successfully investigated sonic boom focusing both analytically and experimentally, while gathering a comprehensive empirical flight test and acoustic dataset, and developing a suite of focused sonic boom prediction tools. An experimental flight and acoustic measurement test was designed during the initial year of the SCAMP program, with execution of the SCAMP flight test occurring in May 2011. The current SCAMP team, led by Wyle, includes partners from the Boeing Company, Pennsylvania State University, Gulfstream Aerospace, Eagle Aeronautics, and Central Washington University. Numerous collaborators have also participated by supporting the experiment with human and equipment resources at their own expense. The experiment involved precision flight of a McDonnell Douglas (now Boeing) F-18B executing different maneuvers that created focused sonic booms. The maneuvers were designed to center on the flight regime expected for commercial supersonic aircraft transonic transition, and also span a range of caustic curvatures in order to provide a variety of conditions for code validations. The SCAMP experiment was designed to capture concurrent F-18B on-board flight instrumentation data, high-fidelity ground-based and airborne acoustic data, and surface and upper air meteorological data. Close coordination with NASA Dryden resulted in the development of new experimental instrumentation and techniques to facilitate the SCAMP flight-test execution, including the development of an F-18B Mach rate cockpit display, TG-14 powered glider in-flight sonic boom measurement instrumentation and ""Where's the Focus?"" (WTF) software for near-real time way-point computation accounting for local atmospherics. In May 2011, 13 F-18B flights were conducted during 5 flying days over a 2 week period. A densely populated 10,000 ft-long ground acoustic array with 125-ft microphone spacing was designed to capture pre-, focus, and post-focus regions. The ground-based acoustic array was placed in a nominally east-west orientation in the remote Cuddeback lakebed region, north of Edwards AFB. This area was carefully selected to avoid placing focused booms on populated areas or solar power facilities. For the SCAMP measurement campaign, approvals were obtained to temporarily extend the Black Mountain supersonic corridor northward by three miles. The SCAMP flight tests successfully captured 70 boom events, with 61 focus passes, and 9 calibration passes. Seventeen of the focus passes and three of the calibration passes were laterally offset; with the others being centerline flights. Airborne incoming sonic boom wave measurements were measured by the TG-14 for 10 of the F-18B flight passes including one maximum focus signature, several N-u combinations, several overlapped N-u signatures, and several evanescent waves. During the 27-month program, the SCAMP team developed a suite of integrated computer codes with sonic boom focusing predictive capabilities: PCBoom, Lossy Nonlinear Tricomi Equation Method (LNTE) and the Nonlinear Progressive wave Equation (NPE) method. PCBoom propagates the rays through the atmosphere and, in addition to legacy focus signature prediction based on the Gill-Seebass method, provides input source characteristics and propagation parameters to LNTE and NPE. LNTE, a Tricomi solver that incorporates atmospheric losses, computes the focus signature at the focus, and computes the focus signature in the vicinity of the focal zone, including the evanescent and post-focus zones. LNTE signature auralization from low-boom vehicle designs has been demonstrated in the NASA Langley Interior Effects Room (IER). The NPE has also been validated for use in prediction of focused ground boom signatures in sonic boom focal zones. The NPE formulation has the capability to incorporate atmospheric turbulence in the predictions. This has been applied to sonic boom propagation in the past. Prediction of turbulence effects on focal zone signatures was not, however, explored during the SCAMP program."
User's manual for three dimensional FDTD version C code for scattering from frequency-independent dielectric and magnetic materials,37.45478,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Version C is a three dimensional numerical electromagnetic scattering code based upon the Finite Difference Time Domain Technique (FDTD). The supplied version of the code is one version of our current three dimensional FDTD code set. This manual provides a description of the code and corresponding results for several scattering problems. The manual is organized into fourteen sections: introduction, description of the FDTD method, operation, resource requirements, Version C code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMONC.FOR), a section briefly discussing Radar Cross Section (RCS) computations, a section discussing some scattering results, a sample problem setup section, a new problem checklist, references and figure titles."
User's manual for two dimensional FDTD version TEA and TMA codes for scattering from frequency-independent dielectic materials,37.191612,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Versions TEA and TMA are two dimensional numerical electromagnetic scattering codes based upon the Finite Difference Time Domain Technique (FDTD) first proposed by Yee in 1966. The supplied version of the codes are two versions of our current two dimensional FDTD code set. This manual provides a description of the codes and corresponding results for the default scattering problem. The manual is organized into eleven sections: introduction, Version TEA and TMA code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include files (TEACOM.FOR TMACOM.FOR), a section briefly discussing scattering width computations, a section discussing the scattering results, a sample problem set section, a new problem checklist, references and figure titles."
Small business innovation research: Program solicitation,37.111423,lossy format estimate,['ADMINISTRATION AND MANAGEMENT'],"This, the seventh annual SBIR solicitation by NASA, describes the program, identifies eligibility requirements, outlines the required proposal format and content, states proposal preparation and submission requirements, describes the proposal evaluation and award selection process, and provides other information to assist those interested in participating in NASA's SBIR program. It also identifies the Technical Topics and Subtopics in which SBIR Phase 1 proposals are solicited in 1989. These Topics and Subtopics cover a broad range of current NASA interests, but do not necessarily include all areas in which NASA plans or currently conducts research. High-risk high pay-off innovations are desired."
User's manual for three dimensional FDTD version B code for scattering from frequency-dependent dielectric materials,36.924473,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Version B is a three dimensional numerical electromagnetic scattering code based upon the Finite Difference Time Domain Technique (FDTD). The supplied version of the code is one version of our current three dimensional FDTD code set. This manual provides a description of the code and corresponding results for several scattering problems. The manual is organized into fourteen sections: introduction, description of the FDTD method, operation, resource requirements, Version B code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMONB.FOR), a section briefly discussing Radar Cross Section (RCS) computations, a section discussing some scattering results, a sample problem setup section, a new problem checklist, references and figure titles."
Shuttle-tethered satellite system definition study.  Volume 1:  Executive study,36.442696,lossy format estimate,['ASTRONAUTICS (GENERAL)'],"The Tethered Satellite System has great prospects for extending orbital operations capability of the Space Transportation System to science, applications, and technology projects not otherwise attainable. The system will installed in the Shuttle Orbiter and will have the capability to deploy a captive satellite up to 100 km away from the Orbiter. Control and retrieval of the satellite are accomplished by means of a tether line connecting the satellite and the cargo bay mounted equipment in the Orbiter. At low satellite altitudes, the system will permit investigations of a duration that could not be pursued with sounding rockets of free-flying spacecraft. The propose of the Shuttle/Tethered Satellite System Definition Study was to produce the preliminary design, preliminary specifications, gross program plans, and program cost estimate for a 1982 operational verification flight. This was accomplished during a fifteen month effort under by the NASA George C. Marshall Space Flight Center (MSFC). The MSFC Phase 1 and related studies demonstrated the feasibility of the system and served as a starting point for the Phase 2 definition study."
User's manual for three dimensional FDTD version A code for scattering from frequency-independent dielectric materials,36.399223,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Finite Difference Time Domain Electromagnetic Scattering Code Version A is a three dimensional numerical electromagnetic scattering code based upon the Finite Difference Time Domain Technique (FDTD). This manual provides a description of the code and corresponding results for the default scattering problem. In addition to the description, the operation, resource requirements, version A code capabilities, a description of each subroutine, a brief discussion of the radar cross section computations, and a discussion of the scattering results."
User's manual for three dimensional FDTD version D code for scattering from frequency-dependent dielectric and magnetic materials,35.89187,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Version D is a three dimensional numerical electromagnetic scattering code based upon the Finite Difference Time Domain Technique (FDTD). The supplied version of the code is one version of our current three dimensional FDTD code set. This manual provides a description of the code and corresponding results for several scattering problems. The manual is organized into fourteen sections: introduction, description of the FDTD method, operation, resource requirements, Version D code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMOND.FOR), a section briefly discussing Radar Cross Section (RCS) computations, a section discussing some scattering results, a sample problem setup section, a new problem checklist, references and figure titles."
Validation of a Digital Noise Power Integration Technique for Radiometric Clear Sky Attenuation Estimation at Q-band,34.85251,lossy format estimate,['Communications and Radar'],"This paper presents the validation of a digital signal processing technique that can be used to estimate radiometric sky noise, and hence, atmospheric absorption, within existing digital receivers at little/no additional cost. To demonstrate this, a receiver was constructed that simultaneously records the beacon signal power from the ALPHASAT Aldo Paraboni technology demonstration payload, as well as the integrated noise power in the adjacent band. Calibration from the digital radiometer is performed using tip-curve calibration procedures. Atmospheric fading is then obtained by observing the beacon as well as the radiometric signals. This enables the comparison of fading obtained by the two techniques and provides a means to calibrate the received beacon power level to obtain total atmospheric attenuation. It is shown that for low levels of fading, up to a few dB, the two techniques provide good agreement. This approach can therefore provide a low-cost option for geostationary mmwave satellite channel measurements in the low fading regime, which can be useful in the design and operation of the feeder links in emerging satcom systems."
Natural Environments Definition for Design,34.54106,lossy format estimate,"['Launch Vehicles and Launch Operations', 'Spacecraft Design, Testing and Performance']","Planning for future National Aeronautics and Space Administration (NASA) missions will encompass a variety of operational and engineering activities that involve a multitude of issues, constraints, and influences derived from the natural environment. This Technical Memorandum (TM) presents a definition of the natural environment, i.e., a description in engineering handbook format of models and data specifically selected to support the architecture development, engineering design, and technology development for NASA's Exploration Systems Development (ESD) initiatives."
Theoretical analysis of the EWEC report,34.197006,lossy format estimate,['ENERGY PRODUCTION AND CONVERSION'],"This analytic investigation shows how the electromagnetic wave energy conversion (EWEC) device, as used for solar-to-electric power conversion, is significantly different from solar cells, with respect to principles of operation. An optimistic estimate of efficiency is about 80% for a full-wave rectifying configuration with solar radiation normally incident. This compares favorably with the theoretical maximum for a CdTe solar cell (23.5%), as well as with the efficiencies of more familiar cells:  Si (19.5%), InP (21.5%), and GaAs (23%). Some key technological issues that must be resolved before the EWEC device can be realized are identified. Those issues include:  the fabrication of a pn semi-conductor junction with no permittivity resonances in the optical band; and the efficient channeling of the power received by countless microscopic horn antennas through a relatively few number of wires."
The relationship of sensor parameters to applications data analysis,34.047577,lossy format estimate,['COMMUNICATIONS AND RADAR'],"A stochastic model for the data acquisition system in a multispectral scanner system, like the one utilized by the LANDSAT satellites, is presented. A list of noise sources which are known or presumed to have a significant effect in the information extraction process was constructed. Since the shot noise introduced by the photodetectors in the sensor system is signal level dependent, an atmospheric model was adopted which could adequately describe the amount of radiation that gets into the sensors based on the atmospheric transmittance. An analysis was carried out to find the output spectral statistics in terms of the input signal statistics and the system parameters. This was integrated into a set of FORTRAN programs that when supplied with, the class statistics, the noise levels introduced by the sensor system, the atmospheric transmittance, and the atmospheric path radiance, can be used to estimate the classification performance. In order to show the beneficts of this model a series of runs were performed in which the Thematic Mapper multispectral scanner was the system under consideration."
Space Shuttle program communication and tracking systems interface analysis,33.98708,lossy format estimate,['SPACE TRANSPORTATION'],"The Space Shuttle Program Communications and Tracking Systems Interface Analysis began April 18, 1983. During this time, the shuttle communication and tracking systems began flight testing. Two areas of analysis documented were a result of observations made during flight tests. These analyses involved the Ku-band communication system. First, there was a detailed analysis of the interface between the solar max data format and the Ku-band communication system including the TDRSS ground station. The second analysis involving the Ku-band communication system was an analysis of the frequency lock loop of the Gunn oscillator used to generate the transmit frequency. The stability of the frequency lock loop was investigated and changes to the design were reviewed to alleviate the potential loss of data due the loop losing lock and entering the reacquisition mode. Other areas of investigation were the S-band antenna analysis and RF coverage analysis."
"Study and Simulation of Enhancements for TCP (Transmission Control Protocol) Performance Over Noisy, High-Latency Links",33.788544,lossy format estimate,['Communications and Radar'],"The designers of the TCP/IP protocol suite explicitly included support of satellites in their design goals. The goal of the Internet Project was to design a protocol which could be layered over different networking technologies to allow them to be concatenated into an internet. The results of this project included two protocols, IP and TCP. IP is the protocol used by all elements in the network and it defines the standard packet format for IP datagrams. TCP is the end-to-end transport protocol commonly used between end systems on the Internet to derive a reliable bi-directional byte-pipe service from the underlying unreliable IP datagram service. Satellite links are explicitly mentioned in Vint Cerf's 2-page article which appeared in 1980 in CCR [2] to introduce the specifications for IP and TCP. In the past fifteen years, TCP has been demonstrated to work over many differing networking technologies, including over paths including satellites links. So if satellite links were in the minds of the designers from the beginning, what is the problem? The problem is that the performance of TCP has in some cases been disappointing. A goal of the authors of the original specification of TCP was to specify only enough behavior to ensure interoperability. The specification left a number of important decisions, in particular how much data is to be sent when, to the implementor. This was deliberately' done. By leaving performance-related decisions to the implementor, this would allow the protocol TCP to be tuned and adapted to different networks and situations in the future without the need to revise the specification of the protocol, or break interoperability. Interoperability would continue while future implementations would be allowed flexibility to adapt to needs which could not be anticipated at the time of the original protocol design."
1994 Science Information Management and Data Compression Workshop,32.67458,lossy format estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
"Controlling Flexible Manipulators, an Experimental Investigation",32.619267,lossy format estimate,['MAN/SYSTEM TECHNOLOGY AND LIFE SUPPORT'],"Lightweight, slender manipulators offer faster response and/or greater workspace range for the same size actuators than tradional manipulators. Lightweight construction of manipulator links results in increased structural flexibility. The increase flexibility must be considered in the design of control systems to properly account for the dynamic flexible vibrations and static deflections. Real time control of the flexible manipulator vibrations are experimentally investigated. Models intended for real-time control of distributed parameter system such as flexible manipulators rely on model approximation schemes. An linear model based on the application of Lagrangian dynamics to a rigid body mode and a series of separable flexible modes is examined with respect to model order requirements, and modal candidate selection. Balanced realizations are applied to the linear flexible model to obtain an estimate of appropriate order for a selected model. Describing the flexible deflections as a linear combination of modes results in measurements of beam state, which yield information about several modes. To realize the potential of linear systems theory, knowledge of each state must be available. State estimation is  also accomplished by implementation of a Kalman Filter. State feedback control laws are implemented based upon linear quadratic regulator design."
Concept report: Experimental vector magnetograph (EXVM) operational configuration balloon flight assembly,32.457897,lossy format estimate,['AERODYNAMICS'],"The observational limitations of earth bound solar studies has prompted a great deal of interest in recent months in being able to gain new scientific perspectives through, what should prove to be, relatively low cost flight of the magnetograph system. The ground work done by TBE for the solar balloon missions (originally planned for SOUP and GRID) as well as the rather advanced state of assembly of the EXVM has allowed the quick formulation of a mission concept for the 30 cm system currently being assembled. The flight system operational configuration will be discussed as it is proposed for short duration flight (on the order of one day) over the continental United States. Balloon hardware design requirements used in formulation of the concept are those set by the National Science Balloon Facility (NSBF), the support agency under NASA contract for flight services. The concept assumes that the flight hardware assembly would come together from three development sources: the scientific investigator package, the integration contractor package, and the NSBF support system. The majority of these three separate packages can be independently developed; however, the computer control interfaces and telemetry links would require extensive preplanning and coordination. A special section of this study deals with definition of a dedicated telemetry link to be provided by the integration contractor for video image data for pointing system performance verification. In this study the approach has been to capitalize to the maximum extent possible on existing hardware and system design. This is the most prudent step that can be taken to reduce eventual program cost for long duration flights. By fielding the existing EXVM as quickly as possible, experience could be gained from several short duration flight tests before it became necessary to commit to major upgrades for long duration flights of this system or of the larger 60 cm version being considered for eventual development."
Studies on image compression and image reconstruction,30.588076,lossy format estimate,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
A browse facility for Earth science remote sensing data: Center director's discretionary fund final report,30.290096,lossy format estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"An image data visual browse facility is developed for a UNIX platform using the X Windows 11 system. It allows one to visually examine reduced resolution image data to determine which data are applicable for further research. Links with a relational data base manager then allow one to extract not only the full resolution image data, but any other ancillary data related to the case study. Various techniques are examined for compression of the image data in order to reduce data storage requirements and time necessary to transmit the data on the internet. Data used were from the WetNet project."
Compressing subbanded image data with Lempel-Ziv-based coders,30.093527,lossy format estimate,['COMMUNICATIONS AND RADAR'],"A method of improving the compression of image data using Lempel-Ziv-based coding is presented. Image data is first processed with a simple transform, such as the Walsh Hadamard Transform, to produce subbands. The subbanded data can be rounded to eight bits or it can be quantized for higher compression at the cost of some reduction in the quality of the reconstructed image. The data is then run-length coded to take advantage of the large runs of zeros produced by quantization. Compression results are presented and contrasted with a subband compression method using quantization followed by run-length coding and Huffman coding. The Lempel-Ziv-based coding in conjunction with run-length coding produces the best compression results at the same reconstruction quality (compared with the Huffman-based coding) on the image data used."
The importance of robust error control in data compression applications,29.898165,lossy format estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
"Multi-rate, real time image compression for images dominated by point sources",29.787138,lossy format estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"An image compression system recently developed for compression of digital images dominated by point sources is presented. Encoding consists of minimum-mean removal, vector quantization, adaptive threshold truncation, and modified Huffman encoding. Simulations are presented showing that the peaks corresponding to point sources can be transmitted losslessly for low signal-to-noise ratios (SNR) and high point source densities while maintaining a reduced output bit rate. Encoding and decoding hardware has been built and tested which processes 552,960 12-bit pixels per second at compression rates of 10:1 and 4:1. Simulation results are presented for the 10:1 case only."
High Data Rate Instrument Study,29.665432,lossy format estimate,['Instrumentation and Photography'],"The High Data Rate Instrument Study was a joint effort between the Jet Propulsion Laboratory (JPL) and the Goddard Space Flight Center (GSFC). The objectives were to assess the characteristics of future high data rate Earth observing science instruments and then to assess the feasibility of developing data processing systems and communications systems required to meet those data rates. Instruments and technology were assessed for technology readiness dates of 2000, 2003, and 2006. The highest data rate instruments are hyperspectral and synthetic aperture radar instruments which are capable of generating 3.2 Gigabits per second (Gbps) and 1.3 Gbps, respectively, with a technology readiness date of 2003. These instruments would require storage of 16.2 Terebits (Tb) of information (RF communications case of two orbits of data) or 40.5 Tb of information (optical communications case of five orbits of data) with a technology readiness date of 2003. Onboard storage capability in 2003 is estimated at 4 Tb; therefore, all the data created cannot be stored without processing or compression. Of the 4 Tb of stored data, RF communications can only send about one third of the data to the ground, while optical communications is estimated at 6.4 Tb across all three technology readiness dates of 2000, 2003, and 2006 which were used in the study. The study includes analysis of the onboard processing and communications technologies at these three dates and potential systems to meet the high data rate requirements. In the 2003 case, 7.8% of the data can be stored and downlinked by RF communications while 10% of the data can be stored and downlinked with optical communications. The study conclusion is that only 1 to 10% of the data generated by high data rate instruments will be sent to the ground from now through 2006 unless revolutionary changes in spacecraft design and operations such as intelligent data extraction are developed."
ViDI: Virtual Diagnostics Interface,29.4915,lossy format estimate,['Instrumentation and Photography'],"The desire to revolutionize the aircraft design cycle from its currently lethargic pace to a fast turn-around operation enabling the optimization of non-traditional configurations is a critical challenge facing the aeronautics industry. In response, a large scale effort is underway to not only advance the state of the art in wind tunnel testing, computational modeling, and information technology, but to unify these often disparate elements into a cohesive design resource. This paper will address Seamless Data Transfer, the critical central nervous system that will enable a wide variety of varied components to work together."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,29.408306,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
A Martian entry propagation study,29.371414,lossy format estimate,"['PHYSICS, PLASMA']",Martian atmosphere plasma effects on interplanetary transmission of electromagnetic radiation
User's manual for three dimensional FDTD version C code for scattering from frequency-independent dielectric and magnetic materials,29.22489,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Version C is a three-dimensional numerical electromagnetic scattering code based on the Finite Difference Time Domain (FDTD) technique. The supplied version of the code is one version of our current three-dimensional FDTD code set. The manual given here provides a description of the code and corresponding results for several scattering problems. The manual is organized into 14 sections: introduction, description of the FDTD method, operation, resource requirements, Version C code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMONC.FOR), a section briefly discussing radar cross section computations, a section discussing some scattering results, a new problem checklist, references, and figure titles."
PCBoom Version 7.1 User's Guide,29.220213,lossy format estimate,['Acoustics'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Version C is a three-dimensional numerical electromagnetic scattering code based on the Finite Difference Time Domain (FDTD) technique. The supplied version of the code is one version of our current three-dimensional FDTD code set. The manual given here provides a description of the code and corresponding results for several scattering problems. The manual is organized into 14 sections: introduction, description of the FDTD method, operation, resource requirements, Version C code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMONC.FOR), a section briefly discussing radar cross section computations, a section discussing some scattering results, a new problem checklist, references, and figure titles."
An assessment of technology alternatives for telecommunications and information management for the space exploration initiative,29.089773,lossy format estimate,['COMMUNICATIONS AND RADAR'],"On the 20th anniversary of the Apollo 11 lunar landing, President Bush set forth ambitious goals for expanding human presence in the solar system. The Space Exploration Initiative (SEI) addresses these goals beginning with Space Station Freedom, followed by a permanent return to the Moon, and a manned mission to Mars. A well designed, adaptive Telecommunications, Navigation, and Information Management (TNIM) infrastructure is vital to the success of these missions. Utilizing initial projections of user requirements, a team under the direction of NASA's Office of Space Operations developed overall architectures and point designs to implement the TNIM functions for the Lunar and Mars mission scenarios. Based on these designs, an assessment of technology alternatives for the telecommunications and information management functions was performed. This technology assessment identifies technology developments necessary to meet the telecommunications and information management system requirements for SEI. Technology requirements, technology needs and alternatives, the present level of technology readiness in each area, and a schedule for development are presented."
User's manual for two dimensional FDTD version TEA and TMA codes for scattering from frequency-independent dielectric materials,29.039227,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code Versions TEA and TMA are two dimensional electromagnetic scattering codes based on the Finite Difference Time Domain Technique (FDTD) first proposed by Yee in 1966. The supplied version of the codes are two versions of our current FDTD code set. This manual provides a description of the codes and corresponding results for the default scattering problem. The manual is organized into eleven sections: introduction, Version TEA and TMA code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include files (TEACOM.FOR TMACOM.FOR), a section briefly discussing scattering width computations, a section discussing the scattering results, a sample problem setup section, a new problem checklist, references, and figure titles."
The 1995 Science Information Management and Data Compression Workshop,28.730732,lossy format estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Microwave remote sensing and its application to soil moisture detection,28.307268,lossy format estimate,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Experimental measurements were utilized to demonstrate a procedure for estimating soil moisture, using a passive microwave sensor. The investigation showed that 1.4 GHz and 10.6 GHz can be used to estimate the average soil moisture within two depths; however, it appeared that a frequency less than 10.6 GHz would be preferable for the surface measurement. Average soil moisture within two depths would provide information on the slope of the soil moisture gradient near the surface. Measurements showed that a uniform surface roughness similar to flat tilled fields reduced the sensitivity of the microwave emission to soil moisture changes. Assuming that the surface roughness was known, the approximate soil moisture estimation accuracy at 1.4 GHz calculated for a 25% average soil moisture and an 80% degree of confidence, was +3% and -6% for a smooth bare surface, +4% and -5% for a medium rough surface, and +5.5% and -6% for a rough surface."
User's manual for three dimensional FDTD version A code for scattering from frequency-independent dielectric materials,28.24684,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain (FDTD) Electromagnetic Scattering Code Version A is a three dimensional numerical electromagnetic scattering code based on the Finite Difference Time Domain technique. The supplied version of the code is one version of our current three dimensional FDTD code set. The manual provides a description of the code and the corresponding results for the default scattering problem. The manual is organized into 14 sections: introduction, description of the FDTD method, operation, resource requirements, Version A code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file (COMMONA.FOR), a section briefly discussing radar cross section (RCS) computations, a section discussing the scattering results, a sample problem setup section, a new problem checklist, references, and figure titles."
The Space and Earth Science Data Compression Workshop,28.159443,lossy format estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
User's manual for three dimensional FDTD version B code for scattering from frequency-dependent dielectric materials,27.983242,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Code Version B is a three dimensional numerical electromagnetic scattering code based upon the Finite Difference Time Domain Technique (FDTD). The supplied version of the code is one version of our current three dimensional FDTD code set. This manual provides a description of the code and corresponding results for several scattering problems. The manual is organized into 14 sections: introduction, description of the FDTD method, operation, resource requirements, Version B code capabilities, a brief description of the default scattering geometry, a brief description of each subroutine, a description of the include file, a discussion of radar cross section computations, a discussion of some scattering results, a sample problem setup section, a new problem checklist, references and figure titles."
The 1993 Space and Earth Science Data Compression Workshop,27.857765,lossy format estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
Progressive transmission of pseudo-color images. Appendix 1: Item 4,27.813326,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"The transmission of digital images can require considerable channel bandwidth. The cost of obtaining such a channel can be prohibitive, or the channel might simply not be available. In this case, progressive transmission (PT) can be useful. PT presents the user with a coarse initial image approximation, and then proceeds to refine it. In this way, the user tends to receive information about the content of the image sooner than if a sequential transmission method is used. PT finds application in image data base browsing, teleconferencing, medical and other applications. A PT scheme is developed for use with a particular type of image data, the pseudo-color or color mapped image. Such images consist of a table of colors called a colormap, plus a 2-D array of index values which indicate which colormap entry is to be used to display a given pixel. This type of image presents some unique problems for a PT coder, and techniques for overcoming these problems are developed. A computer simulation of the color mapped PT scheme is developed to evaluate its performance. Results of simulation using several test images are presented."
User's manual for three dimensional FDTD version D code for scattering from frequency-dependent dielectric and magnetic materials,27.735325,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Penn State Finite Difference Time Domain Electromagnetic Scattering Code version D is a 3-D numerical electromagnetic scattering code based upon the finite difference time domain technique (FDTD). The manual provides a description of the code and corresponding results for several scattering problems. The manual is organized into 14 sections: introduction; description of the FDTD method; operation; resource requirements; version D code capabilities; a brief description of the default scattering geometry; a brief description of each subroutine; a description of the include file; a section briefly discussing Radar Cross Section computations; a section discussing some scattering results; a sample problem setup section; a new problem checklist; references and figure titles. The FDTD technique models transient electromagnetic scattering and interactions with objects of arbitrary shape and/or material composition. In the FDTD method, Maxwell's curl equations are discretized in time-space and all derivatives (temporal and spatial) are approximated by central differences."
Proceedings of the Scientific Data Compression Workshop,27.626324,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Continuing advances in space and Earth science requires increasing amounts of data to be gathered  from spaceborne sensors. NASA expects to launch sensors during the next two decades which will be capable of producing an aggregate of 1500 Megabits  per second if operated simultaneously. Such high data rates cause stresses in all aspects of end-to-end data systems. Technologies and techniques are needed to relieve such stresses. Potential solutions to the massive data rate problems are: data editing, greater transmission bandwidths, higher density and faster media, and data compression. Through four subpanels on Science Payload Operations, Multispectral Imaging,  Microwave Remote Sensing and Science Data Management, recommendations were made for research  in data compression and scientific data applications to space platforms."
An airborne study of microwave surface sensing and boundary layer heat and moisture fluxes for FIFE,27.485481,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The objectives of this work were to perform imaging radar and scatterometer measurements over the Konza Prairie as a part of the First International land surface climatology project Field Experiments (EIFE) and to develop an mm-wave radiometer and the data acquisition system for this radiometer. We collected imaging radar data with the University of Kansas Side-Looking Airborne Radar (SLAR) operating at 9.375 GHz and scatterometer data with a helicopter-mounted scatterometer at 5.3 and 9.6 GHz. We also developed a 35-GHz null-balancing radiometer and data acquisition system. Although radar images showed good delineation of various features of the FIFE site, the data were not useful for quantitative analysis for extracting soil moisture information because of day-to-day changes in the system transfer characteristics. Our scatterometer results show that both C and X bands are sensitive to soil moisture variations over grass-covered soils. Scattering coefficients near vertical are about 4 dB lower for unburned areas because of the presence of a thatch layer, in comparison with those for burned areas. The results of the research have been documented in reports, oral presentations, and published papers."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",27.45565,lossy format estimate,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
A high-speed distortionless predictive image-compression scheme,27.361794,lossy format estimate,['COMMUNICATIONS AND RADAR'],A high-speed distortionless predictive image-compression scheme that is based on differential pulse code modulation output modeling combined with efficient source-code design is introduced. Experimental results show that this scheme achieves compression that is very close to the difference entropy of the source.
Study and simulation of low rate video coding schemes,27.3443,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
Use of a multimission system for cost effective support of planetary science data processing,27.314754,lossy format estimate,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","JPL's Multimission Operations Systems Office (MOSO) provides a multimission facility at JPL for processing science instrument data from NASA's planetary missions. This facility, the Multimission Image Processing System (MIPS), is developed and maintained by MOSO to meet requirements that span the NASA family of planetary missions. Although the word 'image' appears in the title, MIPS is used to process instrument data from a variety of science instruments. This paper describes the design of a new system architecture now being implemented within the MIPS to support future planetary mission activities at significantly reduced operations and maintenance cost."
[Radiation Tolerant Electronics],27.235708,lossy format estimate,['Computer Operations and Hardware'],Research work in the providing radiation tolerant electronics to NASA and the commercial sector is reported herein. There are four major sections to this report: (1) Special purpose VLSI technology section discusses the status of the VLSI projects as well as the new background technologies that have been developed; (2) Lossless data compression results provide the background and direction of new data compression pursued under this grant; (3) Commercial technology transfer presents an itemization of the commercial technology transfer; and (4) Delivery of VLSI to the Government is a solution and progress report that shows how the Government and Government contractors are gaining access to the technology that has been developed by the MRC.
Reduction of blocking effects for the JPEG baseline image compression standard,26.822775,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Transform coding has been chosen for still image compression in the Joint Photographic Experts Group (JPEG) standard. Although transform coding performs superior to many other image compression methods and has fast algorithms for implementation, it is limited by a blocking effect at low bit rates. The blocking effect is inherent in all nonoverlapping transforms. This paper presents a technique for reducing blocking while remaining compatible with the JPEG standard. Simulations show that the system results in subjective performance improvements, sacrificing only a marginal increase in bit rate."
"The deep space network  Technical report, 1 Sep. - 31 Oct. 1968",26.764353,lossy format estimate,"['FACILITIES, RESEARCH, AND SUPPORT']","Description, mission support, engineering projects, network operations, facilities, structures, and utilities of deep space network"
"Voyager radio occultation by the Uranian rings: Structure, dynamics, and particle sizes",26.596357,lossy format estimate,['LUNAR AND PLANETARY EXPLORATION'],"Diffraction of Voyager 2's 3.6 and 13 cm wavelength microwaves by the Uranian rings is removed through an inverse Fresnel transform filtering procedure that accommodates the significant eccentricity of the rings. Resulting 50 m resolution profiles at two observation longitudes: (1) reveal remarkably detailed and longitudinally varying structure, (2) provide eccentricity gradient profiles of Rings alpha, beta, and epsilon which bring into question current theoretical models for observed rigid precession, and (3) suggest that two possible unseen satellites may confine some of the very sharp edges observed via resonant interactions."
Space and Earth Science Data Compression Workshop,26.173162,lossy format estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The focus was on scientists' data requirements, as well as constraints imposed by the data collection, transmission, distribution, and archival systems. The workshop consisted of several invited papers; two described information systems for space and Earth science data, four depicted analysis scenarios for extracting information of scientific interest from data collected by Earth orbiting and deep space platforms, and a final one was a general tutorial on image data compression."
A comparison of lightning and nuclear electromagnetic pulse response of tactical shelters,26.166565,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The internal response (electromagnetic fields and cable responses) of tactical shelters is addressed. Tactical shelters are usually well-shielded systems. Apart from penetrations by signal and power lines, the main leakage paths to the interior are via seams and the environment control unit (ECU) honeycomb filter. The time domain in three-dimensional finite-difference technique is employed to determine the external and internal coupling to a shelter excited by nuclear electromagnetic pulses (NEMP) and attached lightning. The responses of interest are the internal electromagnetic fields and the voltage, current, power, and energy coupled to internal cables. Leakage through the seams and ECU filter is accomplished by their transfer impedances which relate internal electric fields to external current densities. Transfer impedances which were experimentally measured are used in the analysis. The internal numerical results are favorably compared to actual shelter test data under simulated NEMP illumination."
"NASA Tech Briefs, May 1996",25.970419,lossy format estimate,['Man/System Technology and Life Support'],Topics include: Video and Imaging;Electronic Components and Circuits; Electronic Systems; Physical Sciences; Materials; Computer Programs; Mechanics; Machinery/Automation; Manufacturing/Fabrication; Mathematics and Information Sciences; Life Sciences; Books and Reports
Studies and simulations of the DigiCipher system,25.944096,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this period the development of simulators for the various high definition television (HDTV) systems proposed to the FCC was continued. The FCC has indicated that it wants the various proposers to collaborate on a single system. Based on all available information this system will look very much like the advanced digital television (ADTV) system with major contributions only from the DigiCipher system. The results of our simulations of the DigiCipher system are described. This simulator was tested using test sequences from the MPEG committee. The results are extrapolated to HDTV video sequences. Once again, some caveats are in order. The sequences used for testing the simulator and generating the results are those used for testing the MPEG algorithm. The sequences are of much lower resolution than the HDTV sequences would be, and therefore the extrapolations are not totally accurate. One would expect to get significantly higher compression in terms of bits per pixel with sequences that are of higher resolution. However, the simulator itself is a valid one, and should HDTV sequences become available, they could be used directly with the simulator. A brief overview of the DigiCipher system is given. Some coding results obtained using the simulator are looked at. These results are compared to those obtained using the ADTV system. These results are evaluated in the context of the CCSDS specifications and make some suggestions as to how the DigiCipher system could be implemented in the NASA network. Simulations such as the ones reported can be biased depending on the particular source sequence used. In order to get more complete information about the system one needs to obtain a reasonable set of models which mirror the various kinds of sources encountered during video coding. A set of models which can be used to effectively model the various possible scenarios is provided. As this is somewhat tangential to the other work reported, the results are included as an appendix."
"Surface electrical properties experiment study phase, volume 1",25.879902,lossy format estimate,['SPACE SCIENCES'],"The evolution of a conceptual design of the flight hardware for the surface electrical properties experiment (SEP), the definition of requests for proposals, the analysis of proposals submitted by prospective flight hardware subcontractors, and recommendations for the flight configuration to be implemented are discussed. Initial efforts were made to assess the electromagnetic environment of the SEP experiment. An EMI receiver and tri-loop antenna were constructed and tests of opportunity were performed with a lunar roving vehicle (LRV). Initial analyses were made of data from these tests with support from this contract, analyses which were continued in depth under the hardware contract."
"NASA Summer Faculty Fellowship Program 2004, Volumes 1 and 2",25.713758,lossy format estimate,['Behavioral Sciences'],"The objective of the planned summer research was to develop a procedure to determine the isokinetic functional strength of suited and unsuited participants in order to estimate the coefficient of micro-gravity suit on human strength. To accomplish this objective, the Anthropometry and Biomechanics Facility Multipurpose, Multiaxial Isokinetic dynamometer (MMID) was used. Development of procedure involved selection and testing of seven routines to be tested on MMID. We conducted the related experiments and collected the data for 12 participants. In addition to the above objective, we developed a procedure to assess the fatiguing characteristics of suited and unsuited participants using EMG technique. We collected EMG data on 10 participants while performing a programmed routing on MMID. EMG data along with information on the exerted forces, effector speed, number of repetitions, and duration of each routine were recorded for further analysis. Finally, gathering and tabulation Of data for various human strengths for updating of MSIS (HSIS) strength requirement, which started in summer 2003, also continued."
A study of multiplex data bus techniques for the space shuttle,25.563868,lossy format estimate,['SPACE VEHICLES'],"A comprehensive technology base for the design of a multiplexed data bus subsystem is provided. Extensive analyses, both analytical and empirical, were performed. Subjects covered are classified under the following headings: requirements identification and analysis; transmission media studies; signal design and detection studies; synchronization, timing, and control studies; user-subsystem interface studies; operational reliability analyses; design of candidate data bus configurations; and evaluation of candidate data bus designs."
Reachability Maps for In Situ Operations,25.418882,lossy format estimate,"['Cybernetics, Artificial Intelligence and Robotics']","This work covers two programs that accomplish the same goal: creation of a ""reachability map"" from stereo imagery that tells where operators of a robotic arm can reach or touch the surface, and with which instruments. The programs are ""marsreach"" (for MER) and ""phxreach."" These programs make use of the planetary image geometry (PIG) library. However, unlike the other programs, they are not multi-mission. Because of the complexity of arm kinematics, the programs are specific to each mission."
Data compression using adaptive transform coding. Appendix 1: Item 1,25.40367,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Adaptive low-rate source coders are described in this dissertation. These coders adapt by adjusting the complexity of the coder to match the local coding difficulty of the image. This is accomplished by using a threshold driven maximum distortion criterion to select the specific coder used. The different coders are built using variable blocksized transform techniques, and the threshold criterion selects small transform blocks to code the more difficult regions and larger blocks to code the less complex regions. A theoretical framework is constructed from which the study of these coders can be explored. An algorithm for selecting the optimal bit allocation for the quantization of transform coefficients is developed. The bit allocation algorithm is more fully developed, and can be used to achieve more accurate bit assignments than the algorithms currently used in the literature. Some upper and lower bounds for the bit-allocation distortion-rate function are developed. An obtainable distortion-rate function is developed for a particular scalar quantizer mixing method that can be used to code transform coefficients at any rate."
COSMIC monthly progress report,24.80584,lossy format estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Activities of the Computer Software Management and Information Center (COSMIC) are summarized for the month of April 1994. Tables showing the current inventory of programs available from COSMIC are presented and program processing and evaluation activities are summarized. Five articles were prepared for publication in the NASA Tech Brief Journal. These articles (included in this report) describe the following software items: GAP 1.0 - Groove Analysis Program, Version 1.0; SUBTRANS - Subband/Transform MATLAB Functions for Image Processing; CSDM - COLD-SAT Dynamic Model; CASRE - Computer Aided Software Reliability Estimation; and XOPPS - OEL Project Planner/Scheduler Tool. Activities in the areas of marketing, customer service, benefits identification, maintenance and support, and disseminations are also described along with a budget summary."
Radar cross section studies/compact range research,24.686909,lossy format estimate,['COMMUNICATIONS AND RADAR'],"Achievements in advancing the state-of-the-art in the measurement, control, and analysis of electromagnetic scattering from general aerodynamic targets are summarized. The major topics associated with this study include: (1) electromagnetic scattering analysis; (2) indoor scattering measurement systems; (3) RCS control; (4) waveform processing techniques; (5) material scattering and design studies; (6) design and evaluation of standard targets; and (7) antenna studies. Progress in each of these areas is reported and related publications are listed."
Feasibility study of computer prediction of broadband near-field electromagnetic interference in a space vehicle  Final report,24.649582,lossy format estimate,['COMMUNICATIONS'],Feasibility of computer prediction of broadband near-field electromagnetic interference in space vehicles
"Multikilowatt transmitter study for space communications satellites.  Volume 1 - Summary report, phase 1",24.568056,lossy format estimate,['COMMUNICATIONS'],High power transmitters with potential applicability to satellite television broadcasting
A survey of case histories involving spacecraft dynamic interaction,24.545397,lossy format estimate,['SPACE VEHICLES'],"Control system design and analysis, and survey of case histories involving spacecraft dynamic interaction"
Lunar Crater Observation and Sensing Satellite (LCROSS) Instrument Calibration Summary,24.444817,lossy format estimate,['Instrumentation and Photography'],"This document describes the calibration of the LCROSS instruments. It will be released to the public via the Planetary Data System. We need a quick review, if possible, because the data has been delivered to the PDS, and this document is needed to interpret the LCROSS impact data fully. [My mistake [shirley) in not realizing this needed to be treated as a normal publication.] The LCROSS instruments are commercially available units except for one designed and built at Ames. The commercially available instruments don't seem to me to present ITAR issues (Sony video camera, thermal camera from England, and so on.) Also, the internal design details of the instruments are not included in this report, only the process of calibrating them against standard targets. Only very high-level descriptions of the spacecraft are included, comparable to the level of detail included in the public web pages on nasa.gov."
The Fifth NASA Symposium on VLSI Design,24.387777,lossy format estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"The fifth annual NASA Symposium on VLSI Design had 13 sessions including Radiation Effects, Architectures, Mixed Signal, Design Techniques, Fault Testing, Synthesis, Signal Processing, and other Featured Presentations. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The presentations share insights into next generation advances that will serve as a basis for future VLSI design."
Aviation Weather Information Communications Study (AWIN),24.35907,lossy format estimate,['Air Transportation and Safety'],"This two part study examines the communication requirements to provide weather information in the cockpit as well as public and private communication systems available to address the requirements. Ongoing research projects combined with user needs for weather related information are used to identify and describe potential weather products that address decision support in three time frames: Far-Term Strategic, Near-Term Strategic and Tactical. Data requirements of these future products are identified and quantified. Communications systems and technologies available in the public as well as private sector are analyzed to identify potential solutions. Recommendations for further research identify cost, performance, and safety benefits to justify the investment. The study concludes that not all weather information has the same level of urgency to safety-of-flight and some information is more critical to one category of flight than another. Specific weather products need to be matched with communication systems with appropriate levels of reliability to support the criticality of the information. Available bandwidth for highly critical information should be preserved and dedicated to safety. Meanwhile, systems designed for in-flight-entertainment and other passenger/crew services could be used to support less critical information that is used only for planning and economic decision support."
Performance analysis of CCSDS path service,24.302353,lossy format estimate,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","A communications service, called Path Service, is currently being developed by the Consultative Committee for Space Data Systems (CCSDS) to provide a mechanism for the efficient transmission of telemetry data from space to ground for complex space missions of the future. This is an important service, due to the large volumes of telemetry data that will be generated during these missions. A preliminary analysis of performance of Path Service is presented with respect to protocol-processing requirements and channel utilization."
Research in and application of state variable feedback design of guidance control systems for aerospace vehicles  Progress report,24.295881,lossy format estimate,['ELECTRONICS'],"Weighted least squares parameter estimation, Kalman filter, and random search problems for aerospace guidance control system design"
Radiation Effects on Flow Characteristics in Combustion Chambers,24.207726,lossy format estimate,['SPACECRAFT PROPULSION AND POWER'],"A JANNAF sponsored workshop was held to discuss the importance and role of radiative heat transfer in rocket combustion chambers. The potential impact of radiative transfer on hardware design, reliability, and performance was discussed. The current state of radiative transfer prediction capability in CFD modeling was reviewed and concluded to be substantially lacking in both the physical models used and the radiative property data available. There is a clear need to begin to establish a data base for making radiation calculations in rocket combustion chambers. A natural starting point for this effort would be the NASA thermochemical equilibrium code (CEC)."
High data rate systems for the future,24.054853,lossy format estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Information systems in the next century will transfer data at rates that are much greater than those in use today. Satellite based communication systems will play an important role in networking users. Typical data rates; use of microwave, millimeter wave, or optical systems; millimeter wave communication technology; modulators/exciters; solid state power amplifiers; beam waveguide transmission systems; low noise receiver technology; optical communication technology; and the potential commercial applications of these technologies are discussed."
On the electromagnetic scattering from infinite rectangular conducting grids,23.922876,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The study and development of two numerical techniques for the analysis of electromagnetic scattering from a rectangular wire mesh are described. Both techniques follow from one basic formulation and they are both solved in the spectral domain. These techniques were developed as a result of an investigation towards more efficient numerical computation for mesh scattering. These techniques are efficient for the following reasons: (a1) make use of the Fast Fourier Transform; (b2) they avoid any convolution problems by converting integrodifferential equations into algebraic equations; and (c3) they do not require inversions of any matrices. The first method, the SIT or Spectral Iteration Technique, is applied for regions where the spacing between wires is not less than two wavelengths. The second method, the SDCG or Spectral Domain Conjugate Gradient approach, can be used for any spacing between adjacent wires. A study of electromagnetic wave properties, such as reflection coefficient, induced currents and aperture fields, as functions of frequency, angle of incidence, polarization and thickness of wires is presented. Examples and comparisons or results with other methods are also included to support the validity of the new algorithms."
"Surface electrical properties experiment study phase, volume 3",23.909595,lossy format estimate,['SPACE SCIENCES'],"The reliability and quality assurance system and procedures used in developing test equipment for the Lunar Experiment projects are described. The subjects discussed include the following: (1) documentation control, (2) design review, (3) parts and materials selection, (4) material procurement, (5) inspection procedures, (6) qualification and special testing, and failure modes and effects analysis."
"A study of high performance antenna systems for deep space communication  Status report, 15 Jun. - 15 Dec. 1969",23.844284,lossy format estimate,['COMMUNICATIONS'],High performance antenna system for deep space communication
Study to investigate and evaluate means of optimizing the Ku-band combined radar/communication functions for the space shuttle,23.783413,lossy format estimate,['COMMUNICATIONS AND RADAR'],"The Ku band radar system on the shuttle orbiter operates in both a search and a tracking mode, and its transmitter and antennas share time with the communication mode in the integrated system. The power allocation properties and the Costa subloop subcarrier tracking performance associated with the baseline digital phase shift implementation of the three channel orbiter Ku band modulator are discussed."
Progressive Transmission and Compression of Images,23.767574,lossy format estimate,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
The radiation from slots in truncated dielectric-covered surfaces,23.586372,lossy format estimate,['ELECTRONIC EQUIPMENT'],"A theoretical approach based on the geometrical theory of diffraction is used to study the electromagnetic radiation from a narrow slot in a dielectric-covered perfectly-conducting surface terminated at an edge. The total far-zone field is composed of a geometrical optics field and a diffracted field. The geometrical optics field is the direct radiation from the slot to the field point. The slot also generates surface waves which are incident at the termination of the dielectric cover, where singly-diffracted rays and reflected surface waves are excited. The diffraction and reflection coefficients are obtained from the canonical problem of the diffraction of a surface wave by a right-angle wedge where the dielectric-covered surface is approximated by an impedance surface. This approximation is satisfactory for a very thin cover; however, the radiation from its vertical and faces cannot be neglected in treating the thicker dielectric cover. This is taken into account by using a Kirchhoff-type approximation, which contributes a second term to the diffraction coefficient previously obtained. The contributions from the geometrical optics field, the singly-diffracted rays and all significant multiply-diffracted rays are summed to give the total radiation. Calculated and measured patterns are found to be in good agreement."
Visual Information Processing for Television and Telerobotics,23.328104,lossy format estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],"This publication is a compilation of the papers presented at the NASA conference on Visual Information Processing for Television and Telerobotics. The conference was held at the Williamsburg Hilton, Williamsburg, Virginia on May  10 to 12, 1989. The conference was sponsored jointly by NASA Offices of Aeronautics and Space Technology (OAST) and Space Science and Applications (OSSA) and the NASA Langley Research Center. The presentations were grouped into three sessions: Image Gathering, Coding, and Advanced Concepts; Systems; and Technologies. The program was organized to provide a forum in which researchers from industry, universities, and government could be brought together to discuss the state of knowledge in image gathering, coding,  and processing methods."
Personal Access Satellite System (PASS) study. Fiscal year 1989 results,23.205816,lossy format estimate,['COMMUNICATIONS AND RADAR'],The Jet Propulsion Laboratory is exploring the potential and feasibility of a personal access satellite system (PASS) that will offer the user greater freedom and mobility than existing or currently planned communications systems. Studies performed in prior years resulted in a strawman design and the identification of technologies that are critical to the successful implementation of PASS. The study efforts in FY-89 were directed towards alternative design options with the objective of either improving the system performance or alleviating the constraints on the user terminal. The various design options and system issues studied this year and the results of the study are presented.
Lunar Orbiter photo site accuracy analysis. Part 1 - Photo site analysis  Final report,106.40533,computed photo method,['SPACE SCIENCES'],Lunar Orbiter photo site accuracy analysis
"Lunar photo study, phase 2  Final report, 1 Nov. 1965 - 1 Nov. 1966",98.596756,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],Lunar photo study of simulated lunar surfaces
Feasibility test of a solid state spin-scan photo-imaging system,96.91859,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"The feasibility of using a solid-state photo-imaging system to obtain resolution imagery from a Pioneer-type spinning spacecraft in future exploratory missions to the outer planets is discussed. Evaluation of the photo-imaging system performance, based on electrical video signal analysis recorded on magnetic tape, shows that the signal-to-noise (S/N) ratios obtained at low spatial frequencies exceed the anticipated performance and that measured modulation transfer functions exhibited some degradation in comparison with the estimated values, primarily owing to the difficulty in obtaining a precise focus of the optical system in the laboratory with the test patterns in close proximity to the objective lens. A preliminary flight model design of the photo-imaging system is developed based on the use of currently available phototransistor arrays. Image quality estimates that will be obtained are presented in terms of S/N ratios and spatial resolution for the various planets and satellites. Parametric design tradeoffs are also defined."
"Lunar Photo Study, Phase 4 Final Report, 30 Nov. 1967 - 30 Nov. 1968",92.3034,computed photo method,['SPACE SCIENCES'],Lunar photo study
Images constructed from computed flow fields,85.75746,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A method for constructing interferograms, schlieren, and shadowgraphs from ideal- and real-gas, two- and three-dimensional computed flow fields is described. The computational grids can be structured or unstructured, and multiple grids are an option. The constructed images are compared to experimental images for several types of flow, including a ramp, a blunt-body, a nozzle, and a reacting flow. The constructed images simulate the features observed in the experimental images. They are sensitive to errors in the flow-field solutions and can be used to identify solution errors. In addition, techniques for obtaining phase shifts from experimental finite-fringe interferograms and for removing experimentally induced phase-shift errors are discussed. Both the constructed images and calculated phase shifts can be used for validation of computational fluid dynamics (CFD) codes."
Image analysis of particle field by means of computed tomography,82.29819,computed photo method,['FLUID MECHANICS AND HEAT TRANSFER'],"In order to visualize and investigate spray structures, computed tomography technique is applied to analyze droplet information. From the transmitted light intensity through the spray and/or the data of particle size distribution obtained from a Fraunhofer diffraction principle, the quantitative volume of spray droplet or local particle size was calculated and the reconstruction of spray structures was made. The background of computed tomography is described along with some experimental results of the structure of intermittent spray such as diesel spray."
Method for beam hardening correction in quantitative computed X-ray tomography,81.92171,computed photo method,['Instrumentation and Photography'],"Each voxel is assumed to contain exactly two distinct materials, with the volume fraction of each material being iteratively calculated. According to the method, the spectrum of the X-ray beam must be known, and the attenuation spectra of the materials in the object must be known, and be monotonically decreasing with increasing X-ray photon energy. Then, a volume fraction is estimated for the voxel, and the spectrum is iteratively calculated."
Images constructed from computed flowfields,80.121704,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A method for constructing interferograms, schlieren, and shadowgraphs from ideal- and real-gas, two- and three-dimensional computed flowfileds is described. The computational grids can be structured or unstructured, and multiple grids are an option. The constructed images are compared to experimental images for several types of flow, including a ramp, a blunt-body, a nozzle, and a reacting flow. The constructed images simulate the features observed in the experimental images. They are sensitive to errors in the flowfield solutions and can be used to identify solution errors. In addition, techniques for obtaining phase shifts from experimental finite-fringe interferograms and for removing experimentally induced phase-shift errors are discussed. Both the constructed images and calculated phase shifts can be used for validation of computational fluid dynamics (CFD) codes."
Method and Apparatus for Computed Imaging Backscatter Radiography,79.18289,computed photo method,['Instrumentation and Photography'],"Systems and methods of x-ray backscatter radiography are provided. A single-sided, non-destructive imaging technique utilizing x-ray radiation to image subsurface features is disclosed, capable of scanning a region using a fan beam aperture and gathering data using rotational motion."
Analytic aerotriangulation utilizing Skylab earth terrain camera (S-190B) photography,74.03236,computed photo method,['EARTH RESOURCES AND REMOTE SENSING'],The author has identified the following significant results. Inherent errors in using nonmetric Skylab photography and office-identified photo control made it necessary to perform numerous block adjustment solutions involving different combinations of control and weights. The final block adjustment was executed holding to 14 of the office-identified photo control points. Solution accuracy was evaluated by comparing the analytically computed ground positions of the withheld photo control points with their known ground positions and also by determining the standard errors of these points from variance values. A horizontal position RMS error of 15 meters was attained. The maximum observed error in position at a control point was 25 meters.
Spatial image modulation to improve performance of computed tomography imaging spectrometer,73.68614,computed photo method,['Instrumentation and Photography'],"Computed tomography imaging spectrometers (""CTIS""s) having patterns for imposing spatial structure are provided. The pattern may be imposed either directly on the object scene being imaged or at the field stop aperture. The use of the pattern improves the accuracy of the captured spatial and spectral information."
Structure from Motion Photogrammetry and Micro X-Ray Computed Tomography 3-D Reconstruction Data Fusion for Non-Destructive Conservation Documentation of Lunar Samples,73.11192,computed photo method,['Lunar and Planetary Science and Exploration'],"Our team is developing a modern, cross-disciplinary approach to documentation and preservation of astromaterials, specifically lunar and meteorite samples stored at the Johnson Space Center (JSC) Lunar Sample Laboratory Facility. Apollo Lunar Sample 60639, collected as part of rake sample 60610 during the 3rd Extra-Vehicular Activity of the Apollo 16 mission in 1972, served as the first NASA-preserved lunar sample to be examined by our team in the development of a novel approach to internal and external sample visualization. Apollo Sample 60639 is classified as a breccia with a glass-coated side and pristine mare basalt and anorthosite clasts. The aim was to accurately register a 3-dimensional Micro X-Ray Computed Tomography (XCT)-derived internal composition data set and a Structure-From-Motion (SFM) Photogrammetry-derived high-fidelity, textured external polygonal model of Apollo Sample 60639. The developed process provided the means for accurate, comprehensive, non-destructive visualization of NASA's heritage lunar samples. The data products, to be ultimately served via an end-user web interface, will allow researchers and the public to interact with the unique heritage samples, providing a platform to ""slice through"" a photo-realistic rendering of a sample to analyze both its external visual and internal composition simultaneously."
"Photogrammetry and photo interpretation applied to analyses of cloud cover, cloud type, and cloud motion",72.80032,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A determination was made of the areal extent of terrain obscured by clouds and cloud shadows on a portion of an Apollo 9 photograph at the instant of exposure. This photogrammetrically determined area was then compared to the cloud coverage reported by surface weather observers at approximately the same time and location, as a check on result quality. Stereograms prepared from Apollo 9 vertical photographs, illustrating various percentages of cloud coverage, are presented to help provide a quantitative appreciation of the degradation of terrain photography by clouds and their attendant shadows. A scheme, developed for the U.S. Navy, utilizing pattern recognition techniques for determining cloud motion from sequences of satellite photographs, is summarized. Clouds, turbulence, haze, and solar altitude, four elements of our natural environment which affect aerial photographic missions, are each discussed in terms of their effects on imagery obtained by aerial photography. Data of a type useful to aerial photographic mission planners, expressing photographic ground coverage in terms of flying height above terrain and camera focal length, for a standard aerial photograph format, are provided. Two oblique orbital photographs taken during the Apollo 9 flight are shown, and photo-interpretations, discussing the cloud types imaged and certain visible geographical features, are provided."
"Development of X-Ray Computed Tomography Image Quality Indicators, ""IQIs""",72.377014,computed photo method,"['Quality Assurance and Reliability', 'Instrumentation and Photography']","The intent of this effort, which is funded through the NASA OSMA NDE Program, is to develop a methodology and tools to assess Computed Tomography (CT) system performance. In particular, our team has fabricated Image Quality Indicators (IQIs) using materials and internal features useful for assessing CT detectability limits, contrast sensitivity, and resolution. Unlike traditional 2D radiography IQIs, those for CT should be more conducive to volumetric datasets and of uniform aspect ratio cross-sections."
Patterns of equivalent blackbody temperature and reflectance of model clouds computed by changing radiometers field of view,72.253716,computed photo method,['METEOROLOGY'],Equivalent blackbody temperature and reflectance patterns of model clouds computed by changing radiometer field of view
Color camera computed tomography imaging spectrometer for improved spatial-spectral image accuracy,69.87439,computed photo method,['Instrumentation and Photography'],"Computed tomography imaging spectrometers (""CTIS""s) having color focal plane array detectors are provided. The color FPA detector may comprise a digital color camera including a digital image sensor, such as a Foveon X3.RTM. digital image sensor or a Bayer color filter mosaic. In another embodiment, the CTIS includes a pattern imposed either directly on the object scene being imaged or at the field stop aperture. The use of a color FPA detector and the pattern improves the accuracy of the captured spatial and spectral information."
Hypersonic Boundary Layer Transition Measurements Using NO2 approaches NO Photo-dissociation Tagging Velocimetry,67.51479,computed photo method,['Aerodynamics'],"Measurements of instantaneous and mean streamwise velocity profiles in a hypersonic laminar boundary layer as well as a boundary layer undergoing laminar-to-turbulent transition were obtained over a 10-degree half-angle wedge model. A molecular tagging velocimetry technique consisting of a NO2 approaches?NO photo-dissociation reaction and two subsequent excitations of NO was used. The measurement of the transitional boundary layer velocity profiles was made downstream of a 1-mm tall, 4-mm diameter cylindrical trip along several lines lying within a streamwise measurement plane normal to the model surface and offset 6-mm from the model centerline. For laminar and transitional boundary layer measurements, the magnitudes of streamwise velocity fluctuations are compared. In the transitional boundary layer the fluctuations were, in general, 2-4 times larger than those in the laminar boundary layer. Of particular interest were fluctuations corresponding to a height of approximately 50% of the laminar boundary layer thickness having a magnitude of nearly 30% of the mean measured velocity. For comparison, the measured fluctuations in the laminar boundary layer were approximately 5% of the mean measured velocity at the same location. For the highest 10% signal-to-noise ratio data, average single-shot uncertainties using a 1 ?Es and 50 ?Es interframe delay were ~115 m/s and ~3 m/s, respectively. By averaging single-shot measurements of the transitional boundary layer, uncertainties in mean velocity as low as 39 m/s were obtained in the wind tunnel. The wall-normal and streamwise spatial resolutions were 0.14-mm (2 pixel) and 0.82-mm (~11 pixels), respectively. These measurements were performed in the 31-inch Mach 10 Air Wind Tunnel at the NASA Langley Research Center."
X-Ray Computed Tomography: The First Step in Mars Sample Return Processing,63.302082,computed photo method,"['Instrumentation and Photography', 'Lunar and Planetary Science and Exploration']","The Mars 2020 rover mission will collect and cache samples from the martian surface for possible retrieval and subsequent return to Earth. If the samples are returned, that mission would likely present an opportunity to analyze returned Mars samples within a geologic context on Mars. In addition, it may provide definitive information about the existence of past or present life on Mars. Mars sample return presents unique challenges for the collection, containment, transport, curation and processing of samples [1] Foremost in the processing of returned samples are the closely paired considerations of life detection and Planetary Protection. In order to achieve Mars Sample Return (MSR) science goals, reliable analyses will depend on overcoming some challenging signal/noise-related issues where sparse martian organic compounds must be reliably analyzed against the contamination background. While reliable analyses will depend on initial clean acquisition and robust documentation of all aspects of developing and managing the cache [2], there needs to be a reliable sample handling and analysis procedure that accounts for a variety of materials which may or may not contain evidence of past or present martian life. A recent report [3] suggests that a defined set of measurements should be made to effectively inform both science and Planetary Protection, when applied in the context of the two competing null hypotheses: 1) that there is no detectable life in the samples; or 2) that there is martian life in the samples. The defined measurements would include a phased approach that would be accepted by the community to preserve the bulk of the material, but provide unambiguous science data that can be used and interpreted by various disciplines. Fore-most is the concern that the initial steps would ensure the pristine nature of the samples. Preliminary, non-invasive techniques such as computed X-ray tomography (XCT) have been suggested as the first method to interrogate and characterize the cached samples without altering the materials [1,2]. A recent report [4] indicates that XCT may minimally alter samples for some techniques, and work is needed to quantify these effects, maximizing science return from XCT initial analysis while minimizing effects."
Hydrography synthesis using LANDSAT remote sensing and the SCS models,62.827137,computed photo method,['EARTH RESOURCES AND REMOTE SENSING'],The land cover requirements of the Soil Conservation Service (SCS) Model used for hydrograph synthesis in urban areas were modified to be LANDSAT compatible. The Curve Numbers obtained with these alternate land cover categories compare well with those obtained in published example problems using the conventional categories. Emergency spillway hydrographs and synthetic flood frequency flows computed for a 21.1 sq. mi. test area showed excellent agreement between the conventional aerial photo-based and the Landsat-based SCS approaches.
Method and system for efficient video compression with low-complexity encoder,61.568596,computed photo method,['Instrumentation and Photography'],"Disclosed are a method and system for video compression, wherein the video encoder has low computational complexity and high compression efficiency. The disclosed system comprises a video encoder and a video decoder, wherein the method for encoding includes the steps of converting a source frame into a space-frequency representation; estimating conditional statistics of at least one vector of space-frequency coefficients; estimating encoding rates based on the said conditional statistics; and applying Slepian-Wolf codes with the said computed encoding rates. The preferred method for decoding includes the steps of; generating a side-information vector of frequency coefficients based on previously decoded source data, encoder statistics, and previous reconstructions of the source frequency vector; and performing Slepian-Wolf decoding of at least one source frequency vector based on the generated side-information, the Slepian-Wolf code bits and the encoder statistics."
Flight test evaluation of a method to determine the level flight performance propeller-driven aircraft,59.991302,computed photo method,"['AIRCRAFT DESIGN, TESTING AND PERFORMANCE']","A procedure is developed for deriving the level flight drag and propulsive efficiency of propeller-driven aircraft. This is a method in which the overall drag of the aircraft is expressed in terms of the measured increment of power required to overcome a corresponding known increment of drag. The aircraft is flown in unaccelerated, straight and level flight, and thus includes the effects of the propeller drag and slipstream. Propeller efficiency and airplane drag are computed on the basis of data obtained during flight test and do not rely on the analytical calculations of inadequate theory."
Computer-implemented method and apparatus for autonomous position determination using magnetic field data,57.776215,computed photo method,['Instrumentation and Photography'],A computer-implemented method and apparatus for determining position of a vehicle within 100 km autonomously from magnetic field measurements and attitude data without a priori knowledge of position. An inverted dipole solution of two possible position solutions for each measurement of magnetic field data are deterministically calculated by a program controlled processor solving the inverted first order spherical harmonic representation of the geomagnetic field for two unit position vectors 180 degrees apart and a vehicle distance from the center of the earth. Correction schemes such as a successive substitutions and a Newton-Raphson method are applied to each dipole. The two position solutions for each measurement are saved separately. Velocity vectors for the position solutions are calculated so that a total energy difference for each of the two resultant position paths is computed. The position path with the smaller absolute total energy difference is chosen as the true position path of the vehicle.
Computed tomography imaging spectrometer (CTIS) with 2D reflective grating for ultraviolet to long-wave infrared detection especially useful for surveying transient events,57.484085,computed photo method,['Instrumentation and Photography'],"The optical system of this invention is an unique type of imaging spectrometer, i.e. an instrument that can determine the spectra of all points in a two-dimensional scene. The general type of imaging spectrometer under which this invention falls has been termed a computed-tomography imaging spectrometer (CTIS). CTIS's have the ability to perform spectral imaging of scenes containing rapidly moving objects or evolving features, hereafter referred to as transient scenes. This invention, a reflective CTIS with an unique two-dimensional reflective grating, can operate in any wavelength band from the ultraviolet through long-wave infrared. Although this spectrometer is especially useful for rapidly occurring events it is also useful for investigation of some slow moving phenomena as in the life sciences."
Computed Tomography Imaging Spectrometer (CTIS) with 2D Reflective Grating for Ultraviolet to Long-Wave Infrared Detection Especially Useful for Surveying Transient Events,56.783566,computed photo method,['Instrumentation and Photography'],"The optical system of this invention is an unique type of imaging spectrometer, i.e. an instrument that can determine the spectra of all points in a two-dimensional scene. The general type of imaging spectrometer under which this invention falls has been termed a computed-tomography imaging spectrometer (CTIS). CTIS's have the ability to perform spectral imaging of scenes containing rapidly moving objects or evolving features, hereafter referred to as transient scenes. This invention, a reflective CTIS with an unique two-dimensional reflective grating, can operate in any wavelength band from the ultraviolet through long-wave infrared. Although this spectrometer is especially useful for events it is also for investigation of some slow moving phenomena as in the life sciences."
Method to Measure Total Noise Temperature of a Wireless Receiver During Operation,55.588623,computed photo method,"['Instrumentation and Photography', 'Acoustics']","An electromagnetic signal receiver and methods for determining the noise level and signal power in a signal of interest while the receiver is operating. In some embodiments, the signal of interest is a GPS signal. The receiver includes a noise source that provides a noise signal of known power during intervals while the signal of interest is observed. By measuring a signal-to-noise ratio for the signal of interest and the noise power in the signal of interest, the noise level and signal power of the signal of interest can be computed. Various methods of making the measurements and computing the power of the signal of interest are described. Applications of the system and method are described."
Material Characterization and Geometric Segmentation of a Composite Structure Using Microfocus X-Ray Computed Tomography Image-Based Finite Element Modeling,54.90937,computed photo method,['Space Transportation and Safety'],"This study utilizes microfocus x-ray computed tomography (CT) slice sets to model and characterize the damage locations and sizes in thermal protection system materials that underwent impact testing. ScanIP/FE software is used to visualize and process the slice sets, followed by mesh generation on the segmented volumetric rendering. Then, the local stress fields around several of the damaged regions are calculated for realistic mission profiles that subject the sample to extreme temperature and other severe environmental conditions. The resulting stress fields are used to quantify damage severity and make an assessment as to whether damage that did not penetrate to the base material can still result in catastrophic failure of the structure. It is expected that this study will demonstrate that finite element modeling based on an accurate three-dimensional rendered model from a series of CT slices is an essential tool to quantify the internal macroscopic defects and damage of a complex system made out of thermal protection material. Results obtained showing details of segmented images; three-dimensional volume-rendered models, finite element meshes generated, and the resulting thermomechanical stress state due to impact loading for the material are presented and discussed. Further, this study is conducted to exhibit certain high-caliber capabilities that the nondestructive evaluation (NDE) group at NASA Glenn Research Center can offer to assist in assessing the structural durability of such highly specialized materials so improvements in their performance and capacities to handle harsh operating conditions can be made."
"Voyager spacecraft phase B, task D. Volume 4 - Engineering tasks.  Book 5 - Photo imaging  Final report",53.556904,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],Alternate methods of performing photoimaging experiments of Martian surface from orbiting Voyager spacecraft
"Interaction of Delaminations and Matrix Cracks in a CFRP Plate, Part I: A Test Method for Model Validation",52.500763,computed photo method,['Composite Materials'],"Isolating and observing the damage mechanisms associated with low-velocity impact in composites using traditional experiments can be challenging, due to damage process complexity and high strain rates. In this work, a new test method is presented that provides a means to study, in detail, the interaction of common impact damage mechanisms, namely delamination, matrix cracking, and delamination-migration, in a context less challenging than a real impact event. Carbon fiber reinforced polymer specimens containing a thin insert in one region were loaded in a biaxial-bending state of deformation. As a result, three-dimensional damage processes, involving delaminations at no more than three different interfaces that interact with one another via transverse matrix cracks, were observed and documented using ultrasonic testing and X-ray computed tomography. The data generated by the test is intended for use in numerical model validation. Simulations of this test are included in Part II of this paper. "
Development and application of a photoelasto- plastic method to study stress distributions in vicinity of a simulated crack,52.114197,computed photo method,['STRUCTURAL MECHANICS'],Photoelasto-plastic method for studying stress distributions in vicinity of simulated crack
"Voyager spacecraft phase B, task D. Volume 11 - Engineering study task - Photo imaging system  Final report",51.90223,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],Systems engineering analysis of photoimaging systems for optimum photographic coverage of Mars during 1973 Voyager mission
Simulations of 6-DOF Motion with a Cartesian Method,51.575092,computed photo method,"['Aircraft Design, Testing and Performance']","Coupled 6-DOF/CFD trajectory predictions using an automated Cartesian method are demonstrated by simulating a GBU-32/JDAM store separating from an F-18C aircraft. Numerical simulations are performed at two Mach numbers near the sonic speed, and compared with flight-test telemetry and photographic-derived data. Simulation results obtained with a sequential-static series of flow solutions are contrasted with results using a time-dependent flow solver. Both numerical methods show good agreement with the flight-test data through the first half of the simulations. The sequential-static and time-dependent methods diverge over the last half of the trajectory prediction. after the store produces peak angular rates. A cost comparison for the Cartesian method is included, in terms of absolute cost and relative to computing uncoupled 6-DOF trajectories. A detailed description of the 6-DOF method, as well as a verification of its accuracy, is provided in an appendix."
Determination of principal stress in birefringent composites by hole-drilling method,51.136948,computed photo method,['COMPOSITE MATERIALS'],The application of transmission photoelasticity to stress analysis of composite materials is discussed.The method consists in drilling very small holes at points where the state of stress has to be determined. Experiments are described which verify the theoretical predicitons. The limitations of the method are discussed and it is concluded that valuable information concerning the state of stress in a composite model can be obtained through the suggested method.
Space and related biological and instrumentation studies,51.097683,computed photo method,['BIOTECHNOLOGY'],"Research and experimental effort was carried out on high-density photo-optical recorder design, implantable pH electrodes and the mangetic/doppler blood-flow sensor."
Lunar Orbiter photo site accuracy analysis. Part 3 - Error analysis  Final report,50.560646,computed photo method,['SPACE SCIENCES'],Selenographic error analysis of photographs on Lunar Orbiter missions
Solar flare model atmospheres,50.525024,computed photo method,['SOLAR PHYSICS'],"Solar flare model atmospheres computed under the assumption of energetic equilibrium in the chromosphere are presented. The models use a static, one-dimensional plane parallel geometry and are designed within a physically self-consistent coronal loop. Assumed flare heating mechanisms include collisions from a flux of non-thermal electrons and x-ray heating of the chromosphere by the corona. The heating by energetic electrons accounts explicitly for variations of the ionized fraction with depth in the atmosphere. X-ray heating of the chromosphere by the corona incorporates a flare loop geometry by approximating distant portions of the loop with a series of point sources, while treating the loop leg closest to the chromospheric footpoint in the plane-parallel approximation. Coronal flare heating leads to increased heat conduction, chromospheric evaporation and subsequent changes in coronal pressure; these effects are included self-consistently in the models. Cooling in the chromosphere is computed in detail for the important optically thick HI, CaII and MgII transitions using the non-LTE prescription in the program MULTI. Hydrogen ionization rates from x-ray photo-ionization and collisional ionization by non-thermal electrons are included explicitly in the rate equations. The models are computed in the 'impulsive' and 'equilibrium' limits, and in a set of intermediate 'evolving' states. The impulsive atmospheres have the density distribution frozen in pre-flare configuration, while the equilibrium models assume the entire atmosphere is in hydrostatic and energetic equilibrium. The evolving atmospheres represent intermediate stages where hydrostatic equilibrium has been established in the chromosphere and corona, but the corona is not yet in energetic equilibrium with the flare heating source. Thus, for example, chromospheric evaporation is still in the process of occurring."
"Organic semiconductors,  ii-  properties, synthesis, and application",50.44933,computed photo method,"['PHYSICS, SOLID-STATE']","Organic semiconductors - conductivity and semiconductivity, photo conductivity, excited states, and dielectric effects"
THREED:  A computer program for three dimensional transformation of coordinates,50.306965,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],Program THREED was developed for the purpose of a research study on the treatment of control data in lunar phototriangulation. THREED is the code name of a computer program for performing absolute orientation by the method of three-dimensional projective transformation. It has the capability of performing complete error analysis on the computed transformation parameters as well as the transformed coordinates.
A Worst-Case Approach for On-Line Flutter Prediction,49.84493,computed photo method,['Aircraft Stability and Control'],Worst-case flutter margins may be computed for a linear model with respect to a set of uncertainty operators using the structured singular value. This paper considers an on-line implementation to compute these robust margins in a flight test program. Uncertainty descriptions are updated at test points to account for unmodeled time-varying dynamics of the airplane by ensuring the robust model is not invalidated by measured flight data. Robust margins computed with respect to this uncertainty remain conservative to the changing dynamics throughout the flight. A simulation clearly demonstrates this method can improve the efficiency of flight testing by accurately predicting the flutter margin to improve safety while reducing the necessary flight time.
Micro-Raman spectroscopy:  The analysis of micrometer and submicrometer atmospheric aerosols,49.69606,computed photo method,['GEOPHYSICS'],"A nondestructive method of molecular analysis which is required to fully utilize the information contained within a collected particle is discussed. Upper atmosphere reaction mechanisms are assessed when the chemical compounds, the use of micro-Raman spectrometric techniques to perform micron and submicron particle analysis was evaluated. The results are favorable and it is concluded that micron and submicron particles can be analyzed by the micron-Raman approach. Completely automatic analysis should be possible to 0.3 micro m. No problems are anticipated with photo or thermal decomposition. Sample and impurity fluorescence are the key source of background as they cannot be completely eliminated."
Stress Analysis of Columns and Beam Columns by the Photoelastic Method,49.346436,computed photo method,['GEOPHYSICS'],"Principles of similarity and other factors in the design of models for photoelastic testing are discussed. Some approximate theoretical equations, useful in the analysis of results obtained from photoelastic tests are derived. Examples of the use of photoelastic techniques and the analysis of results as applied to uniform and tapered beam columns, circular rings, and statically indeterminate frames, are given. It is concluded that this method is an effective tool for the analysis of structures in which column action is present, particularly in tapered beam columns, and in statically indeterminate structures in which the distribution of loads in the structures is influenced by bending moments due to axial loads in one or more members."
A Method to Analyze Tail Buffet Loads of Aircraft,49.21189,computed photo method,"['Aircraft Design, Testing and Performance']","Aircraft designers commit significant resources to the design of aircraft in meeting performance goals. Despite fulfilling traditional design requirements, many fighter aircraft have encountered buffet loads when demonstrating their high angle-of-attack maneuver capabilities. As a result, during test or initial production phases of fighter development programs, many new designs are impacted, usually in a detrimental way, by resulting in reassessing designs or limiting full mission capability. These troublesome experiences usually stem from overlooking or completely ignoring the effects of buffet during the design phase of aircraft. Perhaps additional requirements are necessary that addresses effects of buffet in achieving best aircraft performance in fulfilling mission goals. This paper describes a reliable, fairly simple, but quite general buffet loads analysis method to use in the initial design phases of fighter-aircraft development. The method is very similar to the random gust load analysis that is now commonly available in a commercial code, which this analysis capability is based, with some key modifications. The paper describes the theory and the implementation of the methodology. The method is demonstrated on a JSF prototype example problem. The demonstration also serves as a validation of the method, since, in the paper, the analysis is shown to nearly match the flight data. In addition, the paper demonstrates how the analysis method can be used to assess candidate design concepts in determining a satisfactory final aircraft configuration."
Urban environmental health applications of remote sensing,48.96955,computed photo method,['SOCIAL SCIENCES (GENERAL)'],"An urban area was studied through the use of the inventory-by-surrogate method rather than by direct interpretation of photographic imagery. Prior uses of remote sensing in urban and public research are examined. The effects of crowding, poor housing conditions, air pollution, and street conditions on public health are considered. Color infrared photography was used to categorize land use features and the grid method was used in photo interpretation analysis. The incidence of shigella and salmonella, hepatitis, meningitis, tuberculosis, myocardial infarction and veneral disease were studied, together with mortality and morbidity rates. Sample census data were randomly collected and validated. The hypothesis that land use and residential quality are associated with and act as an influence upon health and physical well-being was studied and confirmed."
Three-dimensional computed tomography from interferometric measurements within a narrow cone of views,48.66479,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A theory to determine the properties of a fluid from measurements of its projections was developed and tested. Viewing cones as small as 10 degrees were evaluated, with the only assumption being that the property was space limited. The results of applying the theory to numerical and actual interferograms of a spherical discontinuity of refractive index are presented. The theory was developed to test the practicality and limits of using three-dimensional computer tomography in internal fluid dynamics."
Method and System For an Automated Tool for En Route Traffic Controllers,48.631866,computed photo method,['Mechanical Engineering'],"A method and system for a new automation tool for en route air traffic controllers first finds all aircraft flying on inefficient routes, then determines whether it is possible to save time by bypassing some route segments, and finally whether the improved route is free of conflicts with other aircraft. The method displays all direct-to eligible aircraft to an air traffic controller in a list sorted by highest time savings. By allowing the air traffic controller to easily identify and work with the highest pay-off aircraft, the method of the present invention contributes to a significant increase in both air traffic controller and aircraft productivity. A graphical computer interface (GUI) is used to enable the air traffic controller to send the aircraft direct to a waypoint or fix closer to the destination airport by a simple point and click action."
Method and system for an automated tool for en route traffic controllers,48.518974,computed photo method,['Air Transportation and Safety'],"A method and system for a new automation tool for en route air traffic controllers first finds all aircraft flying on inefficient routes, then determines whether it is possible to save time by bypassing some route segments, and finally whether the improved route is free of conflicts with other aircraft. The method displays all direct-to eligible aircraft to an air traffic controller in a list sorted by highest time savings. By allowing the air traffic controller to easily identify and work with the highest pay-off aircraft, the method of the present invention contributes to a significant increase in both air traffic controller and aircraft productivity. A graphical computer interface (GUI) is used to enable the air traffic controller to send the aircraft direct to a waypoint or fix closer to the destination airport by a simple point and click action."
A method of atmospheric density measurements during space shuttle entry using ultraviolet-laser Rayleigh scattering,48.513523,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"An analytical study and its experimental verification are described which show the performance capabilities and the hardware requirements of a method for measuring atmospheric density along the Space Shuttle flightpath during entry. Using onboard instrumentation, the technique relies on Rayleigh scattering of light from a pulsed ArF excimer laser operating at a wavelength of 193 nm. The method is shown to be capable of providing density measurements with an uncertainty of less than 1 percent and with a spatial resolution along the flightpath of 1 km, over an altitude range from 50 to 90 km. Experimental verification of the signal linearity and the expected signal-to-noise ratios is demonstrated in a simulation facility at conditions that duplicate the signal levels of the flight environment."
Treatment of control data in lunar phototriangulation,48.402363,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"In lunar phototriangulation, there is a complete lack of accurate ground control points. The accuracy analysis of the results of lunar phototriangulation must, therefore, be completely dependent on statistical procedure. It was the objective of this investigation to examine the validity of the commonly used statistical procedures, and to develop both mathematical techniques and computer softwares for evaluating (1) the accuracy of lunar phototriangulation; (2) the contribution of the different types of photo support data on the accuracy of lunar phototriangulation; (3) accuracy of absolute orientation as a function of the accuracy and distribution of both the ground and model points; and (4) the relative slope accuracy between any triangulated pass points."
Image processing of aerodynamic data,47.641376,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"The use of digital image processing techniques in analyzing and evaluating aerodynamic data is discussed. An image processing system that converts images derived from digital data or from transparent film into black and white, full color, or false color pictures is described. Applications to black and white images of a model wing with a NACA 64-210 section in simulated rain and to computed low properties for transonic flow past a NACA 0012 airfoil are presented. Image processing techniques are used to visualize the variations of water film thicknesses on the wing model and to illustrate the contours of computed Mach numbers for the flow past the NACA 0012 airfoil. Since the computed data for the NACA 0012 airfoil are available only at discrete spatial locations, an interpolation method is used to provide values of the Mach number over the entire field."
Strain Gauge Balance Uncertainty Analysis at NASA Langley: A Technical Review,47.61003,computed photo method,['Instrumentation and Photography'],This paper describes a method to determine the uncertainties of measured forces and moments from multi-component force balances used in wind tunnel tests. A multivariate regression technique is first employed to estimate the uncertainties of the six balance sensitivities and 156 interaction coefficients derived from established balance calibration procedures. These uncertainties are then employed to calculate the uncertainties of force-moment values computed from observed balance output readings obtained during tests. Confidence and prediction intervals are obtained for each computed force and moment as functions of the actual measurands. Techniques are discussed for separate estimation of balance bias and precision uncertainties.
Method for implementation of recursive hierarchical segmentation on parallel computers,47.33461,computed photo method,['Instrumentation and Photography'],"A method, computer readable storage, and apparatus for implementing a recursive hierarchical segmentation algorithm on a parallel computing platform. The method includes setting a bottom level of recursion that defines where a recursive division of an image into sections stops dividing, and setting an intermediate level of recursion where the recursive division changes from a parallel implementation into a serial implementation. The segmentation algorithm is implemented according to the set levels. The method can also include setting a convergence check level of recursion with which the first level of recursion communicates with when performing a convergence check."
System and method for object localization,47.07718,computed photo method,['Instrumentation and Photography'],"A computer-assisted method for localizing a rack, including sensing an image of the rack, detecting line segments in the sensed image, recognizing a candidate arrangement of line segments in the sensed image indicative of a predetermined feature of the rack, generating a matrix of correspondence between the candidate arrangement of line segments and an expected position and orientation of the predetermined feature of the rack, and estimating a position and orientation of the rack based on the matrix of correspondence."
"Turbulence measurements in supersonic, shock- free jets by the optical crossed-beam method",46.974594,computed photo method,['FLUID MECHANICS'],Turbulence measurements in supersonic shock-free jets by optical crossed beam method
A program for computer gridding of satellite photographs for mesoscale research,46.401176,computed photo method,['GEOPHYSICS'],Computer program for gridding of satellite photographs using coordinate transformations of Fujita graphical method
A Tail Buffet Loads Prediction Method for Aircraft at High Angles of Attack,46.242653,computed photo method,"['Aircraft Design, Testing and Performance']","Aircraft designers commit significant resources to the design of aircraft in meeting performance goals. Despite fulfilling traditional design requirements, many fighter aircraft have encountered buffet loads when demonstrating their high angle-of-attack maneuver capabilities. As a result, during test or initial production phases of fighter development programs, many new designs are impacted, usually in a detrimental way, by resulting in reassessing designs or limiting full mission capability. These troublesome experiences usually stem from overlooking or completely ignoring the effects of buffet during the design phase of aircraft. Perhaps additional requirements are necessary that addresses effects of buffet in achieving best aircraft performance in fulfilling mission goals. This paper describes a reliable, fairly simple, but quite general buffet loads analysis method to use in the initial design phases of fighter-aircraft development. The method is very similar to the random gust load analysis that is now commonly available in a commercial code, which this analysis capability is based, with some key modifications. The paper describes the theory and the implementation of the methodology. The method is demonstrated on a JSF prototype example problem. The demonstration also serves as a validation of the method, since, in the paper, the analysis is shown to nearly match the flight data. In addition, the paper demonstrates how the analysis method can be used to assess candidate design concepts in determining a satisfactory final aircraft configuration."
Work function measurements by the field emission retarding potential method,46.149296,computed photo method,"['PHYSICS, SOLID-STATE']","Using the field emission retarding potential method true work functions have been measured for the following monocrystalline substrates: W(110), W(111), W(100), Nb(100), Ni(100), Cu(100), Ir(110) and Ir(111). The electron elastic and inelastic reflection coefficients from several of these surfaces have also been examined near zero primary beam energy."
Method and System for Object Recognition Search,46.136974,computed photo method,['Instrumentation and Photography'],A method for object recognition using shape and color features of the object to be recognized. An adaptive architecture is used to recognize and adapt the shape and color features for moving objects to enable object recognition.
Method of Poisson's ratio imaging within a material part,46.129364,computed photo method,['Numerical Analysis'],The present invention is directed to a method of displaying the Poisson's ratio image of a material part. In the present invention longitudinal data is produced using a longitudinal wave transducer and shear wave data is produced using a shear wave transducer. The respective data is then used to calculate the Poisson's ratio for the entire material part. The Poisson's ratio approximations are then used to displayed the image.
PRELIMINARY ANALYSIS OF FEASIBILITY OF MAPPING THE MOON BY MEANS OF PHOTOGRAMMETRY AND PHOTO INTERPRETATION PART I. PHOTOGRAMMETRY,45.929085,computed photo method,['BIOTECHNOLOGY'],The present invention is directed to a method of displaying the Poisson's ratio image of a material part. In the present invention longitudinal data is produced using a longitudinal wave transducer and shear wave data is produced using a shear wave transducer. The respective data is then used to calculate the Poisson's ratio for the entire material part. The Poisson's ratio approximations are then used to displayed the image.
A Special Investigation to Develop a General Method for Three-dimensional Photoelastic Stress Analysis,45.88833,computed photo method,['BIOTECHNOLOGY'],The method of strain measurement after annealing is reviewed and found to be satisfactory for the materials available in this country. A new general method is described for the photoelastic determination of the principal stresses at any point of a general body subjected to arbitrary load. The method has been applied to a sphere subjected to diametrical compressive loads. The results show possibilities of high accuracy.
"Lunar cartographic dossier, volume 1",45.720913,computed photo method,['LUNAR AND PLANETARY EXPLORATION'],"The dossier is designed to provide an up to date summary of the extent and quality of cartographic information as well as describing materials available to support lunar scientific investigation and study. It covers the specific photographic, selenodetic and cartographic data considered to be of continuing significance to users of lunar cartographic information. Historical background data is included. Descriptive and evaluative information is presented concerning lunar maps, photomaps and photo mosaics. Discussion comprises identification of series or individual sheet characteristics, control basis, source materials and compilation methodology used. The global, regional and local selenodetic control are described which were produced for lunar feature location in support of lunar mapping or positional study. Further discussion covers the fundamental basis for each control system, number of points produced, techniques employed and evaluated accuracy. Although lunar photography is an informational source rather than a cartographic product, a photography section was included to facilitate correlation to the mapping and control works described. Description of lunar photographic systems, photography and photo support data are presented from a cartographic-photogrammetric viewpoint with commentary on cartographic applications."
A Method to Solve Interior and Exterior Camera Calibration Parameters for Image Resection,45.683914,computed photo method,['Instrumentation and Photography'],"An iterative method is presented to solve the internal and external camera calibration parameters, given model target points and their images from one or more camera locations. The direct linear transform formulation was used to obtain a guess for the iterative method, and herein lies one of the strengths of the present method. In all test cases, the method converged to the correct solution. In general, an overdetermined system of nonlinear equations is solved in the least-squares sense. The iterative method presented is based on Newton-Raphson for solving systems of nonlinear algebraic equations. The Jacobian is analytically derived and the pseudo-inverse of the Jacobian is obtained by singular value decomposition."
A GPS-Based Pitot-Static Calibration Method Using Global Output-Error Optimization,45.609814,computed photo method,['Geophysics'],"Pressure-based airspeed and altitude measurements for aircraft typically require calibration of the installed system to account for pressure sensing errors such as those due to local flow field effects. In some cases, calibration is used to meet requirements such as those specified in Federal Aviation Regulation Part 25. Several methods are used for in-flight pitot-static calibration including tower fly-by, pacer aircraft, and trailing cone methods. In the 1990 s, the introduction of satellite-based positioning systems to the civilian market enabled new inflight calibration methods based on accurate ground speed measurements provided by Global Positioning Systems (GPS). Use of GPS for airspeed calibration has many advantages such as accuracy, ease of portability (e.g. hand-held) and the flexibility of operating in airspace without the limitations of test range boundaries or ground telemetry support. The current research was motivated by the need for a rapid and statistically accurate method for in-flight calibration of pitot-static systems for remotely piloted, dynamically-scaled research aircraft. Current calibration methods were deemed not practical for this application because of confined test range size and limited flight time available for each sortie. A method was developed that uses high data rate measurements of static and total pressure, and GPSbased ground speed measurements to compute the pressure errors over a range of airspeed. The novel application of this approach is the use of system identification methods that rapidly compute optimal pressure error models with defined confidence intervals in nearreal time. This method has been demonstrated in flight tests and has shown 2- bounds of approximately 0.2 kts with an order of magnitude reduction in test time over other methods. As part of this experiment, a unique database of wind measurements was acquired concurrently with the flight experiments, for the purpose of experimental validation of the optimization method. This paper describes the GPS-based pitot-static calibration method developed for the AirSTAR research test-bed operated as part of the Integrated Resilient Aircraft Controls (IRAC) project in the NASA Aviation Safety Program (AvSP). A description of the method will be provided and results from recent flight tests will be shown to illustrate the performance and advantages of this approach. Discussion of maneuver requirements and data reduction will be included as well as potential applications."
Analysis and testing of a new method for drop size measurement using laser scatter interferometry,45.605785,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],Research was conducted on a laser light scatter detection method for measuring the size and velocity of spherical particles. The method is based upon the measurement of the interference fringe pattern produced by spheres passing through the intersection of two laser beams. A theoretical analysis of the method was carried out using the geometrical optics theory. Experimental verification of the theory was obtained by using monodisperse droplet streams. Several optical configurations were tested to identify all of the parametric effects upon the size measurements. Both off-axis forward and backscatter light detection were utilized. Simulated spray environments and fuel spray nozzles were used in the evaluation of the method. The measurements of the monodisperse drops showed complete agreement with the theoretical predictions. The method was demonstrated to be independent of the beam intensity and extinction resulting from the surrounding drops. Signal processing concepts were considered and a method was selected for development.
The Smoke-Trail Method for Obtaining Detailed Measurements of the Vertical Wind Profile for Application to Missile-Dynamic-Response Problems,45.596954,computed photo method,['Instrumentation and Photography'],"Detailed measurements of the vertical profile of the wind are urgently needed f o r studies of the response of rising missiles, but conventional wind-measuring systems cannot provide the accuracy and detail required over the necessary range of altitudes. A method is presented for obtaining a detailed, accurate profile over a large altitude range. The method utilizes photogrammetric measurement of successive positions of the t rail of an ascending rocket. Construction of a rocketborne smoke generator is discussed and the photography, film reading, and data-reduction procedures are described. The accuracy of the method i s examined and error equations are derived. Finally, some examples of wind profiles measured by this method are shown, and their implications for missile response are indicated."
A Method of Poisson's Ration Imaging Within a Material Part,45.33673,computed photo method,['Numerical Analysis'],"The present invention is directed to a method of displaying the Poisson's ratio image of a material part. In the present invention, longitudinal data is produced using a longitudinal wave transducer and shear wave data is produced using a shear wave transducer. The respective data is then used to calculate the Poisson's ratio for the entire material part. The Poisson's ratio approximations are then used to display the data."
"Analytical study to define a helicopter stability derivative extraction method, volume 1",45.167763,computed photo method,['AIRCRAFT'],"A method is developed for extracting six degree-of-freedom stability and control derivatives from helicopter flight data. Different combinations of filtering and derivative estimate are investigated and used with a Bayesian approach for derivative identification. The combination of filtering and estimate found to yield the most accurate time response match to flight test data is determined and applied to CH-53A and CH-54B flight data. The method found to be most accurate consists of (1) filtering flight test data with a digital filter, followed by an extended Kalman filter (2) identifying a derivative estimate with a least square estimator, and (3) obtaining derivatives with the Bayesian derivative extraction method."
Autogenic-Feedback Training Exercise (AFTE) Method and System,44.978737,computed photo method,['Man/System Technology and Life Support'],"The Autogenic-Feedback Training Exercise (AFTE) method of the present invention is a combined application of physiologic and perceptual training techniques. such as autogenic therapy and biofeedback. This combined therapy approach produces a methodology that is appreciably more effective than either of the individual techniques used separately. The AFTE method enables sufficient magnitude of control necessary to significantly reduce the behavioral and physiologic reactions to severe environmental stressors. It produces learned effects that are persistent over time and are resistant to extinction and it can be administered in a short period of time. The AFTE method may be used efficiently in several applications, among which are the following: to improve pilot and crew performance during emergency flying conditions; to train people to prevent the occurrence of nausea and vomiting associated with motion and sea sickness, or morning sickness in early pregnancy; as a training method for preventing or counteracting air-sickness symptoms in high-performance military aircraft; for use as a method for cardiovascular training, as well as for multiple other autonomic responses, which may contribute to the alleviation of Space Motion Sickness (SMS) in astronauts and cosmonauts; training people suffering from migraine or tension headaches to control peripheral blood flow and reduce forehead and/or trapezius muscle tension; training elderly people suffering from fecal incontinence to control their sphincter muscles; training cancer patients to reduce the nauseagenic effects of chemotherapy; and training patients with Chronic Intestinal Pseudo-obstruction (CIP)."
Effect of radiometric errors on accuracy of temperature-profile measurement by the spectral-scanning method.,44.933853,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"The spectral-scanning method may be used to determine the temperature profile of a jet- or rocket-engine exhaust stream by measurements of gas radiation and transmittance at two or more wavelengths. A single, fixed line of sight is used, using immobile radiators and radiometers outside of the gas stream, and there is no interference with the flow. A given radiometric error causes an error in computed temperatures. The ratio between temperature error and radiometric error depends on profile shape, path length, temperature level, the strength of line absorption, and the absorption coefficient and its temperature coefficient. These influence the choice of wavelengths for any given gas. Conditions for minimum temperature error are derived. Numerical results are presented for a two wavelength measurement on a family of profiles that may be expected in a practical case of H2 - O2 combustion. Under favorable conditions, the fractional error in temperature approximates the fractional error in radiant-flux measurement."
Optics-Only Calibration of a Neural-Net Based Optical NDE Method for Structural Health Monitoring,44.92386,computed photo method,['Instrumentation and Photography'],"A calibration process is presented that uses optical measurements alone to calibrate a neural-net based NDE method. The method itself detects small changes in the vibration mode shapes of structures. The optics-only calibration process confirms previous work that the sensitivity to vibration-amplitude changes can be as small as 10 nanometers. A more practical value in an NDE service laboratory is shown to be 50 nanometers. Both model-generated and experimental calibrations are demonstrated using two implementations of the calibration technique. The implementations are based on previously published demonstrations of the NDE method and an alternative calibration procedure that depends on comparing neural-net and point sensor measurements. The optics-only calibration method, unlike the alternative method, does not require modifications of the structure being tested or the creation of calibration objects. The calibration process can be used to test improvements in the NDE process and to develop a vibration-mode-independence of damagedetection sensitivity. The calibration effort was intended to support NASA s objective to promote safety in the operations of ground test facilities or aviation safety, in general, by allowing the detection of the gradual onset of structural changes and damage."
Hook-method measurements of gf-values for ultraviolet Fe I and Fe II lines on a shock tube,44.6817,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"Transition probabilities for 14 lines of Fe II and 12 lines of Fe I in the wavelength region 2560-2737 A were measured by use of a shock tube and the hook method. Absolute oscillator strengths for resonance lines of Fe I reported by Banfield and Huber were used to determine the number density of neutral iron in the shock-heated gas. With the assumption of thermal equilibrium, the density of singly ionized iron atoms in this gas was then computed from the measured temperature and pressure with the aid of the Saha equation. Our results on the 12 strongest of the 13 lines belonging to the first ultraviolet multiplet of Fe II indicate that the multiplet f-value is larger by a factor of 2 than that derived from lifetime measurements by Assousa and Smith."
Analysis of Experimental Investigations of the Planing Process of the Surface of Water,44.58955,computed photo method,"['Aircraft Design, Testing and Performance']","Pressure distribution and spray measurements were carried out on rectangular flat and V-bottom planing surfaces. Lift, resistance, and center of pressure data are analyzed and it is shown how these values may be computed for the pure planing procees of a flat or V-bottom suface of arbitrary beam, load and speed, the method being illustrated with the aid of an example."
A Laboratory Method for Assessing Audibility and Localization of Rotorcraft Fly-In Noise,44.50723,computed photo method,['Acoustics'],"The low frequency content of rotorcraft noise allows it to be heard over great distances. This factor contributes to the disruption of natural quiet in national parks and wilderness areas, and can lead to annoyance in populated areas. Further, it can result in the sound being heard at greater distances compared to higher altitude fixed wing aircraft operations. Human response studies conducted in the field are challenging since test conditions are difficult to control. This paper presents a means of quantitatively determining the audibility and localization of rotorcraft fly-in noise, under a specified ambient noise condition, within a controlled laboratory environment. It is demonstrated using synthetic fly-in noise of a light utility helicopter. The method is shown to resolve differences in audibility distances due to different ground impedances, propagation modeling methods, and directivity angles. Further, it demonstrates the efficacy of an accelerated test method."
Plant chlorophyll content meter,44.397926,computed photo method,['Mechanical Engineering'],"A plant chlorophyll content meter is described which collects light reflected from a target plant and separates the collected light into two different wavelength bands. These wavelength bands, or channels, are described as having center wavelengths of 700 nm and 840 nm. The light collected in these two channels are processed using photo detectors and amplifiers. An analog to digital converter is described which provides a digital representation of the level of light collected by the lens and falling within the two channels. A controller provided in the meter device compares the level of light reflected from a target plant with a level of light detected from a light source, such as light reflected by a target having 100% reflectance, or transmitted through a diffusion receptor. The percent of reflection in the two separate wavelength bands from a target plant are compared to provide a ratio which indicates a relative level of plant physiological stress. A method of compensating for electronic drift is described where a sample is taken when a collection lens is covered to prevent light from entering the device. This compensation method allows for a more accurate reading by reducing error contributions due to electronic drift from environmental conditions at the location where a hand-held unit is used."
Method and Apparatus for Measuring Near-Angle Scattering of Mirror Coatings,44.29941,computed photo method,['Optics'],"Disclosed herein is a method of determining the near angle scattering of a sample reflective surface comprising the steps of: a) splitting a beam of light having a coherence length of greater than or equal to about 2 meters into a sample beam and a reference beam; b) frequency shifting both the sample beam and the reference beam to produce a fixed beat frequency between the sample beam and the reference beam; c) directing the sample beam through a focusing lens and onto the sample reflective surface, d) reflecting the sample beam from the sample reflective surface through a detection restriction disposed on a movable stage; e) recombining the sample beam with the reference beam to form a recombined beam, followed by f) directing the recombined beam to a detector and performing heterodyne analysis on the recombined beam to measure the near-angle scattering of the sample reflective surface, wherein the position of the detection restriction relative to the sample beam is varied to occlude at least a portion of the sample beam to measure the near-angle scattering of the sample reflective surface. An apparatus according to the above method is also disclosed."
System and method for image mapping and visual attention,44.088272,computed photo method,['Instrumentation and Photography'],"A method is described for mapping dense sensory data to a Sensory Ego Sphere (SES). Methods are also described for finding and ranking areas of interest in the images that form a complete visual scene on an SES. Further, attentional processing of image data is best done by performing attentional processing on individual full-size images from the image sequence, mapping each attentional location to the nearest node, and then summing all attentional locations at each node."
System and method for image mapping and visual attention,44.080826,computed photo method,['Instrumentation and Photography'],"A method is described for mapping dense sensory data to a Sensory Ego Sphere (SES). Methods are also described for finding and ranking areas of interest in the images that form a complete visual scene on an SES. Further, attentional processing of image data is best done by performing attentional processing on individual full-size images from the image sequence, mapping each attentional location to the nearest node, and then summing attentional locations at each node."
Method and apparatus for precision control of radiometer,44.014503,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A radiometer controller of a radiation detector is provided with a calibration method and apparatus comprised of mounting all temperature sensitive elements of the controller in thermostatically controlled ovens during calibration and measurements, using a selected temperature that is above any which might be reached in the field. The instrument is calibrated in situ by adjusting heater power (EI) to the receptor cavity in the radiometer detector to a predetermined full scale level and is displayed by a meter."
Method for Reducing the Refresh Rate of Fiber Bragg Grating Sensors,43.94684,computed photo method,"['Instrumentation and Photography', 'Electronics and Electrical Engineering']","The invention provides a method of obtaining the FBG data in final form (transforming the raw data into frequency and location data) by taking the raw FBG sensor data and dividing the data into a plurality of segments over time. By transforming the raw data into a plurality of smaller segments, processing time is significantly decreased. Also, by defining the segments over time, only one processing step is required. By employing this method, the refresh rate of FBG sensor systems can be improved from about 1 scan per second to over 20 scans per second."
Structural Anomaly Detection Using Fiber Optic Sensors and Inverse Finite Element Method,43.910637,computed photo method,['Structural Mechanics'],NASA Langley Research Center is investigating a variety of techniques for mitigating aircraft accidents due to structural component failure. One technique under consideration combines distributed fiber optic strain sensing with an inverse finite element method for detecting and characterizing structural anomalies anomalies that may provide early indication of airframe structure degradation. The technique identifies structural anomalies that result in observable changes in localized strain but do not impact the overall surface shape. Surface shape information is provided by an Inverse Finite Element Method that computes full-field displacements and internal loads using strain data from in-situ fiberoptic sensors. This paper describes a prototype of such a system and reports results from a series of laboratory tests conducted on a test coupon subjected to increasing levels of damage.
A decoding procedure for the Reed-Solomon codes,43.909584,computed photo method,['COMPUTER SYSTEMS'],"A decoding procedure is described for the (n,k) t-error-correcting Reed-Solomon (RS) code, and an implementation of the (31,15) RS code for the I4-TENEX central system. This code can be used for error correction in large archival memory systems. The principal features of the decoder are a Galois field arithmetic unit implemented by microprogramming a microprocessor, and syndrome calculation by using the g(x) encoding shift register. Complete decoding of the (31,15) code is expected to take less than 500 microsecs. The syndrome calculation is performed by hardware using the encoding shift register and a modified Chien search. The error location polynomial is computed by using Lin's table, which is an interpretation of Berlekamp's iterative algorithm. The error location numbers are calculated by using the Chien search. Finally, the error values are computed by using Forney's method."
Flight Test Results of a GPS-Based Pitot-Static Calibration Method Using Output-Error Optimization for a Light Twin-Engine Airplane,43.90788,computed photo method,['Air Transportation and Safety'],"As part of the NASA Aviation Safety Program (AvSP), a novel pitot-static calibration method was developed to allow rapid in-flight calibration for subscale aircraft while flying within confined test areas. This approach uses Global Positioning System (GPS) technology coupled with modern system identification methods that rapidly computes optimal pressure error models over a range of airspeed with defined confidence bounds. This method has been demonstrated in subscale flight tests and has shown small 2- error bounds with significant reduction in test time compared to other methods. The current research was motivated by the desire to further evaluate and develop this method for full-scale aircraft. A goal of this research was to develop an accurate calibration method that enables reductions in test equipment and flight time, thus reducing costs. The approach involved analysis of data acquisition requirements, development of efficient flight patterns, and analysis of pressure error models based on system identification methods. Flight tests were conducted at The University of Tennessee Space Institute (UTSI) utilizing an instrumented Piper Navajo research aircraft. In addition, the UTSI engineering flight simulator was used to investigate test maneuver requirements and handling qualities issues associated with this technique. This paper provides a summary of piloted simulation and flight test results that illustrates the performance and capabilities of the NASA calibration method. Discussion of maneuver requirements and data analysis methods is included as well as recommendations for piloting technique."
A Method of Calibrating Airspeed Installations on Airplanes at Transonic and Supersonic Speeds by the Use of Accelerometer and Attitude-Angle Measurements,43.598763,computed photo method,['Air Transportation and Safety'],A method is described for calibrating airspeed installation on airplanes at transonic and supersonic speeds in vertical-plane maneuvers in which use is made of measurements of normal and longitudinal accelerations and attitude angle. In this method all the required instrumentation is carried within the airplane. An analytical study of the effects of various sources of error on the accuracy of an airspeed calibration by the accelerometer method indicated that the required measurements can be made accurately enough to insure a satisfactory calibration.
Description of an analytic method for the determination of spacecraft window-induced navigation sighting errors,43.489815,computed photo method,['NAVIGATION'],Analytical method for determination of window induced navigation instrument errors in Gemini spacecraft
Heterojunction solar cell calculations,43.46627,computed photo method,['AUXILIARY SYSTEMS'],Solar cell efficiencies computed for semiconductor heterojunction cells
Calculation Method for Predicting AM0 Isc from High Altitude Aircraft Flight Data,43.404976,computed photo method,"['Aircraft Propulsion and Power', 'Avionics and Aircraft Instrumentation']","High altitude aircraft have been used by the space photovoltaic (PV) community to determine the air mass Zero (AM0) performance of solar cells for over fifty years. Relative to in-space measurement opportunities, these methods are generally cheaper and more readily available. The data obtained, however, must be corrected for residual atmospheric effects. This paper details the correction method currently being used for the calculation of the estimated AM0 short-circuit current (Isc) for photovoltaic devices flown on the NASA ER-2 calibration platform. This method would also be applicable to other high altitude methods where Isc data is collected over a sufficiently large range of altitudes. An initial comparison with a four junction (4J) cell flown on the CASOLBA high altitude balloon platform showed an agreement to 0.2%."
Liquid-filled transient pressure measuring systems:  A method for determining frequency response,43.30012,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],An equation is given and experimentally verified for computing the resonant frequency of liquid-filled transient pressure measuring systems. Resonant frequencies of 100 to 1000 Hz are typical of those systems tested. The effect of noncondensable gas bubbles on system response is described. A method for determining transducer volumetric compliance is presented. An example system is described and analyzed to demonstrate the use of the theory.
A nonintrusive laser interferometer method for measurement of skin friction,43.29841,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A method is described for monitoring the changing thickness of a thin oil film subject to an aerodynamic shear stress using two focused laser beams. The measurement is then simply analyzed in terms of the surface skin friction of the flow. The analysis includes the effects of arbitrarily large pressure and skin friction gradients, gravity, and time varying oil temperature. It may also be applied to three dimensional flows with unknown direction. Applications are presented for a variety of flows including two dimensional flows, three dimensional swirling flows, separated flow, supersonic high Reynolds number flows, and delta wing vortical flows."
PRELIMINARY ANALYSIS OF FEASIBILITY OF MAPPING THE MOON BY MEANS OF PHOTOGRAMMETRY AND PHOTO INTERPRETATION PART II. PHOTOGRAPHIC INTERPRETATION,43.123096,computed photo method,['BIOTECHNOLOGY'],"A method is described for monitoring the changing thickness of a thin oil film subject to an aerodynamic shear stress using two focused laser beams. The measurement is then simply analyzed in terms of the surface skin friction of the flow. The analysis includes the effects of arbitrarily large pressure and skin friction gradients, gravity, and time varying oil temperature. It may also be applied to three dimensional flows with unknown direction. Applications are presented for a variety of flows including two dimensional flows, three dimensional swirling flows, separated flow, supersonic high Reynolds number flows, and delta wing vortical flows."
Stresses in a quasi-isotropic pin loaded connector using photoelasticity,43.080154,computed photo method,['STRUCTURAL MECHANICS'],"Birefringent glass-epoxy and a numerical stress separation scheme are used to compute the stresses in the vicinity of a pin-loaded hole. The radial and circumferential stresses at the hole edge, and the net section and shear-out stresses are computed. The numerical and experimental results are compared with the computed stresses. The fixture used to load the connector is discussed and typical isochromatic and isoclinic fringe patterns are presented. The stress-separation scheme is briefly discussed."
LUMIS:  A Land Use Management Information System for urban planning,42.99491,computed photo method,['EARTH RESOURCES AND REMOTE SENSING'],"The Land Use Management Information System (LUMIS) consists of a methodology of compiling land use maps by means of air photo interpretation techniques, digitizing these and other maps into machine-readable form, and numerically overlaying these various maps in two computer software routines to provide land use and natural resource data files referenced to the individual census block. The two computer routines are the Polygon Intersection Overlay System (PIOS) and an interactive graphics APL program. A block referenced file of land use, natural resources, geology, elevation, slope, and fault-line items has been created and supplied to the Los Angeles Department of City Planning for the City's portion of the Santa Monica Mountains. In addition, the interactive system contains one hundred and seventy-three socio-economic data items created by merging the Third Count U.S. Census Bureau tapes and the Los Angeles County Secured Assessor File. This data can be graphically displayed for each and every block, block group, or tract for six test tracts in Woodland Hills, California. Other benefits of LUMIS are the knowledge of air photo availability, flight pattern coverage and frequencies, and private photogrammetry companies flying Southern California, as well as a formal Delphi study of relevant land use informational needs in the Santa Monicas."
A Method to Measure and Estimate Normalized Contrast in Infrared Flash Thermography,42.954277,computed photo method,['Instrumentation and Photography'],"The paper presents further development in normalized contrast processing used in flash infrared thermography method. Method of computing normalized image or pixel intensity contrast, and normalized temperature contrast are provided. Methods of converting image contrast to temperature contrast and vice versa are provided. Normalized contrast processing in flash thermography is useful in quantitative analysis of flash thermography data including flaw characterization and comparison of experimental results with simulation. Computation of normalized temperature contrast involves use of flash thermography data acquisition set-up with high reflectivity foil and high emissivity tape such that the foil, tape and test object are imaged simultaneously. Methods of assessing other quantitative parameters such as emissivity of object, afterglow heat flux, reflection temperature change and surface temperature during flash thermography are also provided. Temperature imaging and normalized temperature contrast processing provide certain advantages over normalized image contrast processing by reducing effect of reflected energy in images and measurements, therefore providing better quantitative data. Examples of incorporating afterglow heat-flux and reflection temperature evolution in flash thermography simulation are also discussed."
Laser Doppler spectrometer method of particle sizing,42.599056,computed photo method,['INSTRUMENTATION AND PHOTOGRAPHY'],"A spectrometer for the detection of airborne particulate pollution in the submicron size range is described. In this device, airborne particles are accelerated through a supersonic nozzle, with different sizes achieving different velocities in the gas flow. Information about the velocities of the accelerated particles is obtained with a laser-heterodyne optical system through the Doppler shift of light scattered from the particles. Detection is accomplished by means of a photomultiplier. Nozzle design and signal processing techniques are also discussed."
Altitude determination and descriptive analysis of clouds on ERTS-1 multispectral photography,42.56613,computed photo method,['GEOPHYSICS'],"A simple method to determine the approximate altitude of clouds is described, with the objective of refining their classification using only marginal data from the photographs. Results of the application of this method on photographs of the Goajira Peninsula, Paraguana Peninsula and the Central Coast of Venezuela are presented. Here, the altitudes computed are used to classify clouds and to identify the genus of others without typical form. Instability of air masses through clouds vertical development, and wind direction as well as other local climatic characteristics such as moisture content, loci of condensation, area, etc. are determined using repetitive coverage for the time interval of the photography. Applications for the regional and urban planning (including airport location and flights schedule) and natural resources evaluation are suggested."
"Non-invasive assessment of otolith formation during development of the Japanese red-bellied newt, Cynops pyrrhogaster",42.45397,computed photo method,['Life Sciences (General)'],"Pre-mated adult female newts and embryos have been flown on the International Microgravity Laboratory-2 (IML-2) Space Shuttle flight in 1994 (Wiederhold et al., 1992b). With the specimens available from this flight, the calcification of otoliths, ulna, radius and backbone of the flown larvae and adult newts were analyzed. The experiments presented here studied the development of the otoliths on the ground. Otoliths of living newts, from embryo to adult, were observed in situ with the application of a new X-ray and bio-imaging analyzer system. For the establishment of this method, newts at different developmental stages were used. An imaging plate temporarily stores the X-ray energy pattern at the bio-imaging analyzer. A latent image on the imaging plate was transformed into a digital time series signal with an image reader. Acquired digital information was computed with the image processor. The processed information was recorded on film with an image recorder, in order to visualize it on an enlargement computed radiograph. To analyze development of the otoliths, photo-stimulated luminescence level was detected by an image analyzer, using transmitted X-ray photons. A single clump of otoconia could first be seen at stage 33. Stage-36 embryos first have distinguishable otoliths, with the utricle in front and saccule behind. Our results show that this X-ray method detects the otoliths equally as well as sectioning. In the newt, the mandibular/maxillary bone formed before the spine. It is suspected that for the newt embryo, living in water, feeding becomes necessary prior to support of the body."
"FIP, FIT or MAD? Analysis of High Signal-to-Noise ASCA Spectra of Coronal Stars",42.382343,computed photo method,['Space Radiation'],"ASCA (Advanced Satellite for Cosmology and Astrophysics) and EUVE (Extreme Ultraviolet Explorer) spectra of active late-type stars imply that Fe and other medium-Z elements may be 2-10 times less abundant in the coronae of these stars than in their photo-spheres (the MAD effect). These deficiencies may be related to the solar FIP (First Ionization Potential) effect, in which Fe and other low First Ionization Potential elements appear enriched in the solar corona over their photospheric values. The FIP effect is time variable. As part of this proposal, the K0-2 III star, 29 Draconis, was observed in X rays with the ASCA spacecraft in order to measure the coronal abundances of this star at three different stellar longitudes over its 31-day rotation cycle. The goal of the observations was to learn whether coronal abundances, and hence coronal magnetic structure, vary across the surface of 29 Draconis in phase with the motion of dark star-spots across its disk. A second task included in this project was a systematic reanalysis of 18-20 deep exposures of active coronal stars, which were extracted from the ASCA public archives. New thermal models were computed for each spectrum in order to derive coronal metal abundances for each star. The goal of this survey was to search for possible trends in coronal abundance with various stellar parameters such as rotation, chromospheric activity levels at ultraviolet and optical wavelengths, or evolutionary stage."
Reducing Centroid Error Through Model-Based Noise Reduction,42.29737,computed photo method,['Instrumentation and Photography'],"A method of processing the digitized output of a charge-coupled device (CCD) image detector has been devised to enable reduction of the error in computed centroid of the image of a point source of light. The method involves model-based estimation of, and correction for, the contributions of bias and noise to the image data. The method could be used to advantage in any of a variety of applications in which there are requirements for measuring precise locations of, and/or precisely aiming optical instruments toward, point light sources. In the present method, prior to normal operations of the CCD, one measures the point-spread function (PSF) of the telescope or other optical system used to project images on the CCD. The PSF is used to construct a database of spot models representing the nominal CCD pixel outputs for a point light source projected onto the CCD at various positions incremented by small fractions of a pixel."
Background-Oriented Schlieren for Large-Scale and High-Speed Aerodynamic Phenomena,42.19029,computed photo method,['Instrumentation and Photography'],"Visualization of the flow field around a generic re-entry capsule in subsonic flow and shock wave visualization with cylindrical explosives have been conducted to demonstrate sensitivity and applicability of background-oriented schlieren (BOS) for field experiments. The wind tunnel experiment suggests that BOS with a fine-pixel imaging device has a density change detection sensitivity on the order of 10(sup -5) in subsonic flow. In a laboratory setup, the structure of the shock waves generated by explosives have been successfully reconstructed by a computed tomography method combined with BOS. "
Tunneling calculations for GaAs-Al(x)Ga(1-x) as graded band-gap sawtooth superlattices,41.81715,computed photo method,['SOLID-STATE PHYSICS'],"Quantum mechanical tunneling calculations for sawtooth (linearly graded band-gap) and step-barrier AlGaAs superlattices were performed by means of a transfer matrix method, within the effective mass approximation. The transmission coefficient and tunneling current versus applied voltage were computed for several representative structures. Particular consideration was given to effective mass variations. The tunneling properties of step and sawtooth superlattices show some qualitative similarities. Both structures exhibit resonant tunneling, however, because they deform differently under applied fields, the J-V curves differ."
Design and Evaluation of a New Boundary-Layer Rake for Flight Testing,41.631226,computed photo method,['Aerodynamics'],"A new boundary-layer rake has been designed and built for flight testing on the NASA Dryden Flight Research Center F-15B/Flight Test Fixture. A feature unique to this rake is its curved body, which allows pitot tubes to be more densely clustered in the near-wall region than conventional rakes allow. This curved rake design has a complex three-dimensional shape that requires innovative solid-modeling and machining techniques. Finite-element stress analysis of the new design shows high factors of safety. The rake has passed a ground test in which random vibration measuring 12 g rms was applied for 20 min in each of the three normal directions. Aerodynamic evaluation of the rake has been conducted in the NASA Glenn Research Center 8 x 6 Supersonic Wind Tunnel at Mach 0-2. The pitot pressures from the new rake agree with conventional rake data over the range of Mach numbers tested. The boundary-layer profiles computed from the rake data have been shown to have the standard logarithmic-law profile. Skin friction values computed from the rake data using the Clauser plot method agree with the Preston tube results and the van Driest II compressible skin friction correlation to approximately +/-5 percent."
Improved CPAS Photogrammetric Capabilities for Engineering Development Unit (EDU) Testing,41.610344,computed photo method,['Instrumentation and Photography'],"This paper focuses on two key improvements to the photogrammetric analysis capabilities of the Capsule Parachute Assembly System (CPAS) for the Orion vehicle. The Engineering Development Unit (EDU) system deploys Drogue and Pilot parachutes via mortar, where an important metric is the muzzle velocity. This can be estimated using a high speed camera pointed along the mortar trajectory. The distance to the camera is computed from the apparent size of features of known dimension. This method was validated with a ground test and compares favorably with simulations. The second major photogrammetric product is measuring the geometry of the Main parachute cluster during steady-state descent using onboard cameras. This is challenging as the current test vehicles are suspended by a single-point attachment unlike earlier stable platforms suspended under a confluence fitting. The mathematical modeling of fly-out angles and projected areas has undergone significant revision. As the test program continues, several lessons were learned about optimizing the camera usage, installation, and settings to obtain the highest quality imagery possible."
Color camera computed tomography imaging spectrometer for improved spatial-spectral image accuracy,120.32509,computed image estimate,['Instrumentation and Photography'],"Computed tomography imaging spectrometers (""CTIS""s) having color focal plane array detectors are provided. The color FPA detector may comprise a digital color camera including a digital image sensor, such as a Foveon X3.RTM. digital image sensor or a Bayer color filter mosaic. In another embodiment, the CTIS includes a pattern imposed either directly on the object scene being imaged or at the field stop aperture. The use of a color FPA detector and the pattern improves the accuracy of the captured spatial and spectral information."
The Effect of Experimental Variables on Industrial X-Ray Micro-Computed Sensitivity,114.16789,computed image estimate,['Quality Assurance and Reliability'],"A study was performed on the effect of experimental variables on radiographic sensitivity (image quality) in x-ray micro-computed tomography images for a high density thin wall metallic cylinder containing micro-EDM holes. Image quality was evaluated in terms of signal-to-noise ratio, flaw detectability, and feature sharpness. The variables included: day-to-day reproducibility, current, integration time, voltage, filtering, number of frame averages, number of projection views, beam width, effective object radius, binning, orientation of sample, acquisition angle range (180deg to 360deg), and directional versus transmission tube. "
Image analysis of particle field by means of computed tomography,109.41878,computed image estimate,['FLUID MECHANICS AND HEAT TRANSFER'],"In order to visualize and investigate spray structures, computed tomography technique is applied to analyze droplet information. From the transmitted light intensity through the spray and/or the data of particle size distribution obtained from a Fraunhofer diffraction principle, the quantitative volume of spray droplet or local particle size was calculated and the reconstruction of spray structures was made. The background of computed tomography is described along with some experimental results of the structure of intermittent spray such as diesel spray."
Terahertz Computed Tomography of NASA Thermal Protection System Materials,106.29831,computed image estimate,['Quality Assurance and Reliability'],"A terahertz axial computed tomography system has been developed that uses time domain measurements in order to form cross-sectional image slices and three-dimensional volume renderings of terahertz-transparent materials. The system can inspect samples as large as 0.0283 cubic meters (1 cubic foot) with no safety concerns as for x-ray computed tomography. In this study, the system is evaluated for its ability to detect and characterize flat bottom holes, drilled holes, and embedded voids in foam materials utilized as thermal protection on the external fuel tanks for the Space Shuttle. X-ray micro-computed tomography was also performed on the samples to compare against the terahertz computed tomography results and better define embedded voids. Limits of detectability based on depth and size for the samples used in this study are loosely defined. Image sharpness and morphology characterization ability for terahertz computed tomography are qualitatively described."
Spatial image modulation to improve performance of computed tomography imaging spectrometer,103.47683,computed image estimate,['Instrumentation and Photography'],"Computed tomography imaging spectrometers (""CTIS""s) having patterns for imposing spatial structure are provided. The pattern may be imposed either directly on the object scene being imaged or at the field stop aperture. The use of the pattern improves the accuracy of the captured spatial and spectral information."
Image Registration Workshop Proceedings,98.40435,computed image estimate,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Quantile Data Analysis of Image Data,96.029305,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Quantile data analysis and functional statistical inference methods are introduced and applied to provide representations of spectral data which may lead to simple statistical discriminators effective for the estimation of ground truth from satellite spectral measurements. To estimate the ground truth of a pixel, the probability of each possible ground truth is estimated, given observed (estimated) quantile theoretic statistical characteristics of the multispectral image data corresponding to the pixel and its neighboring pixels. A strategy for determining which statistical characteristics discriminate best is described. Results are reported of quantile data analysis of an extensive collection of training files of image data."
Introduction to Remote Sensing Image Registration,93.52016,computed image estimate,"['Earth Resources and Remote Sensing', 'Instrumentation and Photography']","For many applications, accurate and fast image registration of large amounts of multi-source data is the first necessary step before subsequent processing and integration. Image registration is defined by several steps and each step can be approached by various methods which all present diverse advantages and drawbacks depending on the type of data, the type of applications, the a priori information known about the data and the type of accuracy that is required. This paper will first present a general overview of remote sensing image registration and then will go over a few specific methods and their applications."
Avoiding Stair-Step Artifacts in Image Registration for GOES-R Navigation and Registration Assessment,93.17319,computed image estimate,"['Astronautics (General)', 'Earth Resources and Remote Sensing', 'Mathematical and Computer Sciences (General)']","In developing software for independent verification and validation (IVV) of the Image Navigation and Registration (INR) capability for the Geostationary Operational Environmental Satellite R Series (GOES-R) Advanced Baseline Imager (ABI), we have encountered an image registration artifact which limits the accuracy of image offset estimation at the subpixel scale using image correlation. Where the two images to be registered have the same pixel size, subpixel image registration preferentially selects registration values where the image pixel boundaries are close to lined up. Because of the shape of a curve plotting input displacement to estimated offset, we call this a stair-step artifact. When one image is at a higher resolution than the other, the stair-step artifact is minimized by correlating at the higher resolution. For validating ABI image navigation, GOES-R images are correlated with Landsat-based ground truth maps. To create the ground truth map, the Landsat image is first transformed to the perspective seen from the GOES-R satellite, and then is scaled to an appropriate pixel size. Minimizing processing time motivates choosing the map pixels to be the same size as the GOES-R pixels. At this pixel size image processing of the shift estimate is efficient, but the stair-step artifact is present. If the map pixel is very small, stair-step is not a problem, but image correlation is computation-intensive. This paper describes simulation-based selection of the scale for truth maps for registering GOES-R ABI images."
Model based estimation of image depth and displacement,91.27774,computed image estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Passive depth and displacement map determinations have become an important part of computer vision processing. Applications that make use of this type of information include autonomous navigation, robotic assembly, image sequence compression, structure identification, and 3-D motion estimation. With the reliance of such systems on visual image characteristics, a need to overcome image degradations, such as random image-capture noise, motion, and quantization effects, is clearly necessary. Many depth and displacement estimation algorithms also introduce additional distortions due to the gradient operations performed on the noisy intensity images. These degradations can limit the accuracy and reliability of the displacement or depth information extracted from such sequences. Recognizing the previously stated conditions, a new method to model and estimate a restored depth or displacement field is presented. Once a model has been established, the field can be filtered using currently established multidimensional algorithms. In particular, the reduced order model Kalman filter (ROMKF), which has been shown to be an effective tool in the reduction of image intensity distortions, was applied to the computed displacement fields. Results of the application of this model show significant improvements on the restored field. Previous attempts at restoring the depth or displacement fields assumed homogeneous characteristics which resulted in the smoothing of discontinuities. In these situations, edges were lost. An adaptive model parameter selection method is provided that maintains sharp edge boundaries in the restored field. This has been successfully applied to images representative of robotic scenarios. In order to accommodate image sequences, the standard 2-D ROMKF model is extended into 3-D by the incorporation of a deterministic component based on previously restored fields. The inclusion of past depth and displacement fields allows a means of incorporating the temporal information into the restoration process. A summary on the conditions that indicate which type of filtering should be applied to a field is provided."
Using X-Ray Computed Tomography to Image Apollo Drive Tube 73002,90.12245,computed image estimate,['Instrumentation and Photography'],"The Apollo missions collected 382 kg of rock, regolith, and core samples from six locations on the nearside of the Moon. Today, just over 84% by mass of the Apollo collection remains in pristine condition within the curation facility at Johnson Space Center. Most Apollo samples have been well characterized, however there are several types of samples that have remained wholly or largely unstudied since their return, and/or that have been curated under special conditions. These sample types are: (1) unopened samples sealed under vacuum on the Moon; (2) unopened (but unsealed) drive tubes; (3) Apollo 17 samples frozen shortly after their return; and (4) Apollo 15 samples opened and stored in a helium atmosphere since their return. NASA solicited proposals for the Apollo Next Generation Sample Analysis Program (ANGSA), and 9 teams were selected to study: (1) unsealed, unopened drive tube 73002; (2) sealed, unopened drive tube 73001 (paired with 73002); and (3) a subset of the frozen and He-purged samples [1]. 

The first sample opened as part of the ANGSA program was drive tube 73002. This was originally a ~30 cm long, 4 cm diameter drive tube collected on a landslide deposit near Lara Crater at the Apollo 17 landing site. It was part of a ~60 cm long double drive tube collected, and the bottom half of the tube (73001) was sealed under vacuum on the Moon [2]. Prior to opening sample 73002, the sample was imaged with a high resolution X-ray Computed Tomography (XCT) scan of the entire tube. Additional XCT scans have been made of large clasts removed from the core as part of the dissection process [3]. Here we present the whole tube and close-up XCT data from 73002, and talk about the utility of the scans as part of the curation process, including the potential for future science returns from the high resolutions scans.
"
Space shuttle main engine computed tomography applications,90.07468,computed image estimate,['SPACECRAFT PROPULSION AND POWER'],"For the past two years the potential applications of computed tomography to the fabrication and overhaul of the Space Shuttle Main Engine were evaluated. Application tests were performed at various government and manufacturer facilities with equipment produced by four different manufacturers. The hardware scanned varied in size and complexity from a small temperature sensor and turbine blades to an assembled heat exchanger and main injector oxidizer inlet manifold. The evaluation of capabilities included the ability to identify and locate internal flaws, measure the depth of surface cracks, measure wall thickness, compare manifold design contours to actual part contours, perform automatic dimensional inspections, generate 3D computer models of actual parts, and image the relationship of the details in a complex assembly. The capabilities evaluated, with the exception of measuring the depth of surface flaws, demonstrated the existing and potential ability to perform many beneficial Space Shuttle Main Engine applications."
Using X-Ray Computed Tomography to Image Apollo Drive Tube 73002,89.24055,computed image estimate,['Lunar and Planetary Science and Exploration']," The Apollo missions collected 382 kg of rock, regolith, and core samples from six locations on the nearside of the Moon. Today, just over 84% by mass of the Apollo collection remains in pristine condition within the curation facility at Johnson Space Center. Most Apollo samples have been well characterized, however there are several types of samples that have remained wholly or largely unstudied since their return, and/or that have been curated under special conditions. These sample types are: (1) unopened samples sealed under vacuum on the Moon; (2) unopened (but unsealed) drive tubes; (3) Apollo 17 samples frozen shortly after their return; and (4) Apollo 15 samples opened and stored in a helium atmosphere since their return. Last summer, NASA solicited proposals for the Apollo Next Generation Sample Analysis Program (ANGSA), and 9 teams were selected to study: (1) unsealed, unopened drive tube 73002; (2) sealed, unopened drive tube 73001 (paired with 73002); and (3) a subset of the frozen and He-purged samples [1]. The first sample opened as part of the ANGSA program was drive tube 73002. This is a 30 cm long, 4 cm diameter drive tube collected on a landslide deposit near Lara Crater at the Apollo 17 landing site. It was part of a 60 cm long double drive tube collected, and the bottom half of the tube (73001) was sealed under vacuum on the Moon [2]. Prior to opening sample 73002, the sample was imaged with a high resolution Xray Computed Tomography (XCT) scan of the entire tube. Additional XCT scans have been made of large clasts removed from the core as part of the dissection process [3]. Here we present a first look at the XCT data from 73002, and talk about the utility of the scans as part of the curation process, including the potential for future science returns from the high resolutions scans. "
State estimation and absolute image registration for geosynchronous satellites,88.465164,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Spacecraft state estimation and the absolute registration of Earth images acquired by cameras onboard geosynchronous satellites are described. The basic data type of the procedure consists of line and element numbers of image points called landmarks whose geodetic coordinates, relative to United States Geodetic Survey topographic maps, are known. A conventional least squares process is used to estimate navigational parameters and camera pointing biases from observed minus computed landmark line and element numbers. These estimated parameters along with orbit and attitude dynamic models are used to register images, using an automated grey level correlation technique, inside the span represented by the landmark data. In addition, the dynamic models can be employed to register images outside of the data span in a near real time mode. An important application of this mode is in support of meteorological studies where rapid data reduction is required for the rapid tracking and predicting of dynamic phenomena."
Improved image decompression for reduced transform coding artifacts,86.55845,computed image estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
A comparison of two position estimate algorithms that use ILS localizer and DME information.  Simulation and flight test results,86.47201,computed image estimate,['AIRCRAFT COMMUNICATIONS AND NAVIGATION'],"Simulation and flight tests were conducted to compare the accuracy of two algorithms designed to compute a position estimate with an airborne navigation computer. Both algorithms used ILS localizer and DME radio signals to compute a position difference vector to be used as an input to the navigation computer position estimate filter. The results of these tests show that the position estimate accuracy and response to artificially induced errors are improved when the position estimate is computed by an algorithm that geometrically combines DME and ILS localizer information to form a single component of error rather than by an algorithm that produces two independent components of error, one from a DMD input and the other from the ILS localizer input."
Comparison of Computed and Measured Vortex Evolution for a UH-60A Rotor in Forward Flight,86.27084,computed image estimate,"['Fluid Mechanics and Thermodynamics', 'Aircraft Design, Testing and Performance']","A Computational Fluid Dynamics (CFD) simulation using the Navier-Stokes equations was performed to determine the evolutionary and dynamical characteristics of the vortex flowfield for a highly flexible aeroelastic UH-60A rotor in forward flight. The experimental wake data were acquired using Particle Image Velocimetry (PIV) during a test of the fullscale UH-60A rotor in the National Full-Scale Aerodynamics Complex 40- by 80-Foot Wind Tunnel. The PIV measurements were made in a stationary cross-flow plane at 90 deg rotor azimuth. The CFD simulation was performed using the OVERFLOW CFD solver loosely coupled with the rotorcraft comprehensive code CAMRAD II. Characteristics of vortices captured in the PIV plane from different blades are compared with CFD calculations. The blade airloads were calculated using two different turbulence models. A limited spatial, temporal, and CFD/comprehensive-code coupling sensitivity analysis was performed in order to verify the unsteady helicopter simulations with a moving rotor grid system."
BOREAS TE-18 Biomass Density Image of the SSA,84.56587,computed image estimate,['Earth Resources and Remote Sensing'],"The BOREAS TE-18 team focused its efforts on using remotely sensed data to characterize the successional and disturbance dynamics of the boreal forest for use in carbon modeling. This biomass density image covers almost the entire BOREAS SSA. The pixels for which biomass density is computed include areas, that are in conifer land cover classes only. The biomass density values represent the amount of overstory biomass (i.e., tree biomass only) per unit area. It is derived from a Landsat-5 TM image collected on 02-Sep-1994. The technique that was used to create this image is very similar to the technique that was as used to create the physical classification of the SSA. The data are provided in a binary image file format. The data files are available on a CD-ROM (see document number 20010000884), or from the Oak Ridge National Laboratory (ORNL) Distributed Activity Archive Center (DAAC)."
"Denoising with Three Dimensional Fourier Transform for Three Dimensional Images, Including Image Sequences",84.493965,computed image estimate,['Instrumentation and Photography'],"A method of mitigating noise in source image data representing pixels of a 3-D image. The ""3-D image"" may be any type of 3-D image, regardless of whether the third dimension is spatial, temporal, or some other parameter. The 3-D image is divided into three-dimensional chunks of pixels. These chunks are apodized and a three-dimensional Fourier transform is performed on each chunk, thereby producing a three-dimensional spectrum of each chunk. The transformed chunks are processed to estimate a noise floor based on spectral values of the pixels within each chunk. A noise threshold is then determined, and the spectrum of each chunk is filtered with a denoising filter based on the noise threshold. The chunks are then inverse transformed, and recombined into a denoised 3-D image."
Image Matching Using Generalized Hough Transforms,84.21829,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"An image matching system specifically designed to match dissimilar images is described. A set of blobs and ribbons is first extracted from each image, and then generalized Hough transform techniques are used to match these sets and compute the transformation that best registers the image. An example of the application of the approach to one pair of remotely sensed images is presented."
"Proceedings of the Image Intensifier Symposium, October 24-26, 1961, Fort Belvoir, VA",80.74459,computed image estimate,['NAVIGATION'],Proceedings of second image intensifier symposium
Images constructed from computed flow fields,80.66925,computed image estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],"A method for constructing interferograms, schlieren, and shadowgraphs from ideal- and real-gas, two- and three-dimensional computed flow fields is described. The computational grids can be structured or unstructured, and multiple grids are an option. The constructed images are compared to experimental images for several types of flow, including a ramp, a blunt-body, a nozzle, and a reacting flow. The constructed images simulate the features observed in the experimental images. They are sensitive to errors in the flow-field solutions and can be used to identify solution errors. In addition, techniques for obtaining phase shifts from experimental finite-fringe interferograms and for removing experimentally induced phase-shift errors are discussed. Both the constructed images and calculated phase shifts can be used for validation of computational fluid dynamics (CFD) codes."
Introduction to computer image processing,80.60274,computed image estimate,['COMPUTERS'],"Theoretical backgrounds and digital techniques for a class of image processing problems are presented. Image formation in the context of linear system theory, image evaluation, noise characteristics, mathematical operations on image and their implementation are discussed. Various techniques for image restoration and image enhancement are presented. Methods for object extraction and the problem of pictorial pattern recognition and classification are discussed."
Guide to Magellan image interpretation,79.88543,computed image estimate,['LUNAR AND PLANETARY EXPLORATION'],"An overview of Magellan Mission requirements, radar system characteristics, and methods of data collection is followed by a description of the image data, mosaic formats, areal coverage, resolution, and pixel DN-to-dB conversion. The availability and sources of image data are outlined. Applications of the altimeter data to estimate relief, Fresnel reflectivity, and surface slope, and the radiometer data to derive microwave emissivity are summarized and illustrated in conjunction with corresponding SAR image data. Same-side and opposite-side stereo images provide examples of parallax differences from which to measure relief with a lateral resolution many times greater than that of the altimeter. Basic radar interactions with geologic surfaces are discussed with respect to radar-imaging geometry, surface roughness, backscatter modeling, and dielectric constant. Techniques are described for interpreting the geomorphology and surface properties of surficial features, impact craters, tectonically deformed terrain, and volcanic landforms. The morphologic characteristics that distinguish impact craters from volcanic craters are defined. Criteria for discriminating extensional and compressional origins of tectonic features are discussed. Volcanic edifices, constructs, and lava channels are readily identified from their radar outlines in images. Geologic map units are identified on the basis of surface texture, image brightness, pattern, and morphology. Superposition, cross-cutting relations, and areal distribution of the units serve to elucidate the geologic history."
Relating Spatial Patterns in Image Data to Scene Characteristics,78.810074,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"In remote sensing, the primary goal is accurate scene inference, in which characteristics of the scene are inferred from the image data. More effective inference of scene characteristics can be accomplished through the use of techniques that use explicit models of spatial pattern. Spatial patterns in image data are functionally related to the size and spacing of elements in the scene and to the spatial resolution of the image data. At resolutions where variance is high, scene inference techniques should rely heavily on data from the spatial domain. As variance decreases, effective scene inference will increasingly rely on spectral data."
Comparison of GEOS-5 AGCM Planetary Boundary Layer Depths Computed with Various Definitions,78.600235,computed image estimate,['Meteorology and Climatology'],"Accurate models of planetary boundary layer (PBL) processes are important for forecasting weather and climate. The present study compares seven methods of calculating PBL depth in the GEOS-5 atmospheric general circulation model (AGCM) over land. These methods depend on the eddy diffusion coefficients, bulk and local Richardson numbers, and the turbulent kinetic energy. The computed PBL depths are aggregated to the Koppen climate classes, and some limited comparisons are made using radiosonde profiles. Most methods produce similar midday PBL depths, although in the warm, moist climate classes, the bulk Richardson number method gives midday results that are lower than those given by the eddy diffusion coefficient methods. Additional analysis revealed that methods sensitive to turbulence driven by radiative cooling produce greater PBL depths, this effect being most significant during the evening transition. Nocturnal PBLs based on Richardson number are generally shallower than eddy diffusion coefficient based estimates. The bulk Richardson number estimate is recommended as the PBL height to inform the choice of the turbulent length scale, based on the similarity to other methods during the day, and the improved nighttime behavior."
Position Estimation Using Image Derivative,78.13092,computed image estimate,"['Instrumentation and Photography', 'Numerical Analysis']",This paper describes an image processing algorithm to process Moon and/or Earth images. The theory presented is based on the fact that Moon hard edge points are characterized by the highest values of the image derivative. Outliers are eliminated by two sequential filters. Moon center and radius are then estimated by nonlinear least-squares using circular sigmoid functions. The proposed image processing has been applied and validated using real and synthetic Moon images.
Material Characterization and Geometric Segmentation of a Composite Structure Using Microfocus X-Ray Computed Tomography Image-Based Finite Element Modeling,78.02926,computed image estimate,['Space Transportation and Safety'],"This study utilizes microfocus x-ray computed tomography (CT) slice sets to model and characterize the damage locations and sizes in thermal protection system materials that underwent impact testing. ScanIP/FE software is used to visualize and process the slice sets, followed by mesh generation on the segmented volumetric rendering. Then, the local stress fields around several of the damaged regions are calculated for realistic mission profiles that subject the sample to extreme temperature and other severe environmental conditions. The resulting stress fields are used to quantify damage severity and make an assessment as to whether damage that did not penetrate to the base material can still result in catastrophic failure of the structure. It is expected that this study will demonstrate that finite element modeling based on an accurate three-dimensional rendered model from a series of CT slices is an essential tool to quantify the internal macroscopic defects and damage of a complex system made out of thermal protection material. Results obtained showing details of segmented images; three-dimensional volume-rendered models, finite element meshes generated, and the resulting thermomechanical stress state due to impact loading for the material are presented and discussed. Further, this study is conducted to exhibit certain high-caliber capabilities that the nondestructive evaluation (NDE) group at NASA Glenn Research Center can offer to assist in assessing the structural durability of such highly specialized materials so improvements in their performance and capacities to handle harsh operating conditions can be made."
Image correlation and sampling study,77.31675,computed image estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],"The development of analytical approaches for solving image correlation and image sampling of multispectral data is discussed. Relevant multispectral image statistics which are applicable to image correlation and sampling are identified. The general image statistics include intensity mean, variance, amplitude histogram, power spectral density function, and autocorrelation function. The translation problem associated with digital image registration and the analytical means for comparing commonly used correlation techniques are considered. General expressions for determining the reconstruction error for specific image sampling strategies are developed."
Geometric assessment of image quality using digital image registration techniques,77.23172,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Image registration techniques were developed to perform a geometric quality assessment of multispectral and multitemporal image pairs. Based upon LANDSAT tapes, accuracies to a small fraction of a pixel were demonstrated. Because it is insensitive to the choice of registration areas, the technique is well suited to performance in an automatic system. It may be implemented at megapixel-per-second rates using a commercial minicomputer in combination with a special purpose digital preprocessor."
Studies on image compression and image reconstruction,76.920715,computed image estimate,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Visual Image Sensor Organ Replacement: Implementation,76.87235,computed image estimate,['Instrumentation and Photography'],"Method and system for enhancing or extending visual representation of a selected region of a visual image, where visual representation is interfered with or distorted, by supplementing a visual signal with at least one audio signal having one or more audio signal parameters that represent one or more visual image parameters, such as vertical and/or horizontal location of the region; region brightness; dominant wavelength range of the region; change in a parameter value that characterizes the visual image, with respect to a reference parameter value; and time rate of change in a parameter value that characterizes the visual image. Region dimensions can be changed to emphasize change with time of a visual image parameter."
Accurate estimation of object location in an image sequence using helicopter flight data,76.82541,computed image estimate,['AIRCRAFT INSTRUMENTATION'],"In autonomous navigation, it is essential to obtain a three-dimensional (3D) description of the static environment in which the vehicle is traveling. For a rotorcraft conducting low-latitude flight, this description is particularly useful for obstacle detection and avoidance. In this paper, we address the problem of 3D position estimation for static objects from a monocular sequence of images captured from a low-latitude flying helicopter. Since the environment is static, it is well known that the optical flow in the image will produce a radiating pattern from the focus of expansion. We propose a motion analysis system which utilizes the epipolar constraint to accurately estimate 3D positions of scene objects in a real world image sequence taken from a low-altitude flying helicopter. Results show that this approach gives good estimates of object positions near the rotorcraft's intended flight-path."
A study of image quality for radar image processing,76.45118,computed image estimate,['COMMUNICATIONS AND RADAR'],Methods developed for image quality metrics are reviewed with focus on basic interpretation or recognition elements including: tone or color; shape; pattern; size; shadow; texture; site; association or context; and resolution. Seven metrics are believed to show promise as a way of characterizing the quality of an image: (1) the dynamic range of intensities in the displayed image; (2) the system signal-to-noise ratio; (3) the system spatial bandwidth or bandpass; (4) the system resolution or acutance; (5) the normalized-mean-square-error as a measure of geometric fidelity; (6) the perceptual mean square error; and (7) the radar threshold quality factor. Selective levels of degradation are being applied to simulated synthetic radar images to test the validity of these metrics.
Proceedings of the NASA Workshop on Image Analysis,76.02861,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Three major topics of image analysis are addressed: segmentation, shape and texture analysis, and structural analysis."
Image 100 procedures manual development: Applications system library definition and Image 100 software definition,75.9161,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],An outline for an Image 100 procedures manual for Earth Resources Program image analysis was developed which sets forth guidelines that provide a basis for the preparation and updating of an Image 100 Procedures Manual. The scope of the outline was limited to definition of general features of a procedures manual together with special features of an interactive system. Computer programs were identified which should be implemented as part of an applications oriented library for the system.
MSS image data study,75.82829,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"A comprehensive mathematical model which includes orbital constraints and high frequency attitude effects was developed, to relate ground and image locations. The parameters of this model were derived from known ground locations of image identifiable points. It should be stressed that the term attitude effect, as used here, includes attitude like effects. The accuracy achieved, using a very dense pattern of control points, is approximately 60 meters rms, for both passes."
Astronomical use of television-type image sensors,75.404495,computed image estimate,['SPACE SCIENCES'],Conference on using TV type image sensors in astronomical photometry
Application of Image Analysis for Characterization of Spatial Arrangements of Features in Microstructure,75.367744,computed image estimate,['Metallic Materials'],"A number of microstructural processes are sensitive to the spatial arrangements of features in microstructure. However, very little attention has been given in the past to the experimental measurements of the descriptors of microstructural distance distributions due to the lack of practically feasible methods. We present a digital image analysis procedure to estimate the micro-structural distance distributions. The application of the technique is demonstrated via estimation of K function, radial distribution function, and nearest-neighbor distribution function of hollow spherical carbon particulates in a polymer matrix composite, observed in a metallographic section."
Onboard image correction,75.11992,computed image estimate,['ASTRODYNAMICS'],"A processor architecture for performing onboard geometric and radiometric correction of LANDSAT imagery is described. The design uses a general purpose processor to calculate the distortion values at selected points in the image and a special purpose processor to resample (calculate distortion at each image point and interpolate the intensity) the sensor output data. A distinct special purpose processor is used for each spectral band. Because of the sensor's high output data rate, 80 M bit per second, the special purpose processors use a pipeline architecture. Sizing has been done on both the general and special purpose hardware."
A summary of image segmentation techniques,74.83671,computed image estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Machine vision systems are often considered to be composed of two subsystems: low-level vision and high-level vision. Low level vision consists primarily of image processing operations performed on the input image to produce another image with more favorable characteristics. These operations may yield images with reduced noise or cause certain features of the image to be emphasized (such as edges). High-level vision includes object recognition and, at the highest level, scene interpretation. The bridge between these two subsystems is the segmentation system. Through segmentation, the enhanced input image is mapped into a description involving regions with common features which can be used by the higher level vision tasks. There is no theory on image segmentation. Instead, image segmentation techniques are basically ad hoc and differ mostly in the way they emphasize one or more of the desired properties of an ideal segmenter and in the way they balance and compromise one desired property against another. These techniques can be categorized in a number of different groups including local vs. global, parallel vs. sequential, contextual vs. noncontextual, interactive vs. automatic. In this paper, we categorize the schemes into three main groups: pixel-based, edge-based, and region-based. Pixel-based segmentation schemes classify pixels based solely on their gray levels. Edge-based schemes first detect local discontinuities (edges) and then use that information to separate the image into regions. Finally, region-based schemes start with a seed pixel (or group of pixels) and then grow or split the seed until the original image is composed of only homogeneous regions. Because there are a number of survey papers available, we will not discuss all segmentation schemes. Rather than a survey, we take the approach of a detailed overview. We focus only on the more common approaches in order to give the reader a flavor for the variety of techniques available yet present enough details to facilitate implementation and experimentation."
STRIPE: Remote Driving Using Limited Image Data,74.73628,computed image estimate,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
A Method to Measure and Estimate Normalized Contrast in Infrared Flash Thermography,74.51594,computed image estimate,['Instrumentation and Photography'],"The paper presents further development in normalized contrast processing used in flash infrared thermography method. Method of computing normalized image or pixel intensity contrast, and normalized temperature contrast are provided. Methods of converting image contrast to temperature contrast and vice versa are provided. Normalized contrast processing in flash thermography is useful in quantitative analysis of flash thermography data including flaw characterization and comparison of experimental results with simulation. Computation of normalized temperature contrast involves use of flash thermography data acquisition set-up with high reflectivity foil and high emissivity tape such that the foil, tape and test object are imaged simultaneously. Methods of assessing other quantitative parameters such as emissivity of object, afterglow heat flux, reflection temperature change and surface temperature during flash thermography are also provided. Temperature imaging and normalized temperature contrast processing provide certain advantages over normalized image contrast processing by reducing effect of reflected energy in images and measurements, therefore providing better quantitative data. Examples of incorporating afterglow heat-flux and reflection temperature evolution in flash thermography simulation are also discussed."
Solid Hydrogen Experiments for Atomic Propellants: Image Analyses,74.42861,computed image estimate,['Propellants and Fuels'],"This paper presents the results of detailed analyses of the images from experiments that were conducted on the formation of solid hydrogen particles in liquid helium. Solid particles of hydrogen were frozen in liquid helium, and observed with a video camera. The solid hydrogen particle sizes, their agglomerates, and the total mass of hydrogen particles were estimated. Particle sizes of 1.9 to 8 mm (0.075 to 0.315 in.) were measured. The particle agglomerate sizes and areas were measured, and the total mass of solid hydrogen was computed. A total mass of from 0.22 to 7.9 grams of hydrogen was frozen. Compaction and expansion of the agglomerate implied that the particles remain independent particles, and can be separated and controlled. These experiment image analyses are one of the first steps toward visually characterizing these particles, and allow designers to understand what issues must be addressed in atomic propellant feed system designs for future aerospace vehicles."
Research relative to automated multisensor image registration,74.01347,computed image estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],The basic aproaches to image registration are surveyed. Three image models are presented as models of the subpixel problem. A variety of approaches to the analysis of subpixel analysis are presented using these models.
Research Issues in Image Registration for Remote Sensing,73.72105,computed image estimate,['Earth Resources and Remote Sensing'],"Image registration is an important element in data processing for remote sensing with many applications and a wide range of solutions. Despite considerable investigation the field has not settled on a definitive solution for most applications and a number of questions remain open. This article looks at selected research issues by surveying the experience of operational satellite teams, application-specific requirements for Earth science, and our experiments in the evaluation of image registration algorithms with emphasis on the comparison of algorithms for subpixel accuracy. We conclude that remote sensing applications put particular demands on image registration algorithms to take into account domain-specific knowledge of geometric transformations and image content."
Image Mission Attitude Support Experiences,73.511505,computed image estimate,['Astrodynamics'],"The spin-stabilized Imager for Magnetopause to Aurora Global Exploration (IMAGE) is the National Aeronautics and Space Administration's (NASA's) first Medium-class Explorer Mission (MIDEX). IMAGE was launched into a highly elliptical polar orbit on March 25, 2000 from Vandenberg Air Force Base, California, aboard a Boeing Delta II 7326 launch vehicle. This paper presents some of the observations of the flight dynamics analyses during the launch and in-orbit checkout period through May 18, 2000. Three new algorithms - one algebraic and two differential correction - for computing the parameters of the coning motion of a spacecraft are described and evaluated using in-flight data from the autonomous star tracker (AST) on IMAGE. Other attitude aspects highlighted include support for active damping consequent upon the failure of the passive nutation damper, performance evaluation of the AST, evaluation of the Sun sensor and magnetometer using AST data, and magnetometer calibration."
Metric Learning to Enhance Hyperspectral Image Segmentation,73.229744,computed image estimate,['Instrumentation and Photography'],"Unsupervised hyperspectral image segmentation can reveal spatial trends that show the physical structure of the scene to an analyst. They highlight borders and reveal areas of homogeneity and change. Segmentations are independently helpful for object recognition, and assist with automated production of symbolic maps. Additionally, a good segmentation can dramatically reduce the number of effective spectra in an image, enabling analyses that would otherwise be computationally prohibitive. Specifically, using an over-segmentation of the image instead of individual pixels can reduce noise and potentially improve the results of statistical post-analysis. In this innovation, a metric learning approach is presented to improve the performance of unsupervised hyperspectral image segmentation. The prototype demonstrations attempt a superpixel segmentation in which the image is conservatively over-segmented; that is, the single surface features may be split into multiple segments, but each individual segment, or superpixel, is ensured to have homogenous mineralogy."
Algorithms for detection of objects in image sequences captured from an airborne imaging system,72.840034,computed image estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],This research was initiated as a part of the effort at the NASA Ames Research Center to design a computer vision based system that can enhance the safety of navigation by aiding the pilots in detecting various obstacles on the runway during critical section of the flight such as a landing maneuver. The primary goal is the development of algorithms for detection of moving objects from a sequence of images obtained from an on-board video camera. Image regions corresponding to the independently moving objects are segmented from the background by applying constraint filtering on the optical flow computed from the initial few frames of the sequence. These detected regions are tracked over subsequent frames using a model based tracking algorithm. Position and velocity of the moving objects in the world coordinate is estimated using an extended Kalman filter. The algorithms are tested using the NASA line image sequence with six static trucks and a simulated moving truck and experimental results are described. Various limitations of the currently implemented version of the above algorithm are identified and possible solutions to build a practical working system are investigated.
Advanced Image Processing for NASA Applications,72.54489,computed image estimate,['Space Sciences (General)'],"The future of space exploration will involve cooperating fleets of spacecraft or sensor webs geared towards coordinated and optimal observation of Earth Science phenomena. The main advantage of such systems is to utilize multiple viewing angles as well as multiple spatial and spectral resolutions of sensors carried on multiple spacecraft but acting collaboratively as a single system. Within this framework, our research focuses on all areas related to sensing in collaborative environments, which means systems utilizing intracommunicating spatially distributed sensor pods or crafts being deployed to monitor or explore different environments. This talk will describe the general concept of sensing in collaborative environments, will give a brief overview of several technologies developed at NASA Goddard Space Flight Center in this area, and then will concentrate on specific image processing research related to that domain, specifically image registration and image fusion."
Detection of Obstacles in Monocular Image Sequences,72.472305,computed image estimate,['Optics'],"The ability to detect and locate runways/taxiways and obstacles in images captured using on-board sensors is an essential first step in the automation of low-altitude flight, landing, takeoff, and taxiing phase of aircraft navigation. Automation of these functions under different weather and lighting situations, can be facilitated by using sensors of different modalities. An aircraft-based Synthetic Vision System (SVS), with sensors of different modalities mounted on-board, complements the current ground-based systems in functions such as detection and prevention of potential runway collisions, airport surface navigation, and landing and takeoff in all weather conditions. In this report, we address the problem of detection of objects in monocular image sequences obtained from two types of sensors, a Passive Millimeter Wave (PMMW) sensor and a video camera mounted on-board a landing aircraft. Since the sensors differ in their spatial resolution, and the quality of the images obtained using these sensors is not the same, different approaches are used for detecting obstacles depending on the sensor type. These approaches are described separately in two parts of this report. The goal of the first part of the report is to develop a method for detecting runways/taxiways and objects on the runway in a sequence of images obtained from a moving PMMW sensor. Since the sensor resolution is low and the image quality is very poor, we propose a model-based approach for detecting runways/taxiways. We use the approximate runway model and the position information of the camera provided by the Global Positioning System (GPS) to define regions of interest in the image plane to search for the image features corresponding to the runway markers. Once the runway region is identified, we use histogram-based thresholding to detect obstacles on the runway and regions outside the runway. This algorithm is tested using image sequences simulated from a single real PMMW image."
A system to monitor stellar image quality,72.39001,computed image estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],Stellar image quality monitoring system
Mapping Bone Mineral Density Obtained by Quantitative Computed Tomography to Bone Volume Fraction,72.20645,computed image estimate,"['Aerospace Medicine', 'Life Sciences (General)']","Methods for relating or mapping estimates of volumetric Bone Mineral Density (vBMD) obtained by Quantitative Computed Tomography to Bone Volume Fraction (BVF) are outlined mathematically. The methods are based on definitions of bone properties, cited experimental studies and regression relations derived from them for trabecular bone in the proximal femur. Using an experimental range of values in the intertrochanteric region obtained from male and female human subjects, age 18 to 49, the BVF values calculated from four different methods were compared to the experimental average and numerical range. The BVF values computed from the conversion method used data from two sources. One source provided pre bed rest vBMD values in the intertrochanteric region from 24 bed rest subject who participated in a 70 day study. Another source contained preflight vBMD values from 18 astronauts who spent 4 to 6 months on the ISS. To aid the use of a mapping from BMD to BVF, the discussion includes how to formulate them for purpose of computational modeling. An application of the conversions would be used to aid in modeling of time varying changes in vBMD as it relates to changes in BVF via bone remodeling and/or modeling."
Image data processing system requirements study. Volume 1:  Analysis,72.1712,computed image estimate,['GEOPHYSICS'],"Digital image processing, image recorders, high-density digital data recorders, and data system element processing for use in an Earth Resources Survey image data processing system are studied. Loading to various ERS systems is also estimated by simulation."
High Resolution Image Reconstruction from Projection of Low Resolution Images DIffering in Subpixel Shifts,72.00685,computed image estimate,['Numerical Analysis'],"In this paper, we demonstrate a simple algorithm that projects low resolution (LR) images differing in subpixel shifts on a high resolution (HR) also called super resolution (SR) grid. The algorithm is very effective in accuracy as well as time efficiency. A number of spatial interpolation techniques using nearest neighbor, inverse-distance weighted averages, Radial Basis Functions (RBF) etc. used in projection yield comparable results. For best accuracy of reconstructing SR image by a factor of two requires four LR images differing in four independent subpixel shifts. The algorithm has two steps: i) registration of low resolution images and (ii) shifting the low resolution images to align with reference image and projecting them on high resolution grid based on the shifts of each low resolution image using different interpolation techniques. Experiments are conducted by simulating low resolution images by subpixel shifts and subsampling of original high resolution image and the reconstructing the high resolution images from the simulated low resolution images. The results of accuracy of reconstruction are compared by using mean squared error measure between original high resolution image and reconstructed image. The algorithm was tested on remote sensing images and found to outperform previously proposed techniques such as Iterative Back Projection algorithm (IBP), Maximum Likelihood (ML), and Maximum a posterior (MAP) algorithms. The algorithm is robust and is not overly sensitive to the registration inaccuracies. "
Progressive Band Selection,71.92032,computed image estimate,['Instrumentation and Photography'],"Progressive band selection (PBS) reduces spectral redundancy without significant loss of information, thereby reducing hyperspectral image data volume and processing time. Used onboard a spacecraft, it can also reduce image downlink time. PBS prioritizes an image's spectral bands according to priority scores that measure their significance to a specific application. Then it uses one of three methods to select an appropriate number of the most useful bands. Key challenges for PBS include selecting an appropriate criterion to generate band priority scores, and determining how many bands should be retained in the reduced image. The image's Virtual Dimensionality (VD), once computed, is a reasonable estimate of the latter. We describe the major design details of PBS and test PBS in a land classification experiment."
Data analysis for GOPEX image frames,71.82631,computed image estimate,['COMMUNICATIONS AND RADAR'],"The data analysis based on the image frames received at the Solid State Imaging (SSI) camera of the Galileo Optical Experiment (GOPEX) demonstration conducted between 9-16 Dec. 1992 is described. Laser uplink was successfully established between the ground and the Galileo spacecraft during its second Earth-gravity-assist phase in December 1992. SSI camera frames were acquired which contained images of detected laser pulses transmitted from the Table Mountain Facility (TMF), Wrightwood, California, and the Starfire Optical Range (SOR), Albuquerque, New Mexico. Laser pulse data were processed using standard image-processing techniques at the Multimission Image Processing Laboratory (MIPL) for preliminary pulse identification and to produce public release images. Subsequent image analysis corrected for background noise to measure received pulse intensities. Data were plotted to obtain histograms on a daily basis and were then compared with theoretical results derived from applicable weak-turbulence and strong-turbulence considerations. Processing steps are described and the theories are compared with the experimental results. Quantitative agreement was found in both turbulence regimes, and better agreement would have been found, given more received laser pulses. Future experiments should consider methods to reliably measure low-intensity pulses, and through experimental planning to geometrically locate pulse positions with greater certainty."
Image reconstruction of IRAS survey scans,71.6671,computed image estimate,['ASTRONOMY'],"The IRAS survey data can be used successfully to produce images of extended objects. The major difficulties, viz. non-uniform sampling, different response functions for each detector, and varying signal-to-noise levels for each detector for each scan, were resolved. The results of three different image construction techniques are compared: co-addition, constrained least squares, and maximum entropy. The maximum entropy result is superior. An image of the galaxy M51 with an average spatial resolution of 45 arc seconds is presented, using 60 micron survey data. This exceeds the telescope diffraction limit of 1 minute of arc, at this wavelength. Data fusion is a proposed method for combining data from different instruments, with different spacial resolutions, at different wavelengths. Data estimates of the physical parameters, temperature, density and composition, can be made from the data without prior image (re-)construction. An increase in the accuracy of these parameters is expected as the result of this more systematic approach."
X-Ray Computed Tomography Inspection of the Stardust Heat Shield,70.885735,computed image estimate,"['Spacecraft Design, Testing and Performance']","The ""Stardust"" heat shield, composed of a PICA (Phenolic Impregnated Carbon Ablator) Thermal Protection System (TPS), bonded to a composite aeroshell, contains important features which chronicle its time in space as well as re-entry. To guide the further study of the Stardust heat shield, NASA reviewed a number of techniques for inspection of the article. The goals of the inspection were: 1) to establish the material characteristics of the shield and shield components, 2) record the dimensions of shield components and assembly as compared with the pre-flight condition, 3) provide flight infonnation for validation and verification of the FIAT ablation code and PICA material property model and 4) through the evaluation of the shield material provide input to future missions which employ similar materials. Industrial X-Ray Computed Tomography (CT) is a 3D inspection technology which can provide infonnation on material integrity, material properties (density) and dimensional measurements of the heat shield components. Computed tomographic volumetric inspections can generate a dimensionally correct, quantitatively accurate volume of the shield assembly. Because of the capabilities offered by X-ray CT, NASA chose to use this method to evaluate the Stardust heat shield. Personnel at NASA Johnson Space Center (JSC) and Lawrence Livermore National Labs (LLNL) recently performed a full scan of the Stardust heat shield using a newly installed X-ray CT system at JSC. This paper briefly discusses the technology used and then presents the following results: 1. CT scans derived dimensions and their comparisons with as-built dimensions anchored with data obtained from samples cut from the heat shield; 2. Measured density variation, char layer thickness, recession and bond line (the adhesive layer between the PICA and the aeroshell) integrity; 3. FIAT predicted recession, density and char layer profiles as well as bondline temperatures Finally suggestions are made as to future uses of this technology as a tool for non-destructively inspecting and verifying both pre and post flight heat shields."
Image Navigation and Registration Performance Assessment Tool Set for the GOES-R Advanced Baseline Imager and Geostationary Lightning Mapper,70.879616,computed image estimate,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']","The GOES-R Flight Project has developed an Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for measuring Advanced Baseline Imager (ABI) and Geostationary Lightning Mapper (GLM) INR performance metrics in the post-launch period for performance evaluation and long term monitoring. For ABI, these metrics are the 3-sigma errors in navigation (NAV), channel-to-channel registration (CCR), frame-to-frame registration (FFR), swath-to-swath registration (SSR), and within frame registration (WIFR) for the Level 1B image products. For GLM, the single metric of interest is the 3-sigma error in the navigation of background images (GLM NAV) used by the system to navigate lightning strikes. 3-sigma errors are estimates of the 99.73rd percentile of the errors accumulated over a 24-hour data collection period. IPATS utilizes a modular algorithmic design to allow user selection of data processing sequences optimized for generation of each INR metric. This novel modular approach minimizes duplication of common processing elements, thereby maximizing code efficiency and speed. Fast processing is essential given the large number of sub-image registrations required to generate INR metrics for the many images produced over a 24-hour evaluation period. Another aspect of the IPATS design that vastly reduces execution time is the off-line propagation of Landsat based truth images to the fixed grid coordinates system for each of the three GOES-R satellite locations, operational East and West and initial checkout locations. This paper describes the algorithmic design and implementation of IPATS and provides preliminary test results."
Inter-image matching,70.854805,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Interimage matching is the process of determining the geometric transformation required to conform spatially one image to another. In principle, the parameters of that transformation are varied until some measure of some difference between the two images is minimized or some measure of sameness (e.g., cross-correlation) is maximized. The number of such parameters to vary is faily large (six for merely an affine transformation), and it is customary to attempt an a priori transformation reducing the complexity of the residual transformation or subdivide the image into small enough match zones (control points or patches) that a simple transformation (e.g., pure translation) is applicable, yet large enough to facilitate matching. In the latter case, a complex mapping function is fit to the results (e.g., translation offsets) in all the patches. The methods reviewed have all chosen one or both of the above options, ranging from a priori along-line correction for line-dependent effects (the high-frequency correction) to a full sensor-to-geobase transformation with subsequent subdivision into a grid of match points."
Image Guider Subsystem Analysis for the GHAPS Project,70.685776,computed image estimate,"['Optics', 'Astronomy']","The Gondola for High-Altitude Planetary Science (GHAPS) project is a balloon-borne astronomical observatory designed operate in the UV, Visible, and near-mid IR spectral region. The GHAPS Optical Telescope Assembly (OTA) is designed around a one meter aperture narrow field-of-view telescope with near-diffraction-limited performance. GHAPS will utilize Wallops Arc-Second Pointing System (WASP) for pointing the OTA with an accuracy of 1 arc second or better. WASP relies heavily on a self-contained star tracker assembly to determine the OTA line of sight. Preliminary structural analysis indicates that potential misalignments could be present between the OTA line of sight and the star tracker FOV center during the expected flight conditions that could compromise GHAPS pointing accuracy. In response the GHAPS project undertook a trade study to resolve the following issues: (1) estimate the worst case long-term (or bias) pointing misalignments for the GHAPS opto-mechanical configuration, (2) examine the need for additional hardware to correct pointing errors, and (3) determine the best hardware and software implementation to do so. Quantitative comparisons of performance and qualitative estimates of other factors such as mass, volume, power consumption, and cost are combined into an overall assessment of potential solutions. Results are discussed and a recommended implementation is given that is optimized to best achieve pointing performance goals, while minimizing impact to the design, cost, and resources of the GHAPS project."
Automatic Extraction of Planetary Image Features,70.50825,computed image estimate,['Instrumentation and Photography'],"With the launch of several Lunar missions such as the Lunar Reconnaissance Orbiter (LRO) and Chandrayaan-1, a large amount of Lunar images will be acquired and will need to be analyzed. Although many automatic feature extraction methods have been proposed and utilized for Earth remote sensing images, these methods are not always applicable to Lunar data that often present low contrast and uneven illumination characteristics. In this paper, we propose a new method for the extraction of Lunar features (that can be generalized to other planetary images), based on the combination of several image processing techniques, a watershed segmentation and the generalized Hough Transform. This feature extraction has many applications, among which image registration."
Image Analysis Based on Soft Computing and Applied on Space Shuttle During the Liftoff Process,70.1844,computed image estimate,['Instrumentation and Photography'],"Imaging techniques based on Soft Computing (SC) and developed at Kennedy Space Center (KSC) have been implemented on a variety of prototype applications related to the safety operation of the Space Shuttle during the liftoff process. These SC-based prototype applications include detection and tracking of moving Foreign Objects Debris (FOD) during the Space Shuttle liftoff, visual anomaly detection on slidewires used in the emergency egress system for the Space Shuttle at the laJlIlch pad, and visual detection of distant birds approaching the Space Shuttle launch pad. This SC-based image analysis capability developed at KSC was also used to analyze images acquired during the accident of the Space Shuttle Columbia and estimate the trajectory and velocity of the foam that caused the accident."
The Laser Atmospheric Wind Sounder (LAWS) Preliminary Error Budget and Performance Estimate,70.00487,computed image estimate,['ENVIRONMENT POLLUTION'],"The Laser Atmospheric Wind Sounder (LAWS) study phase has resulted in a preliminary error budget and an estimate of the instrument performance. This paper will present the line-of-sight (LOS) Velocity Measurement Error Budget, the instrument Boresight Error Budget, and the predicted signal-to-noise ratio (SNR) performance. The measurement requirements and a preliminary design for the LAWS instrument are presented in a companion paper."
Subduction in an Eddy-Resolving State Estimate of the Northeast Atlantic Ocean,69.96417,computed image estimate,['Fluid Mechanics and Thermodynamics'],"Are eddies an important contributor to subduction in the eastern subtropical gyre? Here, an adjoint model is used to combine a regional, eddy-resolving numerical model with observations to produce a state estimate of the ocean circulation. The estimate is a synthesis of a variety of in- situ observations from the Subduction Experiment, TOPEX/POSEIDON altimetry, and the MTI General Circulation Model. The adjoint method is successful because the Northeast Atlantic Ocean is only weakly nonlinear. The state estimate provides a physically-interpretable, eddy-resolving information source to diagnose subduction. Estimates of eddy subduction for the eastern subtropical gyre of the North Atlantic are larger than previously calculated from parameterizations in coarse-resolution models. Furthermore, eddy subduction rates have typical magnitudes of 15% of the total subduction rate. Eddies contribute as much as 1 Sverdrup to water-mass transformation, and hence subduction, in the North Equatorial Current and the Azores Current. The findings of this thesis imply that the inability to resolve or accurately parameterize eddy subduction in climate models would lead to an accumulation of error in the structure of the main thermocline, even in the relatively-quiescent eastern subtropical gyre."
Three-Dimensional Imaging and Numerical Reconstruction of Graphite/Epoxy Composite Microstructure Based on Ultra-High Resolution X-Ray Computed Tomography,69.89201,computed image estimate,['Composite Materials'],"A combined experimental and computational study aimed at high-resolution 3D imaging, visualization, and numerical reconstruction of fiber-reinforced polymer microstructures at the fiber length scale is presented. To this end, a sample of graphite/epoxy composite was imaged at sub-micron resolution using a 3D X-ray computed tomography microscope. Next, a novel segmentation algorithm was developed, based on concepts adopted from computer vision and multi-target tracking, to detect and estimate, with high accuracy, the position of individual fibers in a volume of the imaged composite. In the current implementation, the segmentation algorithm was based on Global Nearest Neighbor data-association architecture, a Kalman filter estimator, and several novel algorithms for virtualfiber stitching, smoothing, and overlap removal. The segmentation algorithm was used on a sub-volume of the imaged composite, detecting 508 individual fibers. The segmentation data were qualitatively compared to the tomographic data, demonstrating high accuracy of the numerical reconstruction. Moreover, the data were used to quantify a) the relative distribution of individual-fiber cross sections within the imaged sub-volume, and b) the local fiber misorientation relative to the global fiber axis. Finally, the segmentation data were converted using commercially available finite element (FE) software to generate a detailed FE mesh of the composite volume. The methodology described herein demonstrates the feasibility of realizing an FE-based, virtual-testing framework for graphite/fiber composites at the constituent level."
Gray-level transformations for interactive image enhancement,69.745766,computed image estimate,['CYBERNETICS'],A gray-level transformation method suitable for interactive image enhancement was presented. It is shown that the well-known histogram equalization approach is a special case of this method. A technique for improving the uniformity of a histogram is also developed. Experimental results which illustrate the capabilities of both algorithms are described. Two proposals for implementing gray-level transformations in a real-time interactive image enhancement system are also presented.
Proceedings of the Second Annual Symposium on Mathematical Pattern Recognition and Image Analysis Program,69.28598,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Several papers addressing image analysis and pattern recognition techniques for satellite imagery are presented. Texture classification, image rectification and registration, spatial parameter estimation, and surface fitting are discussed."
The New CCSDS Image Compression Recommendation,69.1168,computed image estimate,['Instrumentation and Photography'],"The Consultative Committee for Space Data Systems (CCSDS) data compression working group has recently adopted a recommendation for image data compression, with a final release expected in 2005. The algorithm adopted in the recommendation consists of a two-dimensional discrete wavelet transform of the image, followed by progressive bit-plane coding of the transformed data. The algorithm can provide both lossless and lossy compression, and allows a user to directly control the compressed data volume or the fidelity with which the wavelet-transformed data can be reconstructed. The algorithm is suitable for both frame-based image data and scan-based sensor data, and has applications for near-Earth and deep-space missions. The standard will be accompanied by free software sources on a future web site. An Application-Specific Integrated Circuit (ASIC) implementation of the compressor is currently under development. This paper describes the compression algorithm along with the requirements that drove the selection of the algorithm. Performance results and comparisons with other compressors are given for a test set of space images."
The application of image enhancement techniques to remote manipulator operation,68.97302,computed image estimate,"['PHYSICS, GENERAL']","Methods of image enhancement which can be used by an operator who is not experienced with the mechanisms of enhancement to obtain satisfactory results were designed and implemented. Investigation of transformations which operate directly on the image domain resulted in a new technique of contrast enhancement. Transformations on the Fourier transform of the original image, including such techniques as homomorphic filtering, were also investigated. The methods of communication between the enhancement system and the computer operator were analyzed, and a language was developed for use in image enhancement. A working enhancement system was then created, and is included."
Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for the GOES-R Advanced Baseline Imager and Geostationary Lightning Mapper,68.85741,computed image estimate,"['Instrumentation and Photography', 'Meteorology and Climatology']","The GOES-R Flight Project has developed an Image Navigation and Registration (INR) Performance Assessment Tool Set (IPATS) for measuring Advanced Baseline Imager (ABI) and Geostationary Lightning Mapper (GLM) INR performance metrics in the post-launch period for performance evaluation and long term monitoring. For ABI, these metrics are the 3-sigma errors in navigation (NAV), channel-to-channel registration (CCR), frame-to-frame registration (FFR), swath-to-swath registration (SSR), and within frame registration (WIFR) for the Level 1B image products. For GLM, the single metric of interest is the 3-sigma error in the navigation of background images (GLM NAV) used by the system to navigate lightning strikes. 3-sigma errors are estimates of the 99.73rd percentile of the errors accumulated over a 24 hour data collection period. IPATS utilizes a modular algorithmic design to allow user selection of data processing sequences optimized for generation of each INR metric. This novel modular approach minimizes duplication of common processing elements, thereby maximizing code efficiency and speed. Fast processing is essential given the large number of sub-image registrations required to generate INR metrics for the many images produced over a 24 hour evaluation period. Another aspect of the IPATS design that vastly reduces execution time is the off-line propagation of Landsat based truth images to the fixed grid coordinates system for each of the three GOES-R satellite locations, operational East and West and initial checkout locations. This paper describes the algorithmic design and implementation of IPATS and provides preliminary test results."
Precise Image-Based Motion Estimation for Autonomous Small Body Exploration,68.62272,computed image estimate,['Astronautics (General)'],Space science and solar system exploration are driving NASA to develop an array of small body missions ranging in scope from near body flybys to complete sample return. This paper presents an algorithm for onboard motion estimation that will enable the precision guidance necessary for autonomous small body landing. Our techniques are based on automatic feature tracking between a pair of descent camera images followed by two frame motion estimation and scale recovery using laser altimetry data. The output of our algorithm is an estimate of rigid motion (attitude and position) and motion covariance between frames. This motion estimate can be passed directly to the spacecraft guidance and control system to enable rapid execution of safe and precise trajectories.
"Solid state image sensor research, phase 2",68.414375,computed image estimate,['ELECTRONIC EQUIPMENT'],Solid state image sensor array
Proceedings of the NASA Symposium on Mathematical Pattern Recognition and Image Analysis,68.384026,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"The application of mathematical and statistical analyses techniques to imagery obtained by remote sensors is described by Principal Investigators. Scene-to-map registration, geometric rectification, and image matching are among the pattern recognition aspects discussed."
Single-lens computed tomography imaging spectrometer and method of capturing spatial and spectral information,67.997055,computed image estimate,['Optics'],"Computed tomography imaging spectrometers (""CTISs"") employing a single lens are provided. The CTISs may be either transmissive or reflective, and the single lens is either configured to transmit and receive uncollimated light (in transmissive systems), or is configured to reflect and receive uncollimated light (in reflective systems). An exemplary transmissive CTIS includes a focal plane array detector, a single lens configured to transmit and receive uncollimated light, a two-dimensional grating, and a field stop aperture. An exemplary reflective CTIS includes a focal plane array detector, a single mirror configured to reflect and receive uncollimated light, a two-dimensional grating, and a field stop aperture."
Always-Convergent Iterative Noise Removal and Deconvolution for Image Data,67.95369,computed image estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Linear filtering techniques currently used for the restoration of noisy, blurred or otherwise degraded image data are discussed and new techniques related to the iterative techniques of Morrison and van Cittert are developed and implemented. Programs written for the implementation are discussed in the appendices. It is shown that the new techniques are convergent for any system response function, and they are applied to the task of restoring a severely blurred image."
"Synthetic aperture radar target detection, feature extraction, and image formation techniques",67.680954,computed image estimate,['COMMUNICATIONS AND RADAR'],"This report presents new algorithms for target detection, feature extraction, and image formation with the synthetic aperture radar (SAR) technology. For target detection, we consider target detection with SAR and coherent subtraction. We also study how the image false alarm rates are related to the target template false alarm rates when target templates are used for target detection. For feature extraction from SAR images, we present a computationally efficient eigenstructure-based 2D-MODE algorithm for two-dimensional frequency estimation. For SAR image formation, we present a robust parametric data model for estimating high resolution range signatures of radar targets and for forming high resolution SAR images."
Some comments on particle image displacement velocimetry,67.63628,computed image estimate,['FLUID MECHANICS AND HEAT TRANSFER'],"Laser speckle velocimetry (LSV) or particle image displacement velocimetry, is introduced. This technique provides the simultaneous visualization of the two-dimensional streamline pattern in unsteady flows as well as the quantification of the velocity field over an entire plane. The advantage of this technique is that the velocity field can be measured over an entire plane of the flow field simultaneously, with accuracy and spatial resolution. From this the instantaneous vorticity field can be easily obtained. This constitutes a great asset for the study of a variety of flows that evolve stochastically in both space and time. The basic concept of LSV; methods of data acquisition and reduction, examples of its use, and parameters that affect its utilization are described."
A New Approach to Image Fusion Based on Cokriging,67.4468,computed image estimate,['Earth Resources and Remote Sensing'],We consider the image fusion problem involving remotely sensed data. We introduce cokriging as a method to perform fusion. We investigate the advantages of fusing Hyperion with ALI. The evaluation is performed by comparing the classification of the fused data with that of input images and by calculating well-chosen quantitative fusion quality metrics. We consider the Invasive Species Forecasting System (ISFS) project as our fusion application. The fusion of ALI with Hyperion data is studies using PCA and wavelet-based fusion. We then propose utilizing a geostatistical based interpolation method called cokriging as a new approach for image fusion.
Uniform color space analysis of LACIE image products,67.439224,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Analysis and comparison of image products generated by different algorithms show that the scaling and biasing of data channels for control of PFC primaries lead to loss of information (in a probability-of misclassification sense) by two major processes. In order of importance they are: neglecting the input of one channel of data in any one image, and failing to provide sufficient color resolution of the data. The scaling and biasing approach tends to distort distance relationships in data space and provides less than desirable resolution when the data variation is typical of a developed, nonhazy agricultural scene."
EOS image data processing system definition study,67.41385,computed image estimate,['GEOPHYSICS'],"The Image Processing System (IPS) requirements and configuration are defined for NASA-sponsored advanced technology Earth Observatory System (EOS). The scope included investigation and definition of IPS operational, functional, and product requirements considering overall system constraints and interfaces (sensor, etc.) The scope also included investigation of the technical feasibility and definition of a point design reflecting system requirements. The design phase required a survey of present and projected technology related to general and special-purpose processors, high-density digital tape recorders, and image recorders."
Proceedings of the Third Annual Symposium on Mathematical Pattern Recognition and Image Analysis,67.285126,computed image estimate,['NUMERICAL ANALYSIS'],Topics addressed include: multivariate spline method; normal mixture analysis applied to remote sensing; image data analysis; classifications in spatially correlated environments; probability density functions; graphical nonparametric methods; subpixel registration analysis; hypothesis integration in image understanding systems; rectification of satellite scanner imagery; spatial variation in remotely sensed images; smooth multidimensional interpolation; and optimal frequency domain textural edge detection filters.
Purkinje image eyetracking:  A market survey,67.18166,computed image estimate,['AEROSPACE MEDICINE'],"The Purkinje image eyetracking system was analyzed to determine the marketability of the system. The eyetracking system is a synthesis of two separate instruments, the optometer that measures the refractive power of the eye and the dual Purkinje image eyetracker that measures the direction of the visual axis."
Qualification of a Null Lens Using Image-Based Phase Retrieval,66.82503,computed image estimate,['Optics'],"In measuring the figure error of an aspheric optic using a null lens, the wavefront contribution from the null lens must be independently and accurately characterized in order to isolate the optical performance of the aspheric optic alone. Various techniques can be used to characterize such a null lens, including interferometry, profilometry and image-based methods. Only image-based methods, such as phase retrieval, can measure the null-lens wavefront in situ - in single-pass, and at the same conjugates and in the same alignment state in which the null lens will ultimately be used - with no additional optical components. Due to the intended purpose of a Dull lens (e.g., to null a large aspheric wavefront with a near-equal-but-opposite spherical wavefront), characterizing a null-lens wavefront presents several challenges to image-based phase retrieval: Large wavefront slopes and high-dynamic-range data decrease the capture range of phase-retrieval algorithms, increase the requirements on the fidelity of the forward model of the optical system, and make it difficult to extract diagnostic information (e.g., the system F/#) from the image data. In this paper, we present a study of these effects on phase-retrieval algorithms in the context of a null lens used in component development for the Climate Absolute Radiance and Refractivity Observatory (CLARREO) mission. Approaches for mitigation are also discussed."
Image gathering and coding for digital restoration: Information efficiency and visual quality,66.75952,computed image estimate,['INSTRUMENTATION AND PHOTOGRAPHY'],"Image gathering and coding are commonly treated as tasks separate from each other and from the digital processing used to restore and enhance the images. The goal is to develop a method that allows us to assess quantitatively the combined performance of image gathering and coding for the digital restoration of images with high visual quality. Digital restoration is often interactive because visual quality depends on perceptual rather than mathematical considerations, and these considerations vary with the target, the application, and the observer. The approach is based on the theoretical treatment of image gathering as a communication channel (J. Opt. Soc. Am. A2, 1644(1985);5,285(1988). Initial results suggest that the practical upper limit of the information contained in the acquired image data range typically from approximately 2 to 4 binary information units (bifs) per sample, depending on the design of the image-gathering system. The associated information efficiency of the transmitted data (i.e., the ratio of information over data) ranges typically from approximately 0.3 to 0.5 bif per bit without coding to approximately 0.5 to 0.9 bif per bit with lossless predictive compression and Huffman coding. The visual quality that can be attained with interactive image restoration improves perceptibly as the available information increases to approximately 3 bifs per sample. However, the perceptual improvements that can be attained with further increases in information are very subtle and depend on the target and the desired enhancement."
"Image Display and Manipulation System (IDAMS) program documentation, Appendixes A-D",66.5216,computed image estimate,['COMPUTERS'],"The IDAMS Processor is a package of task routines and support software that performs convolution filtering, image expansion, fast Fourier transformation, and other operations on a digital image tape. A unique task control card for that program, together with any necessary parameter cards, selects each processing technique to be applied to the input image. A variable number of tasks can be selected for execution by including the proper task and parameter cards in the input deck. An executive maintains control of the run; it initiates execution of each task in turn and handles any necessary error processing."
A synoptic description of coal basins via image processing,66.50124,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"An existing image processing system is adapted to describe the geologic attributes of a regional coal basin. This scheme handles a map as if it were a matrix, in contrast to more conventional approaches which represent map information in terms of linked polygons. The utility of the image processing approach is demonstrated by a multiattribute analysis of the Herrin No. 6 coal seam in Illinois. Findings include the location of a resource and estimation of tonnage corresponding to constraints on seam thickness, overburden, and Btu value, which are illustrative of the need for new mining technology."
Image analysis of the AXAF VETA-I x ray mirror,66.441025,computed image estimate,['OPTICS'],"Initial core scan data of the VETA-I x-ray mirror proved disappointing, showing considerable unpredicted image structure and poor measured FWHM. 2-D core scans were performed, providing important insight into the nature of the distortion. Image deconvolutions using a ray traced model PSF was performed successfully to reinforce our conclusion regarding the origin of the astigmatism. A mechanical correction was made to the optical structure, and the mirror was tested successfully (FWHM 0.22 arcsec) as a result."
Digital image correlation techniques applied to LANDSAT multispectral imagery,66.381744,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Automatic image registration and resampling techniques applied to LANDSAT data achieved accuracies, resulting in mean radial displacement errors of less than 0.2 pixel. The process method utilized recursive computational techniques and line-by-line updating on the basis of feedback error signals. Goodness of local feature matching was evaluated through the implementation of a correlation algorithm. An automatic restart allowed the system to derive control point coordinates over a portion of the image and to restart the process, utilizing this new control point information as initial estimates."
System and method for image mapping and visual attention,66.34363,computed image estimate,['Instrumentation and Photography'],"A method is described for mapping dense sensory data to a Sensory Ego Sphere (SES). Methods are also described for finding and ranking areas of interest in the images that form a complete visual scene on an SES. Further, attentional processing of image data is best done by performing attentional processing on individual full-size images from the image sequence, mapping each attentional location to the nearest node, and then summing all attentional locations at each node."
System and method for image mapping and visual attention,66.34086,computed image estimate,['Instrumentation and Photography'],"A method is described for mapping dense sensory data to a Sensory Ego Sphere (SES). Methods are also described for finding and ranking areas of interest in the images that form a complete visual scene on an SES. Further, attentional processing of image data is best done by performing attentional processing on individual full-size images from the image sequence, mapping each attentional location to the nearest node, and then summing attentional locations at each node."
Nonnegative Matrix Factorization for Efficient Hyperspectral Image Projection,66.32108,computed image estimate,"['Optics', 'Instrumentation and Photography']","Hyperspectral imaging for remote sensing has prompted development of hyperspectral image projectors that can be used to characterize hyperspectral imaging cameras and techniques in the lab. One such emerging astronomical hyperspectral imaging technique is wide-field double-Fourier interferometry. NASA's current, state-of-the-art, Wide-field Imaging Interferometry Testbed (WIIT) uses a Calibrated Hyperspectral Image Projector (CHIP) to generate test scenes and provide a more complete understanding of wide-field double-Fourier interferometry. Given enough time, the CHIP is capable of projecting scenes with astronomically realistic spatial and spectral complexity. However, this would require a very lengthy data collection process. For accurate but time-efficient projection of complicated hyperspectral images with the CHIP, the field must be decomposed both spectrally and spatially in a way that provides a favorable trade-off between accurately projecting the hyperspectral image and the time required for data collection. We apply nonnegative matrix factorization (NMF) to decompose hyperspectral astronomical datacubes into eigenspectra and eigenimages that allow time-efficient projection with the CHIP. Included is a brief analysis of NMF parameters that affect accuracy, including the number of eigenspectra and eigenimages used to approximate the hyperspectral image to be projected. For the chosen field, the normalized mean squared synthesis error is under 0.01 with just 8 eigenspectra. NMF of hyperspectral astronomical fields better utilizes the CHIP's capabilities, providing time-efficient and accurate representations of astronomical scenes to be imaged with the WIIT."
Onboard utilization of ground control points for image correction.  Volume 2:  Analysis and simulation results,66.284515,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"An approach to remote sensing that meets future mission requirements was investigated. The deterministic acquisition of data and the rapid correction of data for radiometric effects and image distortions are the most critical limitations of remote sensing. The following topics are discussed: onboard image correction systems, GCP navigation system simulation, GCP analysis, and image correction analysis measurement."
Image Discrimination Models for Object Detection in Natural Backgrounds,66.22279,computed image estimate,['Earth Resources and Remote Sensing'],"This paper reviews work accomplished and in progress at NASA Ames relating to visual target detection. The focus is on image discrimination models, starting with Watson's pioneering development of a simple spatial model and progressing through this model's descendents and extensions. The application of image discrimination models to target detection will be described and results reviewed for Rohaly's vehicle target data and the Search 2 data. The paper concludes with a description of work we have done to model the process by which observers learn target templates and methods for elucidating those templates."
Numerical computation of echograms in a room:  Application to determination of intelligibility,66.18745,computed image estimate,"['PHYSICS, GENERAL']","Echogram recordings are used to estimate the acoustic quality of a room. Using two methods based on geometric acoustics, echograms are computed for rooms of irregular shape. The image method and the ray method have been applied to a heptahedral room as well as a conference hall. The results obtained, which have been verified experimentally, permit an estimate of the intelligibility coefficient."
Onboard utilization of ground control points for image correction.  Volume 1:  Executive summary,66.01849,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Operation of a navigation system, centered around image correction, was simulated and the system performance was analyzed. Onboard utilization of ground control points for image correction is summarized. Simulation results, and recommendations for future mission requirements are presented."
Real-time optical image processing techniques,65.79772,computed image estimate,['OPTICS'],"Nonlinear real-time optical processing on spatial pulse frequency modulation has been pursued through the analysis, design, and fabrication of pulse frequency modulated halftone screens and the modification of micro-channel spatial light modulators (MSLMs). Micro-channel spatial light modulators are modified via the Fabry-Perot method to achieve the high gamma operation required for non-linear operation. Real-time nonlinear processing was performed using the halftone screen and MSLM. The experiments showed the effectiveness of the thresholding and also showed the needs of higher SBP for image processing. The Hughes LCLV has been characterized and found to yield high gamma (about 1.7) when operated in low frequency and low bias mode. Cascading of two LCLVs should also provide enough gamma for nonlinear processing. In this case, the SBP of the LCLV is sufficient but the uniformity of the LCLV needs improvement. These include image correlation, computer generation of holograms, pseudo-color image encoding for image enhancement, and associative-retrieval in neural processing. The discovery of the only known optical method for dynamic range compression of an input image in real-time by using GaAs photorefractive crystals is reported. Finally, a new architecture for non-linear multiple sensory, neural processing has been suggested."
LANDSAT (MSS):  Image demographic estimations,65.40106,computed image estimate,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Two sets of urban test sites, one with 35 cities and one with 70 cities, were selected in the State, Sao Paulo. A high degree of colinearity (0.96) was found between urban and areal measurements taken from aerial photographs and LANDSAT MSS imagery. High coefficients were observed when census data were regressed against aerial information (0.95) and LANDSAT data (0.92). The validity of population estimations was tested by regressing three urban variables, against three classes of cities. Results supported the effectiveness of LANDSAT to estimate large city populations with diminishing effectiveness as urban areas decrease in size."
Ionospheric Slant Total Electron Content Analysis Using Global Positioning System Based Estimation,64.98364,computed image estimate,['Computer Programming and Software'],"A method, system, apparatus, and computer program product provide the ability to analyze ionospheric slant total electron content (TEC) using global navigation satellite systems (GNSS)-based estimation. Slant TEC is estimated for a given set of raypath geometries by fitting historical GNSS data to a specified delay model. The accuracy of the specified delay model is estimated by computing delay estimate residuals and plotting a behavior of the delay estimate residuals. An ionospheric threat model is computed based on the specified delay model. Ionospheric grid delays (IGDs) and grid ionospheric vertical errors (GIVEs) are computed based on the ionospheric threat model."
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,193.34854,computed jpeg quality,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
JPEG 2000 Encoding with Perceptual Distortion Control,157.8546,computed jpeg quality,['Man/System Technology and Life Support'],"An alternative approach has been devised for encoding image data in compliance with JPEG 2000, the most recent still-image data-compression standard of the Joint Photographic Experts Group. Heretofore, JPEG 2000 encoding has been implemented by several related schemes classified as rate-based distortion-minimization encoding. In each of these schemes, the end user specifies a desired bit rate and the encoding algorithm strives to attain that rate while minimizing a mean squared error (MSE). While rate-based distortion minimization is appropriate for transmitting data over a limited-bandwidth channel, it is not the best approach for applications in which the perceptual quality of reconstructed images is a major consideration. A better approach for such applications is the present alternative one, denoted perceptual distortion control, in which the encoding algorithm strives to compress data to the lowest bit rate that yields at least a specified level of perceptual image quality. Some additional background information on JPEG 2000 is prerequisite to a meaningful summary of JPEG encoding with perceptual distortion control. The JPEG 2000 encoding process includes two subprocesses known as tier-1 and tier-2 coding. In order to minimize the MSE for the desired bit rate, a rate-distortion- optimization subprocess is introduced between the tier-1 and tier-2 subprocesses. In tier-1 coding, each coding block is independently bit-plane coded from the most-significant-bit (MSB) plane to the least-significant-bit (LSB) plane, using three coding passes (except for the MSB plane, which is coded using only one ""clean up"" coding pass). For M bit planes, this subprocess involves a total number of (3M - 2) coding passes. An embedded bit stream is then generated for each coding block. Information on the reduction in distortion and the increase in the bit rate associated with each coding pass is collected. This information is then used in a rate-control procedure to determine the contribution of each coding block to the output compressed bit stream."
Image coding by way of wavelets,84.6288,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"The application of two wavelet transforms to image compression is discussed. It is noted that the Haar transform, with proper bit allocation, has performance that is visually superior to an algorithm based on a Daubechies filter and to the discrete cosine transform based Joint Photographic Experts Group (JPEG) algorithm at compression ratios exceeding 20:1. In terms of the root-mean-square error, the performance of the Haar transform method is basically comparable to that of the JPEG algorithm. The implementation of the Haar transform can be achieved in integer arithmetic, making it very suitable for applications requiring real-time performance."
Method and Apparatus for Evaluating the Visual Quality of Processed Digital Video Sequences,72.266655,computed jpeg quality,['Electronics and Electrical Engineering'],"A Digital Video Quality (DVQ) apparatus and method that incorporate a model of human visual sensitivity to predict the visibility of artifacts. The DVQ method and apparatus are used for the evaluation of the visual quality of processed digital video sequences and for adaptively controlling the bit rate of the processed digital video sequences without compromising the visual quality. The DVQ apparatus minimizes the required amount of memory and computation. The input to the DVQ apparatus is a pair of color image sequences: an original (R) non-compressed sequence, and a processed (T) sequence. Both sequences (R) and (T) are sampled, cropped, and subjected to color transformations. The sequences are then subjected to blocking and discrete cosine transformation, and the results are transformed to local contrast. The next step is a time filtering operation which implements the human sensitivity to different time frequencies. The results are converted to threshold units by dividing each discrete cosine transform coefficient by its respective visual threshold. At the next stage the two sequences are subtracted to produce an error sequence. The error sequence is subjected to a contrast masking operation, which also depends upon the reference sequence (R). The masked errors can be pooled in various ways to illustrate the perceptual error over various dimensions, and the pooled error can be converted to a visual quality measure."
"MODIS Land Data Products: Generation, Quality Assurance and Validation",70.64564,computed jpeg quality,['Earth Resources and Remote Sensing'],"The Moderate Resolution Imaging Spectrometer (MODIS) on-board NASA's Earth Observing System (EOS) Terra and Aqua Satellites are key instruments for providing data on global land, atmosphere, and ocean dynamics. Derived MODIS land, atmosphere and ocean products are central to NASA's mission to monitor and understand the Earth system. NASA has developed and generated on a systematic basis a suite of MODIS products starting with the first Terra MODIS data sensed February 22, 2000 and continuing with the first MODIS-Aqua data sensed July 2, 2002. The MODIS Land products are divided into three product suites: radiation budget products, ecosystem products, and land cover characterization products. The production and distribution of the MODIS Land products are described, from initial software delivery by the MODIS Land Science Team, to operational product generation and quality assurance, delivery to EOS archival and distribution centers, and product accuracy assessment and validation. Progress and lessons learned since the first MODIS data were in early 2000 are described."
A visual detection model for DCT coefficient quantization,69.76508,computed jpeg quality,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
High performance compression of science data,68.23213,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
High Performance Compression of Science Data,66.474525,computed jpeg quality,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
"NASA Tech Briefs, September 2008",65.633354,computed jpeg quality,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
A comparison of model-based VQ compression with other VQ approaches,61.08596,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"In our previous work on Model-Based Vector Quantization (MVQ), we presented some performance comparisons (both rate distortion and decompression time) with VQ and JPEG/DCT. In this paper, we compare the MVQ's rate distortion performance with Mean Removed Vector Quantization (MRVQ) and include our previous comparison with VQ. MVQ is similar to MRVQ in many ways. Both of these techniques extract means of the vectors (raster-scanned image blocks) and reduce them to mean removed residuals by subtracting block means from the elements of the vectors. In the case of MRVQ, a codebook of residual vectors is generated using a training set. For every vector from the input image, the block mean and address of the codevector from the codebook that matches the input vector closest are transmitted to the decoder. The codebook is generated using generalized Lloyd algorithm on training set of residual vectors. For MVQ the pairs consist of vector means and address of the closest matching vector from codebook generated by models based on statistical properties of the residuals and Human Visual System (HVS). In our experiments, we found that MVQ performance in rate distortion sense is almost always better than VQ and is comparable to MRVQ. Further, MVQ is much easier to use than either VQ or MRVQ, since the training and managing of explicit codebooks is not required."
Improved image decompression for reduced transform coding artifacts,52.167675,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
"Model-based VQ for image data archival, retrieval and distribution",48.999336,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"An ideal image compression technique for image data archival, retrieval and distribution would be one with the asymmetrical computational requirements of Vector Quantization (VQ), but without the complications arising from VQ codebooks. Codebook generation and maintenance are stumbling blocks which have limited the use of VQ as a practical image compression algorithm. Model-based VQ (MVQ), a variant of VQ described here, has the computational properties of VQ but does not require explicit codebooks. The codebooks are internally generated using mean removed error and Human Visual System (HVS) models. The error model assumed is the Laplacian distribution with mean, lambda-computed from a sample of the input image. A Laplacian distribution with mean, lambda, is generated with uniform random number generator. These random numbers are grouped into vectors. These vectors are further conditioned to make them perceptually meaningful by filtering the DCT coefficients from each vector. The DCT coefficients are filtered by multiplying by a weight matrix that is found to be optimal for human perception. The inverse DCT is performed to produce the conditioned vectors for the codebook. The only image dependent parameter used in the generation of codebook is the mean, lambda, that is included in the coded file to repeat the codebook generation process for decoding."
Synthetic aperture radar signal data compression using block adaptive quantization,48.6355,computed jpeg quality,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
Methods of evaluating the effects of coding on SAR data,47.75306,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"It is recognized that mean square error (MSE) is not a sufficient criterion for determining the acceptability of an image reconstructed from data that has been compressed and decompressed using an encoding algorithm. In the case of Synthetic Aperture Radar (SAR) data, it is also deemed to be insufficient to display the reconstructed image (and perhaps error image) alongside the original and make a (subjective) judgment as to the quality of the reconstructed data. In this paper we suggest a number of additional evaluation criteria which we feel should be included as evaluation metrics in SAR data encoding experiments. These criteria have been specifically chosen to provide a means of ensuring that the important information in the SAR data is preserved. The paper also presents the results of an investigation into the effects of coding on SAR data fidelity when the coding is applied in (1) the signal data domain, and (2) the image domain. An analysis of the results highlights the shortcomings of the MSE criterion, and shows which of the suggested additional criterion have been found to be most important."
Compressed/reconstructed test images for CRAF/Cassini,46.62382,computed jpeg quality,['ASTROPHYSICS'],"A set of compressed, then reconstructed, test images submitted to the Comet Rendezvous Asteroid Flyby (CRAF)/Cassini project is presented as part of its evaluation of near lossless high compression algorithms for representing image data. A total of seven test image files were provided by the project. The seven test images were compressed, then reconstructed with high quality (root mean square error of approximately one or two gray levels on an 8 bit gray scale), using discrete cosine transforms or Hadamard transforms and efficient entropy coders. The resulting compression ratios varied from about 2:1 to about 10:1, depending on the activity or randomness in the source image. This was accomplished without any special effort to optimize the quantizer or to introduce special postprocessing to filter the reconstruction errors. A more complete set of measurements, showing the relative performance of the compression algorithms over a wide range of compression ratios and reconstruction errors, shows that additional compression is possible at a small sacrifice in fidelity."
Sub-band/transform compression of video sequences,43.44193,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Photographic Volume Estimation of CPAS Main Parachutes,43.324974,computed jpeg quality,['Aerodynamics'],"Capsule Parachute Assembly System (CPAS) flight tests regularly stage a helicopter to observe inflation of 116 ft D o ringsail Main parachutes. These side views can be used to generate 3-D models of inflating canopies to estimate enclosed volume. Assuming a surface of revolution is inadequate because reefed canopies in a cluster are elongated due to mutual aerodynamic interference. A method was developed to combine the side views with upward looking HD video to account for non-circular cross sections. Approximating the cross sections as elliptical greatly improves accuracy. But since that correction requires manually tracing projected outlines, the actual irregular shapes can be used to generate high fidelity models. Compensation is also made for apparent tilt angle. Validation was accomplished by comparing perimeter and projected area with known line lengths and/or high quality photogrammetry. "
Clementine High Resolution Camera Mosaicking Project,42.808266,computed jpeg quality,['Lunar and Planetary Science and Exploration'],"This report constitutes the final report for NASA Contract NASW-5054. This project processed Clementine I high resolution images of the Moon, mosaicked these images together, and created a 22-disk set of compact disk read-only memory (CD-ROM) volumes. The mosaics were produced through semi-automated registration and calibration of the high resolution (HiRes) camera's data against the geometrically and photometrically controlled Ultraviolet/Visible (UV/Vis) Basemap Mosaic produced by the US Geological Survey (USGS). The HiRes mosaics were compiled from non-uniformity corrected, 750 nanometer (""D"") filter high resolution nadir-looking observations. The images were spatially warped using the sinusoidal equal-area projection at a scale of 20 m/pixel for sub-polar mosaics (below 80 deg. latitude) and using the stereographic projection at a scale of 30 m/pixel for polar mosaics. Only images with emission angles less than approximately 50 were used. Images from non-mapping cross-track slews, which tended to have large SPICE errors, were generally omitted. The locations of the resulting image population were found to be offset from the UV/Vis basemap by up to 13 km (0.4 deg.). Geometric control was taken from the 100 m/pixel global and 150 m/pixel polar USGS Clementine Basemap Mosaics compiled from the 750 nm Ultraviolet/Visible Clementine imaging system. Radiometric calibration was achieved by removing the image nonuniformity dominated by the HiRes system's light intensifier. Also provided are offset and scale factors, achieved by a fit of the HiRes data to the corresponding photometrically calibrated UV/Vis basemap, that approximately transform the 8-bit HiRes data to photometric units. The sub-polar mosaics are divided into tiles that cover approximately 1.75 deg. of latitude and span the longitude range of the mosaicked frames. Images from a given orbit are map projected using the orbit's nominal central latitude. Polar mosaics are tiled into squares 2250 pixels on a side, which spans approximately 2.2 deg. Two mosaics are provided for each pole: one corresponding to data acquired while periapsis was in the south, the other while periapsis was in the north. The CD-ROMs also contain ancillary data files that support the HiRes mosaic. These files include browse images with UV/Vis context stored in a Joint Photographic Experts Group (JPEG) format, index files ('imgindx.tab' and 'srcindx.tab') that tabulate the contents of the CD, and documentation files."
1994 Science Information Management and Data Compression Workshop,37.26449,computed jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Aerospace Technology Innovation,37.003082,computed jpeg quality,['Aerospace Medicine'],"Whether finding new applications for existing NASA technologies or developing unique marketing strategies to demonstrate them, NASA's offices are committed to identifying unique partnering opportunities. Through their efforts NASA leverages resources through joint research and development, and gains new insight into the core areas relevant to all NASA field centers. One of the most satisfying aspects of my job comes when I learn of a mission-driven technology that can be spun-off to touch the lives of everyday people. NASA's New Partnerships in Medical Diagnostic Imaging is one such initiative. Not only does it promise to provide greater dividends for the country's investment in aerospace research, but also to enhance the American quality of life. This issue of Innovation highlights the new NASA-sponsored initiative in medical imaging. Early in 2001, NASA announced the launch of the New Partnerships in Medical Diagnostic Imaging initiative to promote the partnership and commercialization of NASA technologies in the medical imaging industry. NASA and the medical imaging industry share a number of crosscutting technologies in areas such as high-performance detectors and image-processing tools. Many of the opportunities for joint development and technology transfer to the medical imaging market also hold the promise for future spin back to NASA."
Fractal image compression: A resolution independent representation for imagery,36.377384,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"A deterministic fractal is an image which has low information content and no inherent scale. Because of their low information content, deterministic fractals can be described with small data sets. They can be displayed at high resolution since they are not bound by an inherent scale. A remarkable consequence follows. Fractal images can be encoded at very high compression ratios. This fern, for example is encoded in less than 50 bytes and yet can be displayed at resolutions with increasing levels of detail appearing. The Fractal Transform was discovered in 1988 by Michael F. Barnsley. It is the basis for a new image compression scheme which was initially developed by myself and Michael Barnsley at Iterated Systems. The Fractal Transform effectively solves the problem of finding a fractal which approximates a digital 'real world image'."
Video transmission on ATM networks,36.243572,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
The 1993 Space and Earth Science Data Compression Workshop,36.207012,computed jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
Toward Better Modeling of Supercritical Turbulent Mixing,35.883457,computed jpeg quality,['Man/System Technology and Life Support'],"study was done as part of an effort to develop computational models representing turbulent mixing under thermodynamic supercritical (here, high pressure) conditions. The question was whether the large-eddy simulation (LES) approach, developed previously for atmospheric-pressure compressible-perfect-gas and incompressible flows, can be extended to real-gas non-ideal (including supercritical) fluid mixtures. [In LES, the governing equations are approximated such that the flow field is spatially filtered and subgrid-scale (SGS) phenomena are represented by models.] The study included analyses of results from direct numerical simulation (DNS) of several such mixing layers based on the Navier-Stokes, total-energy, and conservation- of-chemical-species governing equations. Comparison of LES and DNS results revealed the need to augment the atmospheric- pressure LES equations with additional SGS momentum and energy terms. These new terms are the direct result of high-density-gradient-magnitude regions found in the DNS and observed experimentally under fully turbulent flow conditions. A model has been derived for the new term in the momentum equation and was found to perform well at small filter size but to deteriorate with increasing filter size. Several alternative models were derived for the new SGS term in the energy equation that would need further investigations to determine if they are too computationally intensive in LES."
Image Registration Workshop Proceedings,35.86122,computed jpeg quality,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Compression of color-mapped images,35.55073,computed jpeg quality,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
The Space and Earth Science Data Compression Workshop,35.339684,computed jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
Studies on image compression and image reconstruction,34.98408,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",34.73559,computed jpeg quality,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
Conditional Entropy-Constrained Residual VQ with Application to Image Coding,34.502842,computed jpeg quality,['Computer Programming and Software'],"This paper introduces an extension of entropy-constrained residual vector quantization (VQ) where intervector dependencies are exploited. The method, which we call conditional entropy-constrained residual VQ, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. When applied to coding images, the proposed method is shown to achieve better rate-distortion performance than that of entropy-constrained residual vector quantization with less computational complexity and lower memory requirements. Moreover, it can be designed to support progressive transmission in a natural way. It is also shown to outperform some of the best predictive and finite-state VQ techniques reported in the literature. This is due partly to the joint optimization between the residual vector quantizer and a high-order conditional entropy coder as well as the efficiency of the multistage residual VQ structure and the dynamic nature of the prediction."
The 1995 Science Information Management and Data Compression Workshop,34.401573,computed jpeg quality,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,34.357796,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
A High Performance Image Data Compression Technique for Space Applications,33.870422,computed jpeg quality,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
Distant Operational Care Centre: Design Project Report,33.634956,computed jpeg quality,['Astronautics (General)'],"The goal of this project is to outline the design of the Distant Operational Care Centre (DOCC), a modular medical facility to maintain human health and performance in space, that is adaptable to a range of remote human habitats. The purpose of this project is to outline a design, not to go into a complete technical specification of a medical facility for space. This project involves a process to produce a concise set of requirements, addressing the fundamental problems and issues regarding all aspects of a space medical facility for the future. The ideas presented here are at a high level, based on existing, researched, and hypothetical technologies. Given the long development times for space exploration, the outlined concepts from this project embodies a collection of identified problems, and corresponding proposed solutions and ideas, ready to contribute to future space exploration efforts. In order to provide a solid extrapolation and speculation in the context of the future of space medicine, the extent of this project's vision is roughly within the next two decades. The Distant Operational Care Centre (DOCC) is a modular medical facility for space. That is, its function is to maintain human health and performance in space environments, through prevention, diagnosis, and treatment. Furthermore, the DOCC must be adaptable to meet the environmental requirements of different remote human habitats, and support a high quality of human performance. To meet a diverse range of remote human habitats, the DOCC concentrates on a core medical capability that can then be adapted. Adaptation would make use of the DOCC's functional modularity, providing the ability to replace, add, and modify core functions of the DOCC by updating hardware, operations, and procedures. Some of the challenges to be addressed by this project include what constitutes the core medical capability in terms of hardware, operations, and procedures, and how DOCC can be adapted to different remote habitats."
Introduction to Image Processing,33.491566,computed jpeg quality,['Instrumentation and Photography'],No abstract available
SEDSAT-1 Technology Development,33.41149,computed jpeg quality,"['Spacecraft Design, Testing and Performance']","The Students for the Exploration and Development of Space Satellite (SEDSAT-1) is an ambitious project to design, build, and fly a generally-accessible low-cost satellite which will 1) act as a technology demonstration to verify the suitability of novel optical, battery, microprocessor, and memory hardware for space flight environments, (2) to advance the understanding of tether dynamics and environmental science through the development of advanced imaging experiments, (3) to act as a communication link for radio amateurs, and (4) to provide graduate and undergraduate students with a unique multi-disciplinary experience in designing complex real-world hardware/software. This report highlights the progress made on this project during the time period from January 2, 1996 to June 1, 1996 at the end of which time the SEASIS 0.7 version software was completed and integrated on the SEASIS breadboard, a functional prototype of the Panoramic Annual Lenses (PAL) camera was developed, the preferred image compression technique was selected, the layout of the SEASIS board was begun, porting of the SCOS operating system to the command data system (CDS) board was begun, a new design for a tether release mechanism was developed, safety circuitry to inhibit tether cutting was developed and prototyped, material was prepared to support a comprehensive safety review of the project which was held at Johnson Space Center (JSC) (which was personally attended by one of the Principal Investigators), and prototype ground software was developed."
Subband Image Coding with Jointly Optimized Quantizers,33.320724,computed jpeg quality,['Computer Programming and Software'],"An iterative design algorithm for the joint design of complexity- and entropy-constrained subband quantizers and associated entropy coders is proposed. Unlike conventional subband design algorithms, the proposed algorithm does not require the use of various bit allocation algorithms. Multistage residual quantizers are employed here because they provide greater control of the complexity-performance tradeoffs, and also because they allow efficient and effective high-order statistical modeling. The resulting subband coder exploits statistical dependencies within subbands, across subbands, and across stages, mainly through complexity-constrained high-order entropy coding. Experimental results demonstrate that the complexity-rate-distortion performance of the new subband coder is exceptional."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,32.078003,computed jpeg quality,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
The CCDS Data Compression Recommendations: Development and Status,31.511929,computed jpeg quality,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
"Robo-line storage: Low latency, high capacity storage systems over geographically distributed networks",31.36307,computed jpeg quality,['COMPUTER OPERATIONS AND HARDWARE'],"Rapid advances in high performance computing are making possible more complete and accurate computer-based modeling of complex physical phenomena, such as weather front interactions, dynamics of chemical reactions, numerical aerodynamic analysis of airframes, and ocean-land-atmosphere interactions. Many of these 'grand challenge' applications are as demanding of the underlying storage system, in terms of their capacity and bandwidth requirements, as they are on the computational power of the processor. A global view of the Earth's ocean chlorophyll and land vegetation requires over 2 terabytes of raw satellite image data. In this paper, we describe our planned research program in high capacity, high bandwidth storage systems. The project has four overall goals. First, we will examine new methods for high capacity storage systems, made possible by low cost, small form factor magnetic and optical tape systems. Second, access to the storage system will be low latency and high bandwidth. To achieve this, we must interleave data transfer at all levels of the storage system, including devices, controllers, servers, and communications links. Latency will be reduced by extensive caching throughout the storage hierarchy. Third, we will provide effective management of a storage hierarchy, extending the techniques already developed for the Log Structured File System. Finally, we will construct a protototype high capacity file server, suitable for use on the National Research and Education Network (NREN). Such research must be a Cornerstone of any coherent program in high performance computing and communications."
"Denoising with Three Dimensional Fourier Transform for Three Dimensional Images, Including Image Sequences",31.291426,computed jpeg quality,['Instrumentation and Photography'],"A method of mitigating noise in source image data representing pixels of a 3-D image. The ""3-D image"" may be any type of 3-D image, regardless of whether the third dimension is spatial, temporal, or some other parameter. The 3-D image is divided into three-dimensional chunks of pixels. These chunks are apodized and a three-dimensional Fourier transform is performed on each chunk, thereby producing a three-dimensional spectrum of each chunk. The transformed chunks are processed to estimate a noise floor based on spectral values of the pixels within each chunk. A noise threshold is then determined, and the spectrum of each chunk is filtered with a denoising filter based on the noise threshold. The chunks are then inverse transformed, and recombined into a denoised 3-D image."
Study and simulation of low rate video coding schemes,30.860725,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
Data compression for full motion video transmission,30.444862,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
The 1992 4th NASA SERC Symposium on VLSI Design,29.349224,computed jpeg quality,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"Papers from the fourth annual NASA Symposium on VLSI Design, co-sponsored by the IEEE, are presented. Each year this symposium is organized by the NASA Space Engineering Research Center (SERC) at the University of Idaho and is held in conjunction with a quarterly meeting of the NASA Data System Technology Working Group (DSTWG). One task of the DSTWG is to develop new electronic technologies that will meet next generation electronic data system needs. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The NASA SERC is proud to offer, at its fourth symposium on VLSI design, presentations by an outstanding set of individuals from national laboratories, the electronics industry, and universities. These speakers share insights into next generation advances that will serve as a basis for future VLSI design."
ARES Biennial Report 2012 Final,29.209301,computed jpeg quality,['Ground Support Systems and Facilities (Space)'],"Since the return of the first lunar samples, what is now the Astromaterials Research and Exploration Science (ARES) Directorate has had curatorial responsibility for all NASA-held extraterrestrial materials. Originating during the Apollo Program (1960s), this capability at Johnson Space Center (JSC) included scientists who were responsible for the science planning and training of astronauts for lunar surface activities as well as experts in the analysis and preservation of the precious returned samples. Today, ARES conducts research in basic and applied space and planetary science, and its scientific staff represents a broad diversity of expertise in the physical sciences (physics, chemistry, geology, astronomy), mathematics, and engineering organized into three offices (figure 1): Astromaterials Research (KR), Astromaterials Acquisition and Curation (KT), and Human Exploration Science (KX). Scientists within the Astromaterials Acquisition and Curation Office preserve, protect, document, and distribute samples of the current astromaterials collections. Since the return of the first lunar samples, ARES has been assigned curatorial responsibility for all NASA-held extraterrestrial materials (Apollo lunar samples, Antarctic meteorites - some of which have been confirmed to have originated on the Moon and on Mars - cosmic dust, solar wind samples, comet and interstellar dust particles, and space-exposed hardware). The responsibilities of curation consist not only of the longterm care of the samples, but also the support and planning for future sample collection missions and research and technology to enable new sample types. Curation provides the foundation for research into the samples. The Lunar Sample Facility and other curation clean rooms, the data center, laboratories, and associated instrumentation are unique NASA resources that, together with our staff's fundamental understanding of the entire collection, provide a service to the external research community, which relies on access to the samples. The curation efforts are greatly enhanced by a strong group of planetary scientists who conduct peerreviewed astromaterials research. Astromaterials Research Office scientists conduct peer-reviewed research as Principal or Co-Investigators in planetary science (e. g., cosmochemistry, origins of solar systems, Mars fundamental research, planetary geology and geophysics) and participate as Co-Investigators or Participating Scientists in many of NASA's robotic planetary missions. Since the last report, ARES has achieved several noteworthy milestones, some of which are documented in detail in the sections that follow. Within the Human Exploration Science Office, ARES is a world leader in orbital debris research, modeling and monitoring the debris environment, designing debris shielding, and developing policy to control and mitigate the orbital debris population. ARES has aggressively pursued refinements in knowledge of the debris environment and the hazard it presents to spacecraft. Additionally, the ARES Image Science and Analysis Group has been recognized as world class as a result of the high quality of near-real-time analysis of ascent and on-orbit inspection imagery to identify debris shedding, anomalies, and associated potential damage during Space Shuttle missions. ARES Earth scientists manage and continuously update the database of astronaut photography that is predominantly from Shuttle and ISS missions, but also includes the results of 40 years of human spaceflight. The Crew Earth Observations Web site (http://eol.jsc.nasa.gov/Education/ESS/crew.htm) continues to receive several million hits per month. ARES scientists are also influencing decisions in the development of the next generation of human and robotic spacecraft and missions through laboratory tests on the optical qualities of materials for windows, micrometeoroid/orbital debris shielding technology, and analog activities to assess surface science operations. ARES serves as host to numerous students and visiting scientists as part of the services provided to the research community and conducts a robust education and outreach program. ARES scientists are recognized nationally and internationally by virtue of their success in publishing in peer-reviewed journals and winning competitive research proposals. ARES scientists have won every major award presented by the Meteoritical Society, including the Leonard Medal, the most prestigious award in planetary science and cosmochemistry; the Barringer Medal, recognizing outstanding work in the field of impact cratering; the Nier Prize for outstanding research by a young scientist; and several recipients of the Nininger Meteorite Award. One of our scientists received the Department of Defense (DoD) Joint Meritorious Civilian Service Award (the highest civilian honor given by the DoD). ARES has established numerous partnerships with other NASA Centers, universities, and national laboratories. ARES scientists serve as journal editors, members of advisory panels and review committees, and society officers, and several scientists have been elected as Fellows in their professional societies. This biennial report summarizes a subset of the accomplishments made by each of the ARES offices and highlights participation in ongoing human and robotic missions, development of new missions, and planning for future human and robotic exploration of the solar system beyond low Earth orbit."
Studies and simulations of the DigiCipher system,29.19802,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this period the development of simulators for the various high definition television (HDTV) systems proposed to the FCC was continued. The FCC has indicated that it wants the various proposers to collaborate on a single system. Based on all available information this system will look very much like the advanced digital television (ADTV) system with major contributions only from the DigiCipher system. The results of our simulations of the DigiCipher system are described. This simulator was tested using test sequences from the MPEG committee. The results are extrapolated to HDTV video sequences. Once again, some caveats are in order. The sequences used for testing the simulator and generating the results are those used for testing the MPEG algorithm. The sequences are of much lower resolution than the HDTV sequences would be, and therefore the extrapolations are not totally accurate. One would expect to get significantly higher compression in terms of bits per pixel with sequences that are of higher resolution. However, the simulator itself is a valid one, and should HDTV sequences become available, they could be used directly with the simulator. A brief overview of the DigiCipher system is given. Some coding results obtained using the simulator are looked at. These results are compared to those obtained using the ADTV system. These results are evaluated in the context of the CCSDS specifications and make some suggestions as to how the DigiCipher system could be implemented in the NASA network. Simulations such as the ones reported can be biased depending on the particular source sequence used. In order to get more complete information about the system one needs to obtain a reasonable set of models which mirror the various kinds of sources encountered during video coding. A set of models which can be used to effectively model the various possible scenarios is provided. As this is somewhat tangential to the other work reported, the results are included as an appendix."
Emerging standards for still image compression: A software implementation and simulation study,28.97825,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],The software implementation is described of an emerging standard for the lossy compression of continuous tone still images. This software program can be used to compress planetary images and other 2-D instrument data. It provides a high compression image coding capability that preserves image fidelity at compression rates competitive or superior to most known techniques. This software implementation confirms the usefulness of such data compression and allows its performance to be compared with other schemes used in deep space missions and for data based storage.
The Telecommunications and Data Acquisition Report,28.877365,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"A compilation is presented of articles on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition. In space communications, radio navigation, radio science, and ground based radio and radar astronomy, activities of the Deep Space Network are reported in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA. In the search for extraterrestrial intelligence (SETI), implementation and operations are reported for searching the microwave spectrum."
An Accurate and Dynamic Computer Graphics Muscle Model,28.624023,computed jpeg quality,['Computer Programming and Software'],"A computer based musculo-skeletal model was developed at the University in the departments of Mechanical and Biomedical Engineering. This model accurately represents human shoulder kinematics. The result of this model is the graphical display of bones moving through an appropriate range of motion based on inputs of EMGs and external forces. The need existed to incorporate a geometric muscle model in the larger musculo-skeletal model. Previous muscle models did not accurately represent muscle geometries, nor did they account for the kinematics of tendons. This thesis covers the creation of a new muscle model for use in the above musculo-skeletal model. This muscle model was based on anatomical data from the Visible Human Project (VHP) cadaver study. Two-dimensional digital images from the VHP were analyzed and reconstructed to recreate the three-dimensional muscle geometries. The recreated geometries were smoothed, reduced, and sliced to form data files defining the surfaces of each muscle. The muscle modeling function opened these files during run-time and recreated the muscle surface. The modeling function applied constant volume limitations to the muscle and constant geometry limitations to the tendons."
"Power, Avionics and Software - Phase 1.0:",28.045431,computed jpeg quality,"['Space Communications, Spacecraft Communications, Command and Tracking', 'Computer Programming and Software']","This report describes Power, Avionics and Software (PAS) 1.0 subsystem integration testing and test results that occurred in August and September of 2013. This report covers the capabilities of each PAS assembly to meet integration test objectives for non-safety critical, non-flight, non-human-rated hardware and software development. This test report is the outcome of the first integration of the PAS subsystem and is meant to provide data for subsequent designs, development and testing of the future PAS subsystems. The two main objectives were to assess the ability of the PAS assemblies to exchange messages and to perform audio testing of both inbound and outbound channels. This report describes each test performed, defines the test, the data, and provides conclusions and recommendations."
Data compression in remote sensing applications,26.973724,computed jpeg quality,['EARTH RESOURCES AND REMOTE SENSING'],"A survey of current data compression techniques which are being used to reduce the amount of data in remote sensing applications is provided. The survey aspect is far from complete, reflecting the substantial activity in this area. The purpose of the survey is more to exemplify the different approaches being taken rather than to provide an exhaustive list of the various proposed approaches."
Data compression using Chebyshev transform,26.68714,computed jpeg quality,['Computer Programming and Software'],"The present invention is a method, system, and computer program product for implementation of a capable, general purpose compression algorithm that can be engaged on the fly. This invention has particular practical application with time-series data, and more particularly, time-series data obtained form a spacecraft, or similar situations where cost, size and/or power limitations are prevalent, although it is not limited to such applications. It is also particularly applicable to the compression of serial data streams and works in one, two, or three dimensions. The original input data is approximated by Chebyshev polynomials, achieving very high compression ratios on serial data streams with minimal loss of scientific information."
"Third International Symposium on Space Mission Operations and Ground Data Systems, part 1",26.567484,computed jpeg quality,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Under the theme of 'Opportunities in Ground Data Systems for High Efficiency Operations of Space Missions,' the SpaceOps '94 symposium included presentations of more than 150 technical papers spanning five topic areas: Mission Management, Operations, Data Management, System Development, and Systems Engineering. The papers focus on improvements in the efficiency, effectiveness, productivity, and quality of data acquisition, ground systems, and mission operations. New technology, techniques, methods, and human systems are discussed. Accomplishments are also reported in the application of information systems to improve data retrieval, reporting, and archiving; the management of human factors; the use of telescience and teleoperations; and the design and implementation of logistics support for mission operations."
In-Vacuum Photogrammetry of a 10-Meter Solar Sail,26.5654,computed jpeg quality,['Spacecraft Propulsion and Power'],"In July 2004, a 10-meter solar sail structure developed by L Garde, Inc. was tested in vacuum at the NASA Glenn 30-meter Plum Brook Space Power Facility in Sandusky, Ohio. The three main objections of the test were to demonstrate unattended deployment from a stowed configuration, to measure the deployed shape of the sail at both ambient and cryogenic room temperatures, and to measure the deployed structural dynamic characteristics (vibration modes). This paper summarizes the work conducted to fulfill the second test objective. The deployed shape was measured photogrammetrically in vacuum conditions with four 2-megapixel digital video cameras contained in custom made pressurized canisters. The canisters included high-intensity LED ring lights to illuminate a grid of retroreflective targets distributed on the solar sail. The test results closely matched pre-test photogrammetry numerical simulations and compare well with ABAQUS finite-element model predictions."
Progressive transmission and compression images,26.412802,computed jpeg quality,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
"Recent Advances in Registration, Integration and Fusion of Remotely Sensed Data: Redundant Representations and Frames",26.359442,computed jpeg quality,"['Earth Resources and Remote Sensing', 'Mathematical and Computer Sciences (General)']","In recent years, sophisticated mathematical techniques have been successfully applied to the field of remote sensing to produce significant advances in applications such as registration, integration and fusion of remotely sensed data. Registration, integration and fusion of multiple source imagery are the most important issues when dealing with Earth Science remote sensing data where information from multiple sensors, exhibiting various resolutions, must be integrated. Issues ranging from different sensor geometries, different spectral responses, differing illumination conditions, different seasons, and various amounts of noise need to be dealt with when designing an image registration, integration or fusion method. This tutorial will first define the problems and challenges associated with these applications and then will review some mathematical techniques that have been successfully utilized to solve them. In particular, we will cover topics on geometric multiscale representations, redundant representations and fusion frames, graph operators, diffusion wavelets, as well as spatial-spectral and operator-based data fusion. All the algorithms will be illustrated using remotely sensed data, with an emphasis on current and operational instruments."
Development and Application of Non-Linear Image Enhancement and Multi-Sensor Fusion Techniques for Hazy and Dark Imaging,26.311987,computed jpeg quality,['Avionics and Aircraft Instrumentation'],"The purpose of this research was to develop enhancement and multi-sensor fusion algorithms and techniques to make it safer for the pilot to fly in what would normally be considered Instrument Flight Rules (IFR) conditions, where pilot visibility is severely restricted due to fog, haze or other weather phenomenon. We proposed to use the non-linear Multiscale Retinex (MSR) as the basic driver for developing an integrated enhancement and fusion engine. When we started this research, the MSR was being applied primarily to grayscale imagery such as medical images, or to three-band color imagery, such as that produced in consumer photography: it was not, however, being applied to other imagery such as that produced by infrared image sources. However, we felt that it was possible by using the MSR algorithm in conjunction with multiple imaging modalities such as long-wave infrared (LWIR), short-wave infrared (SWIR), and visible spectrum (VIS), we could substantially improve over the then state-of-the-art enhancement algorithms, especially in poor visibility conditions. We proposed the following tasks: 1) Investigate the effects of applying the MSR to LWIR and SWIR images. This consisted of optimizing the algorithm in terms of surround scales, and weights for these spectral bands; 2) Fusing the LWIR and SWIR images with the VIS images using the MSR framework to determine the best possible representation of the desired features; 3) Evaluating different mixes of LWIR, SWIR and VIS bands for maximum fog and haze reduction, and low light level compensation; 4) Modifying the existing algorithms to work with video sequences. Over the course of the 3 year research period, we were able to accomplish these tasks and report on them at various internal presentations at NASA Langley Research Center, and in presentations and publications elsewhere. A description of the work performed under the tasks is provided in Section 2. The complete list of relevant publications during the research periods is provided in Section 5. This research also resulted in the generation of intellectual property."
SIMBIOS Project Data Processing and Analysis Results,26.257072,computed jpeg quality,['Oceanography'],"The Sensor Intercomparison and Merger for Biological and Interdisciplinary Oceanic Studies (SIMBIOS) Project is concerned with ocean color satellite sensor data intercomparison and merger for biological and interdisciplinary studies of the global oceans. Imagery from different ocean color sensors can now be processed by a single software package using the same algorithms, adjusted by different sensor spectral characteristics, and the same ancillary meteorological and environmental data. This enables cross-comparison and validation of the data derived from satellite sensors and, consequently, creates continuity in ocean color information on both the temporal and spatial scale. The next step in this process is the integration of in situ ocean and atmospheric parameters to enable cross-validation and further refinement of the ocean color methodology. The SIMBIOS Project Office accomplishments during 2000 year are summarized under satellite data processing, data product validation, SeaWiFS Bio-Optical Archive and Storage System (SeaBASS) database, supporting services, sun photometers and calibration activities, and calibration round robins. These accomplishments are described."
The Fifth NASA Symposium on VLSI Design,26.084057,computed jpeg quality,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"The fifth annual NASA Symposium on VLSI Design had 13 sessions including Radiation Effects, Architectures, Mixed Signal, Design Techniques, Fault Testing, Synthesis, Signal Processing, and other Featured Presentations. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The presentations share insights into next generation advances that will serve as a basis for future VLSI design."
Progressive Transmission and Compression of Images,26.070015,computed jpeg quality,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
STRIPE: Remote Driving Using Limited Image Data,26.05685,computed jpeg quality,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
Compression of spectral meteorological imagery,25.948713,computed jpeg quality,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression is essential to current low-earth-orbit spectral sensors with global coverage, e.g., meteorological sensors. Such sensors routinely produce in excess of 30 Gb of data per orbit (over 4 Mb/s for about 110 min) while typically limited to less than 10 Gb of downlink capacity per orbit (15 minutes at 10 Mb/s). Astro-Space Division develops spaceborne compression systems for compression ratios from as little as three to as much as twenty-to-one for high-fidelity reconstructions. Current hardware production and development at Astro-Space Division focuses on discrete cosine transform (DCT) systems implemented with the GE PFFT chip, a 32x32 2D-DCT engine. Spectral relations in the data are exploited through block mean extraction followed by orthonormal transformation. The transformation produces blocks with spatial correlation that are suitable for further compression with any block-oriented spatial compression system, e.g., Astro-Space Division's Laplacian modeler and analytic encoder of DCT coefficients."
"Technology 2003: The Fourth National Technology Transfer Conference and Exposition, volume 2",25.184162,computed jpeg quality,['GENERAL'],"Proceedings from symposia of the Technology 2003 Conference and Exposition, Dec. 7-9, 1993, Anaheim, CA, are presented. Volume 2 features papers on artificial intelligence, CAD&E, computer hardware, computer software, information management, photonics, robotics, test and measurement, video and imaging, and virtual reality/simulation."
The Telecommunications and Data Acquisition Report,24.74556,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"This quarterly publication provides archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA). In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, supporting research and technology, implementation, and operations. Also included are standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA."
Issues in the Imprecise Computation Approach to Fault Tolerance,24.438795,computed jpeg quality,['Computer Programming and Software'],"The imprecise computation technique can be used in a natural way to enhance fault tolerance. By providing a usable, approximate result whenever a failure or overload prevent the system from producing the desired, precise result, we can increase the availability of data and services, reduce the need for error-recovery operations, and minimize the costs in replication. This paper describes the domain-specific fault tolerance mechanisms that are needed to support the provision and correct usage of imprecise results for several representative application domains. The elements of an application-domain-independent architecture that can effectively integrate these domain-specific mechanisms are also described."
"Third International Symposium on Space Mission Operations and Ground Data Systems, part 2",24.348604,computed jpeg quality,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Under the theme of 'Opportunities in Ground Data Systems for High Efficiency Operations of Space Missions,' the SpaceOps '94 symposium included presentations of more than 150 technical papers spanning five topic areas: Mission Management, Operations, Data Management, System Development, and Systems Engineering. The symposium papers focus on improvements in the efficiency, effectiveness, and quality of data acquisition, ground systems, and mission operations. New technology, methods, and human systems are discussed. Accomplishments are also reported in the application of information systems to improve data retrieval, reporting, and archiving; the management of human factors; the use of telescience and teleoperations; and the design and implementation of logistics support for mission operations. This volume covers expert systems, systems development tools and approaches, and systems engineering issues."
"NASA Tech Briefs, August 2006",24.23025,computed jpeg quality,['Man/System Technology and Life Support'],"Topics covered include: Measurement and Controls Data Acquisition System IMU/GPS System Provides Position and Attitude Data Using Artificial Intelligence to Inform Pilots of Weather Fast Lossless Compression of Multispectral-Image Data Developing Signal-Pattern-Recognition Programs Implementing Access to Data Distributed on Many Processors Compact, Efficient Drive Circuit for a Piezoelectric Pump; Dual Common Planes for Time Multiplexing of Dual-Color QWIPs; MMIC Power Amplifier Puts Out 40 mW From 75 to 110 GHz; 2D/3D Visual Tracker for Rover Mast; Adding Hierarchical Objects to Relational Database General-Purpose XML-Based Information Managements; Vaporizable Scaffolds for Fabricating Thermoelectric Modules; Producing Quantum Dots by Spray Pyrolysis; Mobile Robot for Exploring Cold Liquid/Solid Environments; System Would Acquire Core and Powder Samples of Rocks; Improved Fabrication of Lithium Films Having Micron Features; Manufacture of Regularly Shaped Sol-Gel Pellets; Regulating Glucose and pH, and Monitoring Oxygen in a Bioreactor; Satellite Multiangle Spectropolarimetric Imaging of Aerosols; Interferometric System for Measuring Thickness of Sea Ice; Microscale Regenerative Heat Exchanger Protocols for Handling Messages Between Simulation Computers Statistical Detection of Atypical Aircraft Flights NASA's Aviation Safety and Modeling Project Multimode-Guided-Wave Ultrasonic Scanning of Materials Algorithms for Maneuvering Spacecraft Around Small Bodies Improved Solar-Radiation-Pressure Models for GPS Satellites Measuring Attitude of a Large, Flexible, Orbiting Structure"
"NASA Tech Briefs, December 1999",23.981195,computed jpeg quality,['Man/System Technology and Life Support'],Topics include: Imaging/Videos/Cameras; Electronic Components and Circuits; Electronic Systems; Physical Sciences; Materials; Computer Programs; Mechanics; Machinery/Automation; Books and Reports.
Locally adaptive vector quantization: Data compression with feature preservation,23.642664,computed jpeg quality,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
Pathfinder Photogrammetry Research for Ultra-Lightweight and Inflatable Space Structures,23.461266,computed jpeg quality,['Structural Mechanics'],"The defining characteristic of ultra-lightweight and inflatable space structures is that they are both very large and very low mass. This makes standard contacting methods of measurement (e.g. attaching accelerometers) impractical because the dynamics of the structure would be changed by the mass of the contacting instrument. Optical measurements are therefore more appropriate. Photogrammetry is a leading candidate for the optical analysis of gossamer structures because it allows for the measurement of a large number of points, is amenable to time sequences, and offers the potential for a high degree of accuracy. The purpose of this thesis is to develop the methodology and determine the effectiveness of a photogrammetry system in measuring ultra-lightweight and inflatable space structures. The results of this thesis will be considered in the design of an automated photogrammetry system for the l6m-diameter vacuum chamber at the NASA Langley Research Center."
Buckets: Smart Objects for Digital Libraries,22.36242,computed jpeg quality,['Documentation and Information Science'],"Current discussion of digital libraries (DLs) is often dominated by the merits of the respective storage, search and retrieval functionality of archives, repositories, search engines, search interfaces and database systems. While these technologies are necessary for information management, the information content is more important than the systems used for its storage and retrieval. Digital information should have the same long-term survivability prospects as traditional hardcopy information and should be protected to the extent possible from evolving search engine technologies and vendor vagaries in database management systems. Information content and information retrieval systems should progress on independent paths and make limited assumptions about the status or capabilities of the other. Digital information can achieve independence from archives and DL systems through the use of buckets. Buckets are an aggregative, intelligent construct for publishing in DLs. Buckets allow the decoupling of information content from information storage and retrieval. Buckets exist within the Smart Objects and Dumb Archives model for DLs in that many of the functionalities and responsibilities traditionally associated with archives are pushed down (making the archives dumber) into the buckets (making them smarter). Some of the responsibilities imbued to buckets are the enforcement of their terms and conditions, and maintenance and display of their contents."
The Telecommunications and Data Acquisition Report,22.335869,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"Archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA) are provided. In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information. In the search for extraterrestrial intelligence (SETI), the TDA Progress Report reports on implementation and operations for searching the microwave spectrum. Topics covered include tracking and ground-based navigation; communications, spacecraft-ground; station control and system technology; capabilities for new projects; network upgrade and sustaining; network operations and operations support; and TDA program management and analysis."
The Sensor Test for Orion RelNav Risk Mitigation (STORRM) Development Test Objective,22.128838,computed jpeg quality,"['Spacecraft Design, Testing and Performance']","The Sensor Test for Orion Relative-Navigation Risk Mitigation (STORRM) Development Test Objective (DTO) flew aboard the Space Shuttle Endeavour on STS-134 in May- June 2011, and was designed to characterize the performance of the flash LIDAR and docking camera being developed for the Orion Multi-Purpose Crew Vehicle. The flash LIDAR, called the Vision Navigation Sensor (VNS), will be the primary navigation instrument used by the Orion vehicle during rendezvous, proximity operations, and docking. The DC will be used by the Orion crew for piloting cues during docking. This paper provides an overview of the STORRM test objectives and the concept of operations. It continues with a description of STORRM's major hardware components, which include the VNS, docking camera, and supporting avionics. Next, an overview of crew and analyst training activities will describe how the STORRM team prepared for flight. Then an overview of in-flight data collection and analysis is presented. Key findings and results from this project are summarized. Finally, the paper concludes with lessons learned from the STORRM DTO."
Data Understanding Applied to Optimization,22.094397,computed jpeg quality,['Computer Programming and Software'],"The goal of this research is to explore and develop software for supporting visualization and data analysis of search and optimization. Optimization is an ever-present problem in science. The theory of NP-completeness implies that the problems can only be resolved by increasingly smarter problem specific knowledge, possibly for use in some general purpose algorithms. Visualization and data analysis offers an opportunity to accelerate our understanding of key computational bottlenecks in optimization and to automatically tune aspects of the computation for specific problems. We will prototype systems to demonstrate how data understanding can be successfully applied to problems characteristic of NASA's key science optimization tasks, such as central tasks for parallel processing, spacecraft scheduling, and data transmission from a remote satellite."
Applied Meteorology Unit (AMU),22.046642,computed jpeg quality,['Meteorology and Climatology'],"Develop climatologies of gridded CG lightning densities and frequencies of occurrence for the Melbourne, FL National Weather Service (NWS MLB) county warning area. These grids are used to create a first-guess field for the lightning threat index map that is available on the NWS MLB NASA KSCIKT website. Forecasters previously created this map from scratch. Having the climatologies as a background field will increase consistency between forecasters and decrease their workload. Delivered all files containing the lightning climatologies, the data, and the code used to create the climatologies to NWS MLB. Completed and distributed a final memorandum describing how the climatologies were created. All the files were installed on the NWS MLB computer system, and then the code was compiled and tested to ensure that it worked properly on their operating system. The climatologies and their descriptions are posted on the NWS MLB website. Forecasting Low-Level Convergent Bands Under Southeast Flow Provide guidance to operational personnel that will help improve their forecasts of cloud bands under large-scale southeast flow. When these bands occur, they can lead to cloud, rain, and thunderstorm occurrences that adversely affect launch, landing, and ground operations at Kennedy Space Center/Cape Canaveral Air Force Station (KSC/CCAFS). Completed the first draft of the final report. The conclusions from this task indicated low-level wind speed and direction, low-level high pressure ridge position, east coast sea breeze front activity and upper-level jet streak position have the greatest influence on convergent band formation and movement during  southeasterly flow."
NASA/ASEE Faculty Fellowship Program: 2003 Research Reports,22.02076,computed jpeg quality,['Engineering (General)'],"This document is a collection of technical reports on research conducted by the participants in the 2003 NASA/ASEE Faculty Fellowship Program at the John F. Kennedy Space Center (KSC). This was the nineteenth year that a NASA/ASEE program has been conducted at KSC. The 2003 program was administered by the University of Central Florida (UCF) in cooperation with KSC. The program was operated under the auspices of the American Society for Engineering Education (ASEE) and the Education Division, NASA Headquarters, Washington, D.C. The KSC program was one of nine such Aeronautics and Space Research Programs funded by NASA Headquarters in 2003. The basic common objectives of the NASA/ASEE Faculty Fellowship Program are: A) To further the professional knowledge of qualified engineering and science faculty members; B) To stimulate an exchange of ideas between teaching participants and employees of NASA; C) To enrich and refresh the research and teaching activities of participants institutions; D) To contribute to the research objectives of the NASA center. The KSC Faculty Fellows spent ten weeks (May 19 through July 25, 2003) working with NASA scientists and engineers on research of mutual interest to the university faculty member and the NASA colleague. The editors of this document were responsible for selecting appropriately qualified faculty to address some of the many research areas of current interest to NASA/KSC. A separate document reports on the administrative aspects of the 2003 program. The NASA/ASEE program is intended to be a two-year program to allow in-depth research by the university faculty member. In many cases a faculty member has developed a close working relationship with a particular NASA group that had provided funding beyond the two-year limit."
Photogrammetry Methodology Development for Gossamer Spacecraft Structures,21.956549,computed jpeg quality,['Structural Mechanics'],"Photogrammetry--the science of calculating 3D object coordinates from images--is a flexible and robust approach for measuring the static and dynamic characteristics of future ultra-lightweight and inflatable space structures (a.k.a., Gossamer structures), such as large membrane reflectors, solar sails, and thin-film solar arrays. Shape and dynamic measurements are required to validate new structural modeling techniques and corresponding analytical models for these unconventional systems. This paper summarizes experiences at NASA Langley Research Center over the past three years to develop or adapt photogrammetry methods for the specific problem of measuring Gossamer space structures. Turnkey industrial photogrammetry systems were not considered a cost-effective choice for this basic research effort because of their high purchase and maintenance costs. Instead, this research uses mainly off-the-shelf digital-camera and software technologies that are affordable to most organizations and provide acceptable accuracy."
Optical Diagnostic System for Solar Sails: Phase 1 Final Report,21.867668,computed jpeg quality,['Structural Mechanics'],"NASA's In-Space Propulsion program recently selected AEC-ABLE Engineering and L'Garde, Inc. to develop scale-model solar sail hardware and demonstrate its functionality on the ground. Both are square sail designs with lightweight diagonal booms (<100 g/m) and ultra-thin membranes (<10 g/sq m). To support this technology, the authors are developing an integrated diagnostics instrumentation package for monitoring solar sail structures such as these in a near-term flight experiment. We refer to this activity as the ""Optical Diagnostic System (ODS) for Solar Sails"" project. The approach uses lightweight optics and photogrammetric techniques to measure solar sail membrane and boom shape and dynamics, thermography to map temperature, and non-optical sensors including MEMS accelerometers and load cells. The diagnostics package must measure key structural characteristics including deployment dynamics, sail support tension, boom and sail deflection, boom and sail natural frequencies, sail temperature, and sail integrity. This report summarizes work in the initial 6-month Phase I period (conceptual design phase) and complements the final presentation given in Huntsville, AL on January 14, 2004."
NASA Technical Management Report (533Q),21.860258,computed jpeg quality,['Astronomy'],"The objective of this task is analytical support of the NASA Satellite Laser Ranging (SLR) program in the areas of SLR data analysis, software development, assessment of SLR station performance, development of improved models for atmospheric propagation and interpretation of station calibration techniques, and science coordination and analysis functions for the NASA led Central Bureau of the International Laser Ranging Service (ILRS). The contractor shall in each year of the five year contract: (1) Provide software development and analysis support to the NASA SLR program and the ILRS. Attend and make analysis reports at the monthly meetings of the Central Bureau of the ILRS covering data received during the previous period. Provide support to the Analysis Working Group of the ILRS including special tiger teams that are established to handle unique analysis problems. Support the updating of the SLR Bibliography contained on the ILRS web site; (2) Perform special assessments of SLR station performance from available data to determine unique biases and technical problems at the station; (3) Develop improvements to models of atmospheric propagation and for handling pre- and post-pass calibration data provided by global network stations; (4) Provide review presentation of overall ILRS network data results at one major scientific meeting per year; (5) Contribute to and support the publication of NASA SLR and ILRS reports highlighting the results of SLR analysis activity."
Aircraft Conceptual Design Using Vehicle Sketch Pad,21.671785,computed jpeg quality,['Aerodynamics'],"Vehicle Sketch Pad (VSP) is a parametric geometry modeling tool that is intended for use in the conceptual design of aircraft. The intent of this software is to rapidly model aircraft configurations without expending the expertise and time that is typically required for modeling with traditional Computer Aided Design (CAD) packages. VSP accomplishes this by using parametrically defined components, such as a wing that is defined by span, area, sweep, taper ratio, thickness to cord, and so on. During this phase of frequent design builds, changes to the model can be rapidly visualized along with the internal volumetric layout. Using this geometry-based approach, parameters such as wetted areas and cord lengths can be easily extracted for rapid external performance analyses, such as a parasite drag buildup. At the completion of the conceptual design phase, VSP can export its geometry to higher fidelity tools. This geometry tool was developed by NASA and is freely available to U.S. companies and universities. It has become integral to conceptual design in the Aeronautics Systems Analysis Branch (ASAB) here at NASA Langley Research Center and is currently being used at over 100 universities, aerospace companies, and other government agencies. This paper focuses on the use of VSP in recent NASA conceptual design studies to facilitate geometry-centered design methodology. Such a process is shown to promote greater levels of creativity, more rapid assessment of critical design issues, and improved ability to quickly interact with higher order analyses. A number of VSP vehicle model examples are compared to CAD-based conceptual design, from a designer perspective; comparisons are also made of the time and expertise required to build the geometry representations as well."
"NASA Tech Briefs, February 2011",21.53216,computed jpeg quality,['Man/System Technology and Life Support'],"Topics covered include: Multi-Segment Radius Measurement Using an Absolute Distance Meter Through a Null Assembly; Fiber-Optic Magnetic-Field-Strength Measurement System for Lightning Detection; Photocatalytic Active Radiation Measurements and Use; Computer Generated Hologram System for Wavefront Measurement System Calibration; Non-Contact Thermal Properties Measurement with Low-Power Laser and IR Camera System; SpaceCube 2.0: An Advanced Hybrid Onboard Data Processor; CMOS Imager Has Better Cross-Talk and Full-Well Performance; High-Performance Wireless Telemetry; Telemetry-Based Ranging; JWST Wavefront Control Toolbox; Java Image I/O for VICAR, PDS, and ISIS; X-Band Acquisition Aid Software; Antimicrobial-Coated Granules for Disinfecting Water; Range 7 Scanner Integration with PaR Robot Scanning System; Methods of Antimicrobial Coating of Diverse Materials; High-Operating-Temperature Barrier Infrared Detector with Tailorable Cutoff Wavelength; A Model of Reduced Kinetics for Alkane Oxidation Using Constituents and Species for N-Heptane; Thermally Conductive Tape Based on Carbon Nanotube Arrays; Two Catalysts for Selective Oxidation of Contaminant Gases; Nanoscale Metal Oxide Semiconductors for Gas Sensing; Lightweight, Ultra-High-Temperature, CMC-Lined Carbon/Carbon Structures; Sample Acquisition and Handling System from a Remote Platform; Improved Rare-Earth Emitter Hollow Cathode; High-Temperature Smart Structures for Engine Noise Reduction and Performance Enhancement; Cryogenic Scan Mechanism for Fourier Transform Spectrometer; Piezoelectric Rotary Tube Motor; Thermoelectric Energy Conversion Technology for High-Altitude Airships; Combustor Computations for CO2-Neutral Aviation; Use of Dynamic Distortion to Predict and Alleviate Loss of Control; Cycle Time Reduction in Trapped Mercury Ion Atomic Frequency Standards; and A (201)Hg+ Comagnetometer for (199)Hg+ Trapped Ion Space Atomic Clocks."
CESDIS,21.31037,computed jpeg quality,['Documentation and Information Science'],"CESDIS, the Center of Excellence in Space Data and Information Sciences was developed jointly by NASA, Universities Space Research Association (USRA), and the University of Maryland in 1988 to focus on the design of advanced computing techniques and data systems to support NASA Earth and space science research programs. CESDIS is operated by USRA under contract to NASA. The Director, Associate Director, Staff Scientists, and administrative staff are located on-site at NASA's Goddard Space Flight Center in Greenbelt, Maryland. The primary CESDIS mission is to increase the connection between computer science and engineering research programs at colleges and universities and NASA groups working with computer applications in Earth and space science. The 1993-94 CESDIS year included a broad range of computer science research applied to NASA problems. This report provides an overview of these research projects and programs as well as a summary of the various other activities of CESDIS in support of NASA and the university research community, We have had an exciting and challenging year."
"NASA University Research Centers Technical Advances in Education, Aeronautics, Space, Autonomy, Earth and Environment",21.29076,computed jpeg quality,['Social and Information Sciences (General)'],"This first volume of the Autonomous Control Engineering (ACE) Center Press Series on NASA University Research Center's (URC's) Advanced Technologies on Space Exploration and National Service constitute a report on the research papers and presentations delivered by NASA Installations and industry and Report of the NASA's fourteen URC's held at the First National Conference in Albuquerque, New Mexico from February 16-19, 1997."
An Overview of SIMBIOS Program Activities and Accomplishments,21.155117,computed jpeg quality,['Oceanography'],"The SIMBIOS Program was conceived in 1994 as a result of a NASA management review of the agency's strategy for monitoring the bio-optical properties of the global ocean through space-based ocean color remote sensing. At that time, the NASA ocean color flight manifest included two data buy missions, the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) and Earth Observing System (EOS) Color, and three sensors, two Moderate Resolution Imaging Spectroradiometers (MODIS) and the Multi-angle Imaging Spectro-Radiometer (MISR), scheduled for flight on the EOS-Terra and EOS-Aqua satellites. The review led to a decision that the international assemblage of ocean color satellite systems provided ample redundancy to assure continuous global coverage, with no need for the EOS Color mission. At the same time, it was noted that non-trivial technical difficulties attended the challenge (and opportunity) of combining ocean color data from this array of independent satellite systems to form consistent and accurate global bio-optical time series products. Thus, it was announced at the October 1994 EOS Interdisciplinary Working Group meeting that some of the resources budgeted for EOS Color should be redirected into an intercalibration and validation program (McClain et al., 2002)."
Research and Technology 1997,21.00703,computed jpeg quality,['General'],"This report highlights the challenging work accomplished during fiscal year 1997 by Ames research scientists and engineers. The work is divided into accomplishments that support the goals of NASA s four Strategic Enterprises: Aeronautics and Space Transportation Technology, Space Science, Human Exploration and Development of Space (HEDS), and Earth Science. NASA Ames Research Center s research effort in the Space, Earth, and HEDS Enterprises is focused i n large part to support Ames lead role for Astrobiology, which broadly defined is the scientific study of the origin, distribution, and future of life in the universe. This NASA initiative in Astrobiology is a broad science effort embracing basic research, technology development, and flight missions. Ames contributions to the Space Science Enterprise are focused in the areas of exobiology, planetary systems, astrophysics, and space technology. Ames supports the Earth Science Enterprise by conducting research and by developing technology with the objective of expanding our knowledge of the Earth s atmosphere and ecosystems. Finallv, Ames supports the HEDS Enterprise by conducting research, managing spaceflight projects, and developing technologies. A key objective is to understand the phenomena surrounding the effects of gravity on living things. Ames has also heen designated the Agency s Center of Evcellence for Information Technnlogv. The three cornerstones of Information Technology research at Ames are automated reasoning, human-centered computing, and high performance computing and networking."
The Geophysical Fluid Flow Cell Experiment,20.978098,computed jpeg quality,['Fluid Mechanics and Heat Transfer'],"The Geophysical Fluid Flow Cell (GFFC) experiment performed visualizations of thermal convection in a rotating differentially heated spherical shell of fluid. In these experiments dielectric polarization forces are used to generate a radially directed buoyancy force. This enables the laboratory simulation of a number of geophysically and astrophysically important situations in which sphericity and rotation both impose strong constraints on global scale fluid motions. During USML-2 a large set of experiments with spherically symmetric heating were carried out. These enabled the determination of critical points for the transition to various forms of nonaxisymmetric convection and, for highly turbulent flows, the transition latitudes separating the different modes of motion. This paper presents a first analysis of these experiments as well as data on the general performance of the instrument during the USML-2 flight."
"NASA Tech Briefs, April 1998",20.564413,computed jpeg quality,['Man/System Technology and Life Support'],"Topics include: special coverage on video and imaging, electronic components and circuits, electronic systems, physical sciences, materials, computer software, mechanics, machinery/automation, and a special section of Photonics Tech Briefs."
The Telecommunications and Data Acquisition Report,20.396755,computed jpeg quality,['COMMUNICATIONS AND RADAR'],"This quarterly reports on space communications, radio navigation, radio science, and ground based radio and radar astronomy in connection with the Deep Space Network (DSN) in planning, supporting research and technology, implementation, and in operations. Also included is standards activity at  JPL for space data and information systems and DSN work. Specific areas of research are: Tracking  and ground based navigation; Spacecraft and ground communications; Station control and system technology; DSN Systems Implementation; and DSN Operations."
Multimedia content description framework,20.182966,computed jpeg quality,['Documentation and Information Science'],"A framework is provided for describing multimedia content and a system in which a plurality of multimedia storage devices employing the content description methods of the present invention can interoperate. In accordance with one form of the present invention, the content description framework is a description scheme (DS) for describing streams or aggregations of multimedia objects, which may comprise audio, images, video, text, time series, and various other modalities. This description scheme can accommodate an essentially limitless number of descriptors in terms of features, semantics or metadata, and facilitate content-based search, index, and retrieval, among other capabilities, for both streamed or aggregated multimedia objects."
[Activity of Institute for Computer Applications in Science and Engineering],19.767628,computed jpeg quality,['Mathematical and Computer Sciences (General)'],"This report summarizes research conducted at the Institute for Computer Applications in Science and Engineering in applied mathematics, fluid mechanics, and computer science."
Global Satellite Observations for Smart Cities,19.765528,computed jpeg quality,['Earth Resources and Remote Sensing'],"The smart city approach requires collection of interdisciplinary data and information from multiple sources and integration with modern technologies to provide a new and cost-effective way for researchers and decision makers to study and manage cities. In this book chapter, we introduce NASA satellite-based global and regional observations with emphasis on the hydrologic cycle (e.g., precipitation, wind, temperature, soil moisture) for smart cities. These products, consisting of both near-real-time and historical datasets, are publicly available free of charge and can be used for global and regional research and applications. Examples of using these datasets in smart cities are included. The chapter is organized as follows, first, a brief overview of NASA global satellite-based data products, followed by data services and tools, two examples of using satellite-based datasets in megacities, and finally summary and future plans."
CESDIS,19.72099,computed jpeg quality,['Documentation and Information Science'],"CESDIS, the Center of Excellence in Space Data and Information Sciences was developed jointly by NASA, Universities Space Research Association (USRA), and the University of Maryland in 1988 to focus on the design of advanced computing techniques and data systems to support NASA Earth and space science research programs. CESDIS is operated by USRA under contract to NASA. The Director, Associate Director, Staff Scientists, and administrative staff are located on-site at NASA's Goddard Space Flight Center in Greenbelt, Maryland. The primary CESDIS mission is to increase the connection between computer science and engineering research programs at colleges and universities and NASA groups working with computer applications in Earth and space science. Research areas of primary interest at CESDIS include: 1) High performance computing, especially software design and performance evaluation for massively parallel machines; 2) Parallel input/output and data storage systems for high performance parallel computers; 3) Data base and intelligent data management systems for parallel computers; 4) Image processing; 5) Digital libraries; and 6) Data compression. CESDIS funds multiyear projects at U. S. universities and colleges. Proposals are accepted in response to calls for proposals and are selected on the basis of peer reviews. Funds are provided to support faculty and graduate students working at their home institutions. Project personnel visit Goddard during academic recess periods to attend workshops, present seminars, and collaborate with NASA scientists on research projects. Additionally, CESDIS takes on specific research tasks of shorter duration for computer science research requested by NASA Goddard scientists."
Technology Directions for the 21st Century,19.68267,computed jpeg quality,['Communications and Radar'],"Data compression is an important tool for reducing the bandwidth of communications systems, and thus for reducing the size, weight, and power of spacecraft systems. For data requiring lossless transmissions, including most science data from spacecraft sensors, small compression factors of two to three may be expected. Little improvement can be expected over time. For data that is suitable for lossy compression, such as video data streams, much higher compression factors can be expected, such as 100 or more. More progress can be expected in this branch of the field, since there is more hidden redundancy and many more ways to exploit that redundancy."
Orion Multi-Purpose Crew Vehicle (MPCV) Capsule Parachute Assembly System (CPAS) Wake Deficit Wind Tunnel Testing,19.624466,computed jpeg quality,['Space Transportation and Safety'],"During descent after re-entry into the Earth's atmosphere, the Orion CM deploys its drogue parachutes at approximately Mach 0.7. Accurately predicting the dynamic pressure experienced by the drogue parachutes at deployment is critical to properly designing the parachutes. This NASA Engineering and Safety Center assessment was designed to provide a complete set of flowfield measurements on and around an idealized Orion Crew Module shape with the most appropriate wind tunnel simulation of the Orion flight conditions prior to parachute deployment. This document contains the details of testing and the outcome of the assessment."
Visualization of Atmospheric Water Vapor Data for SAGE,19.397352,computed jpeg quality,['Meteorology and Climatology'],"The goal of this project was to develop visualization tools to study the water vapor dynamics using the Stratospheric Aerosol and Gas Experiment 11 (SAGE 11) water vapor data. During the past years, we completed the development of a visualization tool called EZSAGE, and various Gridded Water Vapor plots, tools deployed on the web to provide users with new insight into the water vapor dynamics. Results and experiences from this project, including papers, tutorials and reviews were published on the main Web page. Additional publishing effort has been initiated to package EZSAGE software for CD production and distribution. There have been some major personnel changes since Fall, 1998. Dr. Mou-Liang Kung, a Professor of Computer Science assumed the PI position vacated by Dr. Waldo Rodriguez who was on leave. However, former PI, Dr. Rodriguez continued to serve as a research adviser to this project to assure smooth transition and project completion. Typically in each semester, five student research assistants were hired and trained. Weekly group meetings were held to discuss problems, progress, new research direction, and activity planning. Other small group meetings were also held regularly for different objectives of this project. All student research assistants were required to submit reports for conference submission."
Space Telecommunications Radio System (STRS) Application Repository Design and Analysis,19.253942,computed jpeg quality,"['Space Communications, Spacecraft Communications, Command and Tracking', 'Computer Programming and Software']","The Space Telecommunications Radio System (STRS) Application Repository Design and Analysis document describes the STRS application repository for software-defined radio (SDR) applications intended to be compliant to the STRS Architecture Standard. The document provides information about the submission of artifacts to the STRS application repository, to provide information to the potential users of that information, and for the systems engineer to understand the requirements, concepts, and approach to the STRS application repository. The STRS application repository is intended to capture knowledge, documents, and other artifacts for each waveform application or other application outside of its project so that when the project ends, the knowledge is retained. The document describes the transmission of technology from mission to mission capturing lessons learned that are used for continuous improvement across projects and supporting NASA Procedural Requirements (NPRs) for performing software engineering projects and NASAs release process."
Methods and Apparatus for Autonomous Robotic Control,19.201319,computed jpeg quality,"['Cybernetics, Artificial Intelligence and Robotics']","Sensory processing of visual, auditory, and other sensor information (e.g., visual imagery, LIDAR, RADAR) is conventionally based on ""stovepiped,"" or isolated processing, with little interactions between modules. Biological systems, on the other hand, fuse multi-sensory information to identify nearby objects of interest more quickly, more efficiently, and with higher signal-to-noise ratios. Similarly, examples of the OpenSense technology disclosed herein use neurally inspired processing to identify and locate objects in a robot's environment. This enables the robot to navigate its environment more quickly and with lower computational and power requirements."
"NASA Tech Briefs, September 2013",18.700289,computed jpeg quality,['Man/System Technology and Life Support'],"Topics include: ISS Ammonia Leak Detection Through X-Ray Fluorescence; A System for Measuring the Sway of the Vehicle Assembly Building; Fast, High-Precision Readout Circuit for Detector Arrays; Victim Simulator for Victim Detection Radar; Hydrometeor Size Distribution Measurements by Imaging the Attenuation of a Laser Spot; Quasi-Linear Circuit; High-Speed, High-Resolution Time-to-Digital Conversion; Li-Ion Battery and Supercapacitor Hybrid Design for Long Extravehicular Activities; Ultrasonic Low-Friction Containment Plate for Thermal and Ultrasonic Stir Weld Processes; High-Powered, Ultrasonically Assisted Thermal Stir Welding; Next-Generation MKIII Lightweight HUT/Hatch Assembly; Centrifugal Sieve for Gravity-Level-Independent Size; Segregation of Granular Materials; Ion Exchange Technology Development in Support of the Urine Processor Assembly; Nickel-Graphite Composite Compliant Interface and/or Hot Shoe Material; UltraSail CubeSat Solar Sail Flight Experiment; Mechanism for Deploying a Long, Thin-Film Antenna From a Rover; Counterflow Regolith Heat Exchanger; Acquisition and Retaining Granular Samples via a Rotating Coring Bit; Very-Low-Cost, Rugged Vacuum System; Medicine Delivery Device With Integrated Sterilization and Detection; FRET-Aptamer Assays for Bone Marker Assessment, C-Telopeptide, Creatinine, and Vitamin D; Multimode Directional Coupler for Utilization of Harmonic Frequencies from TWTAs; Dual-Polarization, Multi-Frequency Antenna Array for use with Hurricane Imaging Radiometer; Complementary Barrier Infrared Detector (CBIRD) Contact Methods; Autonomous Control of Space Nuclear Reactors; High-Power, High-Speed Electro-Optic Pockels Cell Modulator; Covariance Analysis Tool (G-CAT) for Computing Ascent, Descent, and Landing Errors; Enigma Version 12; Micrometeoroid and Orbital Debris (MMOD) Shield Ballistic Limit Analysis Program; Spitzer Telemetry Processing System; Planetary Protection Bioburden Analysis Program; Wing Leading Edge RCC Rapid Response Damage Prediction Tool (IMPACT2); ISSM: Ice Sheet System Model; Automated Loads Analysis System (ATLAS); Integrated Main Propulsion System Performance Reconstruction Process/Models. Phoenix Telemetry Processor; Contact Graph Routing Enhancements Developed in ION for DTN; GFEChutes Lo-Fi; Advanced Strategic and Tactical Relay Request Management for the Mars Relay Operations Service; Software for Generating Troposphere Corrections for InSAR Using GPS and Weather Model Data; Ionospheric Specifications for SAR Interferometry (ISSI); Implementation of a Wavefront-Sensing Algorithm; Sally Ride EarthKAM - Automated Image Geo-Referencing Using Google Earth Web Plug-In; Trade Space Specification Tool (TSST) for Rapid Mission Architecture (Version 1.2); Acoustic Emission Analysis Applet (AEAA) Software; Memory-Efficient Onboard Rock Segmentation; Advanced Multimission Operations System (ATMO); Robot Sequencing and Visualization Program (RSVP); Automating Hyperspectral Data for Rapid Response in Volcanic Emergencies; Raster-Based Approach to Solar Pressure Modeling; Space Images for NASA JPL Android Version; Kinect Engineering with Learning (KEWL); Spacecraft 3D Augmented Reality Mobile App; MPST Software: grl_pef_check; Real-Time Multimission Event Notification System for Mars Relay; SIM_EXPLORE: Software for Directed Exploration of Complex Systems; Mobile Timekeeping Application Built on Reverse-Engineered JPL Infrastructure; Advanced Query and Data Mining Capabilities for MaROS; Jettison Engineering Trajectory Tool; MPST Software: grl_suppdoc; PredGuid+A: Orion Entry Guidance Modified for Aerocapture; Planning Coverage Campaigns for Mission Design and Analysis: CLASP for DESDynl; and Space Place Prime. "
Low Cost Mission Operations Workshop,18.614878,computed jpeg quality,['SYSTEMS ANALYSIS'],"The presentations given at the Low Cost (Space) Mission Operations (LCMO) Workshop are outlined. The LCMO concepts are covered in four introductory sections: Definition of Mission Operations (OPS); Mission Operations (MOS) Elements; The Operations Concept; and Mission Operations for Two Classes of Missions (operationally simple and complex). Individual presentations cover the following topics: Science Data Processing and Analysis; Mis sion Design, Planning, and Sequencing; Data Transport and Delivery, and Mission Coordination and Engineering Analysis. A list of panelists who participated in the conference is included along with a listing of the contact persons for obtaining more information concerning LCMO at JPL. The presentation of this document is in outline and graphic form."
IGARSS 1997: International Geoscience and Remote Sensing Symposium,18.440254,computed jpeg quality,['Geosciences (General)'],This paper includes geoscience and remote sensing.
Performance of the JPEG Estimated Spectrum Adaptive Postfilter (JPEG-ESAP) for Low Bit Rates,178.26173,jpeg algorithm,['Instrumentation and Photography'],"Frequency-based, pixel-adaptive filtering using the JPEG-ESAP algorithm for low bit rate JPEG formatted color images may allow for more compressed images while maintaining equivalent quality at a smaller file size or bitrate. For RGB, an image is decomposed into three color bands--red, green, and blue. The JPEG-ESAP algorithm is then applied to each band (e.g., once for red, once for green, and once for blue) and the output of each application of the algorithm is rebuilt as a single color image. The ESAP algorithm may be repeatedly applied to MPEG-2 video frames to reduce their bit rate by a factor of 2 or 3, while maintaining equivalent video quality, both perceptually, and objectively, as recorded in the computed PSNR values."
A comparison of the fractal and JPEG algorithms,174.4765,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A proprietary fractal image compression algorithm and the Joint Photographic Experts Group (JPEG) industry standard algorithm for image compression are compared. In every case, the JPEG algorithm was superior to the fractal method at a given compression ratio according to a root mean square criterion and a peak signal to noise criterion."
JPEG 2000 Encoding with Perceptual Distortion Control,155.89752,jpeg algorithm,['Man/System Technology and Life Support'],"An alternative approach has been devised for encoding image data in compliance with JPEG 2000, the most recent still-image data-compression standard of the Joint Photographic Experts Group. Heretofore, JPEG 2000 encoding has been implemented by several related schemes classified as rate-based distortion-minimization encoding. In each of these schemes, the end user specifies a desired bit rate and the encoding algorithm strives to attain that rate while minimizing a mean squared error (MSE). While rate-based distortion minimization is appropriate for transmitting data over a limited-bandwidth channel, it is not the best approach for applications in which the perceptual quality of reconstructed images is a major consideration. A better approach for such applications is the present alternative one, denoted perceptual distortion control, in which the encoding algorithm strives to compress data to the lowest bit rate that yields at least a specified level of perceptual image quality. Some additional background information on JPEG 2000 is prerequisite to a meaningful summary of JPEG encoding with perceptual distortion control. The JPEG 2000 encoding process includes two subprocesses known as tier-1 and tier-2 coding. In order to minimize the MSE for the desired bit rate, a rate-distortion- optimization subprocess is introduced between the tier-1 and tier-2 subprocesses. In tier-1 coding, each coding block is independently bit-plane coded from the most-significant-bit (MSB) plane to the least-significant-bit (LSB) plane, using three coding passes (except for the MSB plane, which is coded using only one ""clean up"" coding pass). For M bit planes, this subprocess involves a total number of (3M - 2) coding passes. An embedded bit stream is then generated for each coding block. Information on the reduction in distortion and the increase in the bit rate associated with each coding pass is collected. This information is then used in a rate-control procedure to determine the contribution of each coding block to the output compressed bit stream."
Scan-Based Implementation of JPEG 2000 Extensions,149.89803,jpeg algorithm,['Computer Programming and Software'],"JPEG 2000 Part 2 (Extensions) contains a number of technologies that are of potential interest in remote sensing applications. These include arbitrary wavelet transforms, techniques to limit boundary artifacts in tiles, multiple component transforms, and trellis-coded quantization (TCQ). We are investigating the addition of these features to the low-memory (scan-based) implementation of JPEG 2000 Part 1. A scan-based implementation of TCQ has been realized and tested, with a very small performance loss as compared with the full image (frame-based) version. A proposed amendment to JPEG 2000 Part 2 will effect the syntax changes required to make scan-based TCQ compatible with the standard."
Reduction of blocking effects for the JPEG baseline image compression standard,135.02162,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"Transform coding has been chosen for still image compression in the Joint Photographic Experts Group (JPEG) standard. Although transform coding performs superior to many other image compression methods and has fast algorithms for implementation, it is limited by a blocking effect at low bit rates. The blocking effect is inherent in all nonoverlapping transforms. This paper presents a technique for reducing blocking while remaining compatible with the JPEG standard. Simulations show that the system results in subjective performance improvements, sacrificing only a marginal increase in bit rate."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,113.676155,jpeg algorithm,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Image coding by way of wavelets,107.643265,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"The application of two wavelet transforms to image compression is discussed. It is noted that the Haar transform, with proper bit allocation, has performance that is visually superior to an algorithm based on a Daubechies filter and to the discrete cosine transform based Joint Photographic Experts Group (JPEG) algorithm at compression ratios exceeding 20:1. In terms of the root-mean-square error, the performance of the Haar transform method is basically comparable to that of the JPEG algorithm. The implementation of the Haar transform can be achieved in integer arithmetic, making it very suitable for applications requiring real-time performance."
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,90.820694,jpeg algorithm,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
Blocking reduction of Landsat Thematic Mapper JPEG browse images using optimal PSNR estimated spectra adaptive postfiltering,89.97058,jpeg algorithm,['COMPUTER SYSTEMS'],"Two representative sample images of Band 4 of the Landsat Thematic Mapper are compressed with the JPEG algorithm at 8:1, 16:1 and 24:1 Compression Ratios for experimental browsing purposes. We then apply the Optimal PSNR Estimated Spectra Adaptive Postfiltering (ESAP) algorithm to reduce the DCT blocking distortion. ESAP reduces the blocking distortion while preserving most of the image's edge information by adaptively postfiltering the decoded image using the block's spectral information already obtainable from each block's DCT coefficients. The algorithm iteratively applied a one dimensional log-sigmoid weighting function to the separable interpolated local block estimated spectra of the decoded image until it converges to the optimal PSNR with respect to the original using a 2-D steepest ascent search. Convergence is obtained in a few iterations for integer parameters. The optimal logsig parameters are transmitted to the decoder as a negligible byte of overhead data. A unique maxima is guaranteed due to the 2-D asymptotic exponential overshoot shape of the surface generated by the algorithm. ESAP is based on a DFT analysis of the DCT basis functions. It is implemented with pixel-by-pixel spatially adaptive separable FIR postfilters. PSNR objective improvements between 0.4 to 0.8 dB are shown together with their corresponding optimal PSNR adaptive postfiltered images."
High performance compression of science data,88.833176,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
Fast computational scheme of image compression for 32-bit microprocessors,88.30025,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"This paper presents a new computational scheme of image compression based on the discrete cosine transform (DCT), underlying JPEG and MPEG International Standards. The algorithm for the 2-d DCT computation uses integer operations (register shifts and additions / subtractions only); its computational complexity is about 8 additions per image pixel. As a meaningful example of an on-board image compression application we consider the software implementation of the algorithm for the Mars Rover (Marsokhod, in Russian) imaging system being developed as a part of Mars-96 International Space Project. It's shown that fast software solution for 32-bit microprocessors may compete with the DCT-based image compression hardware."
High Performance Compression of Science Data,88.29392,jpeg algorithm,['Mathematical and Computer Sciences (General)'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
Compression through decomposition into browse and residual images,84.67039,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Economical archival and retrieval of image data is becoming increasingly important considering the unprecedented data volumes expected from the Earth Observing System (EOS) instruments. For cost effective browsing the image data (possibly from remote site), and retrieving the original image data from the data archive, we suggest an integrated image browse and data archive system employing incremental transmission. We produce our browse image data with the JPEG/DCT lossy compression approach. Image residual data is then obtained by taking the pixel by pixel differences between the original data and the browse image data. We then code the residual data with a form of variable length coding called diagonal coding. In our experiments, the JPEG/DCT is used at different quality factors (Q) to generate the browse and residual data. The algorithm has been tested on band 4 of two Thematic mapper (TM) data sets. The best overall compression ratios (of about 1.7) were obtained when a quality factor of Q=50 was used to produce browse data at a compression ratio of 10 to 11. At this quality factor the browse image data has virtually no visible distortions for the images tested."
The effects of video compression on acceptability of images for monitoring life sciences experiments,84.5742,jpeg algorithm,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Future manned space operations for Space Station Freedom will call for a variety of carefully planned multimedia digital communications, including full-frame-rate color video, to support remote operations of scientific experiments. This paper presents the results of an investigation to determine if video compression is a viable solution to transmission bandwidth constraints. It reports on the impact of different levels of compression and associated calculational parameters on image acceptability to investigators in life-sciences research at ARC. Three nonhuman life-sciences disciplines (plant, rodent, and primate biology) were selected for this study. A total of 33 subjects viewed experimental scenes in their own scientific disciplines. Ten plant scientists viewed still images of wheat stalks at various stages of growth. Each image was compressed to four different compression levels using the Joint Photographic Expert Group (JPEG) standard algorithm, and the images were presented in random order. Twelve and eleven staffmembers viewed 30-sec videotaped segments showing small rodents and a small primate, respectively. Each segment was repeated at four different compression levels in random order using an inverse cosine transform (ICT) algorithm. Each viewer made a series of subjective image-quality ratings. There was a significant difference in image ratings according to the type of scene viewed within disciplines; thus, ratings were scene dependent. Image (still and motion) acceptability does, in fact, vary according to compression level. The JPEG still-image-compression levels, even with the large range of 5:1 to 120:1 in this study, yielded equally high levels of acceptability. In contrast, the ICT algorithm for motion compression yielded a sharp decline in acceptability below 768 kb/sec. Therefore, if video compression is to be used as a solution for overcoming transmission bandwidth constraints, the effective management of the ratio and compression parameters according to scientific discipline and experiment type is critical to the success of remote experiments."
Resiliency of the Multiscale Retinex Image Enhancement Algorithm,76.862595,jpeg algorithm,['Computer Programming and Software'],"The multiscale retinex with color restoration (MSRCR) continues to prove itself in extensive testing to be very versatile automatic image enhancement algorithm that simultaneously provides dynamic range compression, color constancy, and color rendition, However, issues remain with regard to the resiliency of the MSRCR to different image sources and arbitrary image manipulations which may have been applied prior to retinex processing. In this paper we define these areas of concern, provide experimental results, and, examine the effects of commonly occurring image manipulation on retinex performance. In virtually all cases the MSRCR is highly resilient to the effects of both the image source variations and commonly encountered prior image-processing. Significant artifacts are primarily observed for the case of selective color channel clipping in large dark zones in a image. These issues are of concerning the processing of digital image archives and other applications where there is neither control over the image acquisition process, nor knowledge about any processing done on th data beforehand."
Non-linear Post Processing Image Enhancement,70.352,jpeg algorithm,"['Cybernetics, Artificial Intelligence and Robotics']","A non-linear filter for image post processing based on the feedforward Neural Network topology is presented. This study was undertaken to investigate the usefulness of ""smart"" filters in image post processing. The filter has shown to be useful in recovering high frequencies, such as those lost during the JPEG compression-decompression process. The filtered images have a higher signal to noise ratio, and a higher perceived image quality. Simulation studies comparing the proposed filter with the optimum mean square non-linear filter, showing examples of the high frequency recovery, and the statistical properties of the filter are given,"
The effect of lossy image compression on image classification,69.57072,jpeg algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
A comparison of model-based VQ compression with other VQ approaches,68.952484,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"In our previous work on Model-Based Vector Quantization (MVQ), we presented some performance comparisons (both rate distortion and decompression time) with VQ and JPEG/DCT. In this paper, we compare the MVQ's rate distortion performance with Mean Removed Vector Quantization (MRVQ) and include our previous comparison with VQ. MVQ is similar to MRVQ in many ways. Both of these techniques extract means of the vectors (raster-scanned image blocks) and reduce them to mean removed residuals by subtracting block means from the elements of the vectors. In the case of MRVQ, a codebook of residual vectors is generated using a training set. For every vector from the input image, the block mean and address of the codevector from the codebook that matches the input vector closest are transmitted to the decoder. The codebook is generated using generalized Lloyd algorithm on training set of residual vectors. For MVQ the pairs consist of vector means and address of the closest matching vector from codebook generated by models based on statistical properties of the residuals and Human Visual System (HVS). In our experiments, we found that MVQ performance in rate distortion sense is almost always better than VQ and is comparable to MRVQ. Further, MVQ is much easier to use than either VQ or MRVQ, since the training and managing of explicit codebooks is not required."
Phoenix Telemetry Processor,68.58497,jpeg algorithm,['Computer Programming and Software'],"Phxtelemproc is a C/C++ based telemetry processing program that processes SFDU telemetry packets from the Telemetry Data System (TDS). It generates Experiment Data Records (EDRs) for several instruments including surface stereo imager (SSI); robotic arm camera (RAC); robotic arm (RA); microscopy, electrochemistry, and conductivity analyzer (MECA); and the optical microscope (OM). It processes both uncompressed and compressed telemetry, and incorporates unique subroutines for the following compression algorithms: JPEG Arithmetic, JPEG Huffman, Rice, LUT3, RA, and SX4. This program was in the critical path for the daily command cycle of the Phoenix mission. The products generated by this program were part of the RA commanding process, as well as the SSI, RAC, OM, and MECA image and science analysis process. Its output products were used to advance science of the near polar regions of Mars, and were used to prove that water is found in abundance there. Phxtelemproc is part of the MIPL (Multi-mission Image Processing Laboratory) system. This software produced Level 1 products used to analyze images returned by in situ spacecraft. It ultimately assisted in operations, planning, commanding, science, and outreach."
DSP Implementation of the Retinex Image Enhancement Algorithm,68.18517,jpeg algorithm,['Numerical Analysis'],"The Retinex is a general-purpose image enhancement algorithm that is used to produce good visual representations of scenes. It performs a non-linear spatial/spectral transform that synthesizes strong local contrast enhancement and color constancy. A real-time, video frame rate implementation of the Retinex is required to meet the needs of various potential users. Retinex processing contains a relatively large number of complex computations, thus to achieve real-time performance using current technologies requires specialized hardware and software. In this paper we discuss the design and development of a digital signal processor (DSP) implementation of the Retinex. The target processor is a Texas Instruments TMS320C6711 floating point DSP. NTSC video is captured using a dedicated frame-grabber card, Retinex processed, and displayed on a standard monitor. We discuss the optimizations used to achieve real-time performance of the Retinex and also describe our future plans on using alternative architectures."
"Modeling of video traffic in packet networks, low rate video compression, and the development of a lossy+lossless image compression algorithm",67.666565,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this reporting period we have worked on three somewhat different problems. These are modeling of video traffic in packet networks, low rate video compression, and the development of a lossy + lossless image compression algorithm, which might have some application in browsing algorithms. The lossy + lossless scheme is an extension of work previously done under this grant. It provides a simple technique for incorporating browsing capability. The low rate coding scheme is also a simple variation on the standard discrete cosine transform (DCT) coding approach. In spite of its simplicity, the approach provides surprisingly high quality reconstructions. The modeling approach is borrowed from the speech recognition literature, and seems to be promising in that it provides a simple way of obtaining an idea about the second order behavior of a particular coding scheme. Details about these are presented."
A visual detection model for DCT coefficient quantization,67.294815,jpeg algorithm,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
Low bit rate coding of Earth science images,67.152054,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"In this paper, the authors discuss compression based on some new ideas in vector quantization and their incorporation in a sub-band coding framework. Several variations are considered, which collectively address many of the individual compression needs within the earth science community. The approach taken in this work is based on some recent advances in the area of variable rate residual vector quantization (RVQ). This new RVQ method is considered separately and in conjunction with sub-band image decomposition. Very good results are achieved in coding a variety of earth science images. The last section of the paper provides some comparisons that illustrate the improvement in performance attributable to this approach relative the the JPEG coding standard."
Radiometric resolution enhancement by lossy compression as compared to truncation followed by lossless compression,62.93125,jpeg algorithm,['COMPUTER SYSTEMS'],"Recent advances in imaging technology make it possible to obtain imagery data of the Earth at high spatial, spectral and radiometric resolutions from Earth orbiting satellites. The rate at which the data is collected from these satellites can far exceed the channel capacity of the data downlink. Reducing the data rate to within the channel capacity can often require painful trade-offs in which certain scientific returns are sacrificed for the sake of others. In this paper we model the radiometric version of this form of lossy compression by dropping a specified number of least significant bits from each data pixel and compressing the remaining bits using an appropriate lossless compression technique. We call this approach 'truncation followed by lossless compression' or TLLC. We compare the TLLC approach with applying a lossy compression technique to the data for reducing the data rate to the channel capacity, and demonstrate that each of three different lossy compression techniques (JPEG/DCT, VQ and Model-Based VQ) give a better effective radiometric resolution than TLLC for a given channel rate."
Design and Performance of an Open-Source Star Tracker Algorithm on Commercial Off-the-Shelf Cameras and Computers,61.43592,jpeg algorithm,['Spacecraft Instrumentation and Astrionics'],"Recent frustration in finding low size, weight, power (SWaP), cost, and lead time star trackers has driven an internal research and development effort at Johnson Space Center (JSC) in partnership with Rensselaer Polytechnic Institute (RPI) to develop and demonstrate a commercial off-the-shelf (COTS) camera and COTS computer-based star tracker system. A set of open-source algorithms has been developed and their function demonstrated on multiple low-cost COTS single board computers (SBCs) across a variety of operating systems and COTS cameras. The goal of this effort is to release the software and setup guide to the community in order to reduce spacecraft development costs while increasing their capability (perhaps most of interest to low-cost missions like CubeSats). This material will show the high level architecture of the system, detail the algo-rithm, various tested configurations, and results. Forward work and applications will also be discussed."
System considerations for efficient communication and storage of MSTI image data,57.99418,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
Fast Lossless Compression of Multispectral-Image Data,56.54367,jpeg algorithm,['Computer Programming and Software'],"An algorithm that effects fast lossless compression of multispectral-image data is based on low-complexity, proven adaptive-filtering algorithms. This algorithm is intended for use in compressing multispectral-image data aboard spacecraft for transmission to Earth stations. Variants of this algorithm could be useful for lossless compression of three-dimensional medical imagery and, perhaps, for compressing image data in general."
Low-Complexity Lossless Compression of Hyperspectral Imagery via Adaptive Filtering,54.026714,jpeg algorithm,['Optics'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral data is presented. The technique relies on the sign algorithm from the repertoire of adaptive filtering. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
"NASA Tech Briefs, September 2008",53.308105,jpeg algorithm,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
Subband Image Coding with Jointly Optimized Quantizers,52.626778,jpeg algorithm,['Computer Programming and Software'],"An iterative design algorithm for the joint design of complexity- and entropy-constrained subband quantizers and associated entropy coders is proposed. Unlike conventional subband design algorithms, the proposed algorithm does not require the use of various bit allocation algorithms. Multistage residual quantizers are employed here because they provide greater control of the complexity-performance tradeoffs, and also because they allow efficient and effective high-order statistical modeling. The resulting subband coder exploits statistical dependencies within subbands, across subbands, and across stages, mainly through complexity-constrained high-order entropy coding. Experimental results demonstrate that the complexity-rate-distortion performance of the new subband coder is exceptional."
Evaluation of Algorithms for Compressing Hyperspectral Data,52.56291,jpeg algorithm,['Computer Programming and Software'],"With EO-1 Hyperion in orbit NASA is showing their continued commitment to hyperspectral imaging (HSI). As HSI sensor technology continues to mature, the ever-increasing amounts of sensor data generated will result in a need for more cost effective communication and data handling systems. Lockheed Martin, with considerable experience in spacecraft design and developing special purpose onboard processors, has teamed with Applied Signal & Image Technology (ASIT), who has an extensive heritage in HSI spectral compression and Mapping Science (MSI) for JPEG 2000 spatial compression expertise, to develop a real-time and intelligent onboard processing (OBP) system to reduce HSI sensor downlink requirements. Our goal is to reduce the downlink requirement by a factor > 100, while retaining the necessary spectral and spatial fidelity of the sensor data needed to satisfy the many science, military, and intelligence goals of these systems. Our compression algorithms leverage commercial-off-the-shelf (COTS) spectral and spatial exploitation algorithms. We are currently in the process of evaluating these compression algorithms using statistical analysis and NASA scientists. We are also developing special purpose processors for executing these algorithms onboard a spacecraft."
A High Performance Image Data Compression Technique for Space Applications,52.086327,jpeg algorithm,['Computer Systems'],"A highly performing image data compression technique is currently being developed for space science applications under the requirement of high-speed and pushbroom scanning. The technique is also applicable to frame based imaging data. The algorithm combines a two-dimensional transform with a bitplane encoding; this results in an embedded bit string with exact desirable compression rate specified by the user. The compression scheme performs well on a suite of test images acquired from spacecraft instruments. It can also be applied to three-dimensional data cube resulting from hyper-spectral imaging instrument. Flight qualifiable hardware implementations are in development. The implementation is being designed to compress data in excess of 20 Msampledsec and support quantization from 2 to 16 bits. This paper presents the algorithm, its applications and status of development."
Locally adaptive vector quantization: Data compression with feature preservation,49.815388,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study of a locally adaptive vector quantization (LAVQ) algorithm for data compression is presented. This algorithm provides high-speed one-pass compression and is fully adaptable to any data source and does not require a priori knowledge of the source statistics. Therefore, LAVQ is a universal data compression algorithm. The basic algorithm and several modifications to improve performance are discussed. These modifications are nonlinear quantization, coarse quantization of the codebook, and lossless compression of the output. Performance of LAVQ on various images using irreversible (lossy) coding is comparable to that of the Linde-Buzo-Gray algorithm, but LAVQ has a much higher speed; thus this algorithm has potential for real-time video compression. Unlike most other image compression algorithms, LAVQ preserves fine detail in images. LAVQ's performance as a lossless data compression algorithm is comparable to that of Lempel-Ziv-based algorithms, but LAVQ uses far less memory during the coding process."
Integer cosine transform for image compression,49.609657,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"This article describes a recently introduced transform algorithm called the integer cosine transform (ICT), which is used in transform-based data compression schemes. The ICT algorithm requires only integer operations on small integers and at the same time gives a rate-distortion performance comparable to that offered by the floating-point discrete cosine transform (DCT). The article addresses the issue of implementation complexity, which is of prime concern for source coding applications of interest in deep-space communications. Complexity reduction in the transform stage of the compression scheme is particularly relevant, since this stage accounts for most (typically over 80 percent) of the computational load."
Low-Complexity Adaptive Lossless Compression of Hyperspectral Imagery,49.15788,jpeg algorithm,['Instrumentation and Photography'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral imagery is described. This technique is designed to be suitable for implementation in hardware such as a field programmable gate array (FPGA); such an implementation could be used for high-speed compression of hyperspectral imagery onboard a spacecraft. The predictive step of the technique makes use of the sign algorithm, which is a relative of the least mean square (LMS) algorithm from the field of low-complexity adaptive filtering. The compressed data stream consists of prediction residuals encoded using a method similar to that of the JPEG-LS lossless image compression standard. Compression results are presented for several datasets including some raw Airborne Visible/ Infrared Imaging Spectrometer (AVIRIS) datasets and raw Atmospheric Infrared Sounder (AIRS) datasets. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
Data compression by wavelet transforms,48.64462,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A wavelet transform algorithm is applied to image compression. It is observed that the algorithm does not suffer from the blockiness characteristic of the DCT-based algorithms at compression ratios exceeding 25:1, but the edges do not appear as sharp as they do with the latter method. Some suggestions for the improved performance of the wavelet transform method are presented."
A simplified Integer Cosine Transform and its application in image compression,47.250896,jpeg algorithm,['NUMERICAL ANALYSIS'],"A simplified version of the integer cosine transform (ICT) is described. For practical reasons, the transform is considered jointly with the quantization of its coefficients. It differs from conventional ICT algorithms in that the combined factors for normalization and quantization are approximated by powers of two. In conventional algorithms, the normalization/quantization stage typically requires as many integer divisions as the number of transform coefficients. By restricting the factors to powers of two, these divisions can be performed by variable shifts in the binary representation of the coefficients, with speed and cost advantages to the hardware implementation of the algorithm. The error introduced by the factor approximations is compensated for in the inverse ICT operation, executed with floating point precision. The simplified ICT algorithm has potential applications in image-compression systems with disparate cost and speed requirements in the encoder and decoder ends. For example, in deep space image telemetry, the image processors on board the spacecraft could take advantage of the simplified, faster encoding operation, which would be adjusted on the ground, with high-precision arithmetic. A dual application is found in compressed video broadcasting. Here, a fast, high-performance processor at the transmitter would precompensate for the factor approximations in the inverse ICT operation, to be performed in real time, at a large number of low-cost receivers."
Methods of evaluating the effects of coding on SAR data,47.180138,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"It is recognized that mean square error (MSE) is not a sufficient criterion for determining the acceptability of an image reconstructed from data that has been compressed and decompressed using an encoding algorithm. In the case of Synthetic Aperture Radar (SAR) data, it is also deemed to be insufficient to display the reconstructed image (and perhaps error image) alongside the original and make a (subjective) judgment as to the quality of the reconstructed data. In this paper we suggest a number of additional evaluation criteria which we feel should be included as evaluation metrics in SAR data encoding experiments. These criteria have been specifically chosen to provide a means of ensuring that the important information in the SAR data is preserved. The paper also presents the results of an investigation into the effects of coding on SAR data fidelity when the coding is applied in (1) the signal data domain, and (2) the image domain. An analysis of the results highlights the shortcomings of the MSE criterion, and shows which of the suggested additional criterion have been found to be most important."
A seismic data compression system using subband coding,46.64999,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"This article presents a study of seismic data compression techniques and a compression algorithm based on subband coding. The algorithm includes three stages: a decorrelation stage, a quantization stage that introduces a controlled amount of distortion to allow for high compression ratios, and a lossless entropy coding stage based on a simple but efficient arithmetic coding method. Subband coding methods are particularly suited to the decorrelation of nonstationary processes such as seismic events. Adaptivity to the nonstationary behavior of the waveform is achieved by dividing the data into separate blocks that are encoded separately with an adaptive arithmetic encoder. This is done with high efficiency due to the low overhead introduced by the arithmetic encoder in specifying its parameters. The technique could be used as a progressive transmission system, where successive refinements of the data can be requested by the user. This allows seismologists to first examine a coarse version of waveforms with minimal usage of the channel and then decide where refinements are required. Rate-distortion performance results are presented and comparisons are made with two block transform methods."
Synthetic aperture radar signal data compression using block adaptive quantization,46.58216,jpeg algorithm,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
Proposed data compression schemes for the Galileo S-band contingency mission,46.168427,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Galileo spacecraft is currently on its way to Jupiter and its moons. In April 1991, the high gain antenna (HGA) failed to deploy as commanded. In case the current efforts to deploy the HGA fails, communications during the Jupiter encounters will be through one of two low gain antenna (LGA) on an S-band (2.3 GHz) carrier. A lot of effort has been and will be conducted to attempt to open the HGA. Also various options for improving Galileo's telemetry downlink performance are being evaluated in the event that the HGA will not open at Jupiter arrival. Among all viable options the most promising and powerful one is to perform image and non-image data compression in software onboard the spacecraft. This involves in-flight re-programming of the existing flight software of Galileo's Command and Data Subsystem processors and Attitude and Articulation Control System (AACS) processor, which have very limited computational and memory resources. In this article we describe the proposed data compression algorithms and give their respective compression performance. The planned image compression algorithm is a 4 x 4 or an 8 x 8 multiplication-free integer cosine transform (ICT) scheme, which can be viewed as an integer approximation of the popular discrete cosine transform (DCT) scheme. The implementation complexity of the ICT schemes is much lower than the DCT-based schemes, yet the performances of the two algorithms are indistinguishable. The proposed non-image compression algorith is a Lempel-Ziv-Welch (LZW) variant, which is a lossless universal compression algorithm based on a dynamic dictionary lookup table. We developed a simple and efficient hashing function to perform the string search."
Digital storage and analysis of color Doppler echocardiograms,45.638077,jpeg algorithm,['Life Sciences (General)'],"Color Doppler flow mapping has played an important role in clinical echocardiography. Most of the clinical work, however, has been primarily qualitative. Although qualitative information is very valuable, there is considerable quantitative information stored within the velocity map that has not been extensively exploited so far. Recently, many researchers have shown interest in using the encoded velocities to address the clinical problems such as quantification of valvular regurgitation, calculation of cardiac output, and characterization of ventricular filling. In this article, we review some basic physics and engineering aspects of color Doppler echocardiography, as well as drawbacks of trying to retrieve velocities from video tape data. Digital storage, which plays a critical role in performing quantitative analysis, is discussed in some detail with special attention to velocity encoding in DICOM 3.0 (medical image storage standard) and the use of digital compression. Lossy compression can considerably reduce file size with minimal loss of information (mostly redundant); this is critical for digital storage because of the enormous amount of data generated (a 10 minute study could require 18 Gigabytes of storage capacity). Lossy JPEG compression and its impact on quantitative analysis has been studied, showing that images compressed at 27:1 using the JPEG algorithm compares favorably with directly digitized video images, the current goldstandard. Some potential applications of these velocities in analyzing the proximal convergence zones, mitral inflow, and some areas of future development are also discussed in the article."
"Java Image I/O for VICAR, PDS, and ISIS",45.122734,jpeg algorithm,['Man/System Technology and Life Support'],"This library, written in Java, supports input and output of images and metadata (labels) in the VICAR, PDS image, and ISIS-2 and ISIS-3 file formats. Three levels of access exist. The first level comprises the low-level, direct access to the file. This allows an application to read and write specific image tiles, lines, or pixels and to manipulate the label data directly. This layer is analogous to the C-language ""VICAR Run-Time Library"" (RTL), which is the image I/O library for the (C/C++/Fortran) VICAR image processing system from JPL MIPL (Multimission Image Processing Lab). This low-level library can also be used to read and write labeled, uncompressed images stored in formats similar to VICAR, such as ISIS-2 and -3, and a subset of PDS (image format). The second level of access involves two codecs based on Java Advanced Imaging (JAI) to provide access to VICAR and PDS images in a file-format-independent manner. JAI is supplied by Sun Microsystems as an extension to desktop Java, and has a number of codecs for formats such as GIF, TIFF, JPEG, etc. Although Sun has deprecated the codec mechanism (replaced by IIO), it is still used in many places. The VICAR and PDS codecs allow any program written using the JAI codec spec to use VICAR or PDS images automatically, with no specific knowledge of the VICAR or PDS formats. Support for metadata (labels) is included, but is format-dependent. The PDS codec, when processing PDS images with an embedded VIAR label (""dual-labeled images,"" such as used for MER), presents the VICAR label in a new way that is compatible with the VICAR codec. The third level of access involves VICAR, PDS, and ISIS Image I/O plugins. The Java core includes an ""Image I/O"" (IIO) package that is similar in concept to the JAI codec, but is newer and more capable. Applications written to the IIO specification can use any image format for which a plug-in exists, with no specific knowledge of the format itself."
"Model-based VQ for image data archival, retrieval and distribution",44.750313,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"An ideal image compression technique for image data archival, retrieval and distribution would be one with the asymmetrical computational requirements of Vector Quantization (VQ), but without the complications arising from VQ codebooks. Codebook generation and maintenance are stumbling blocks which have limited the use of VQ as a practical image compression algorithm. Model-based VQ (MVQ), a variant of VQ described here, has the computational properties of VQ but does not require explicit codebooks. The codebooks are internally generated using mean removed error and Human Visual System (HVS) models. The error model assumed is the Laplacian distribution with mean, lambda-computed from a sample of the input image. A Laplacian distribution with mean, lambda, is generated with uniform random number generator. These random numbers are grouped into vectors. These vectors are further conditioned to make them perceptually meaningful by filtering the DCT coefficients from each vector. The DCT coefficients are filtered by multiplying by a weight matrix that is found to be optimal for human perception. The inverse DCT is performed to produce the conditioned vectors for the codebook. The only image dependent parameter used in the generation of codebook is the mean, lambda, that is included in the coded file to repeat the codebook generation process for decoding."
Searching for patterns in remote sensing image databases using neural networks,44.374744,jpeg algorithm,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
Visually Lossless Data Compression for Real-Time Frame/Pushbroom Space Science Imagers,44.364975,jpeg algorithm,['Computer Programming and Software'],"A visually lossless data compression technique is currently being developed for space science applications under the requirement of high-speed push-broom scanning. The technique is also applicable to frame based imaging and is error-resilient in that error propagation is contained within a few scan lines. The algorithm is based on a block transform of a hybrid of modulated lapped transform (MLT) and discrete cosine transform (DCT), or a 2-dimensional lapped transform, followed by bit-plane encoding; this combination results in an embedded bit string with exactly the desirable compression rate as desired by the user. The approach requires no unique table to maximize its performance. The compression scheme performs well on a suite of test images typical of images from spacecraft instruments. Flight qualified hardware implementations are in development; a functional chip set is expected by the end of 2001. The chip set is being designed to compress data in excess of 20 Msamples/sec and support quantizations from 2 to 16 bits."
Sub-band/transform compression of video sequences,42.417747,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
Evaluation of the VIIRS Land Algorithms at Land PEATE,42.164917,jpeg algorithm,['Earth Resources and Remote Sensing'],"The Land Product Evaluation and Algorithm Testing Element (Land PEATE), a component of the Science Data Segment of the National Polar-orbiting Operational Environmental Satellite System (NPOESS) Preparatory Project (NPP), is being developed at the NASA Goddard Space Flight Center (GSFC). The primary task of the Land PEATE is to assess the quality of the Visible Infrared Imaging Radiometer Suite (VIIRS) Land data products made by the Interface Data Processing System (IDPS) using the Operational (OPS) Code during the NPP era and to recommend improvements to the algorithms in the IDPS OPS code. The Land PEATE uses a version of the MODIS Adaptive Processing System (MODAPS), NPPDAPS, that has been modified to produce products from the IDPS OPS code and software provided by the VIIRS Science Team, and uses the MODIS Land Data Operational Product Evaluation (LDOPE) team for evaluation of the data records generated by the NPPDAPS. Land PEATE evaluates the algorithms by comparing data products generated using different versions of the algorithm and also by comparing to heritage products generated from different instrument such as MODIS using various quality assessment tools developed at LDOPE. This paper describes the Land PEATE system and some of the approaches used by the Land PEATE for evaluating the VIIRS Land algorithms during the pre-launch period of the NPP mission and the proposed plan for long term monitoring of the quality of the VIIRS Land products post-launch."
Data compression using Chebyshev transform,42.09884,jpeg algorithm,['Computer Programming and Software'],"The present invention is a method, system, and computer program product for implementation of a capable, general purpose compression algorithm that can be engaged on the fly. This invention has particular practical application with time-series data, and more particularly, time-series data obtained form a spacecraft, or similar situations where cost, size and/or power limitations are prevalent, although it is not limited to such applications. It is also particularly applicable to the compression of serial data streams and works in one, two, or three dimensions. The original input data is approximated by Chebyshev polynomials, achieving very high compression ratios on serial data streams with minimal loss of scientific information."
Video transmission on ATM networks,41.16532,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
Clementine High Resolution Camera Mosaicking Project,40.15378,jpeg algorithm,['Lunar and Planetary Science and Exploration'],"This report constitutes the final report for NASA Contract NASW-5054. This project processed Clementine I high resolution images of the Moon, mosaicked these images together, and created a 22-disk set of compact disk read-only memory (CD-ROM) volumes. The mosaics were produced through semi-automated registration and calibration of the high resolution (HiRes) camera's data against the geometrically and photometrically controlled Ultraviolet/Visible (UV/Vis) Basemap Mosaic produced by the US Geological Survey (USGS). The HiRes mosaics were compiled from non-uniformity corrected, 750 nanometer (""D"") filter high resolution nadir-looking observations. The images were spatially warped using the sinusoidal equal-area projection at a scale of 20 m/pixel for sub-polar mosaics (below 80 deg. latitude) and using the stereographic projection at a scale of 30 m/pixel for polar mosaics. Only images with emission angles less than approximately 50 were used. Images from non-mapping cross-track slews, which tended to have large SPICE errors, were generally omitted. The locations of the resulting image population were found to be offset from the UV/Vis basemap by up to 13 km (0.4 deg.). Geometric control was taken from the 100 m/pixel global and 150 m/pixel polar USGS Clementine Basemap Mosaics compiled from the 750 nm Ultraviolet/Visible Clementine imaging system. Radiometric calibration was achieved by removing the image nonuniformity dominated by the HiRes system's light intensifier. Also provided are offset and scale factors, achieved by a fit of the HiRes data to the corresponding photometrically calibrated UV/Vis basemap, that approximately transform the 8-bit HiRes data to photometric units. The sub-polar mosaics are divided into tiles that cover approximately 1.75 deg. of latitude and span the longitude range of the mosaicked frames. Images from a given orbit are map projected using the orbit's nominal central latitude. Polar mosaics are tiled into squares 2250 pixels on a side, which spans approximately 2.2 deg. Two mosaics are provided for each pole: one corresponding to data acquired while periapsis was in the south, the other while periapsis was in the north. The CD-ROMs also contain ancillary data files that support the HiRes mosaic. These files include browse images with UV/Vis context stored in a Joint Photographic Experts Group (JPEG) format, index files ('imgindx.tab' and 'srcindx.tab') that tabulate the contents of the CD, and documentation files."
JWST Wavefront Control Toolbox,39.30717,jpeg algorithm,['Man/System Technology and Life Support'],"A Matlab-based toolbox has been developed for the wavefront control and optimization of segmented optical surfaces to correct for possible misalignments of James Webb Space Telescope (JWST) using influence functions. The toolbox employs both iterative and non-iterative methods to converge to an optimal solution by minimizing the cost function. The toolbox could be used in either of constrained and unconstrained optimizations. The control process involves 1 to 7 degrees-of-freedom perturbations per segment of primary mirror in addition to the 5 degrees of freedom of secondary mirror. The toolbox consists of a series of Matlab/Simulink functions and modules, developed based on a ""wrapper"" approach, that handles the interface and data flow between existing commercial optical modeling software packages such as Zemax and Code V. The limitations of the algorithm are dictated by the constraints of the moving parts in the mirrors."
Development and Application of Non-Linear Image Enhancement and Multi-Sensor Fusion Techniques for Hazy and Dark Imaging,38.08864,jpeg algorithm,['Avionics and Aircraft Instrumentation'],"The purpose of this research was to develop enhancement and multi-sensor fusion algorithms and techniques to make it safer for the pilot to fly in what would normally be considered Instrument Flight Rules (IFR) conditions, where pilot visibility is severely restricted due to fog, haze or other weather phenomenon. We proposed to use the non-linear Multiscale Retinex (MSR) as the basic driver for developing an integrated enhancement and fusion engine. When we started this research, the MSR was being applied primarily to grayscale imagery such as medical images, or to three-band color imagery, such as that produced in consumer photography: it was not, however, being applied to other imagery such as that produced by infrared image sources. However, we felt that it was possible by using the MSR algorithm in conjunction with multiple imaging modalities such as long-wave infrared (LWIR), short-wave infrared (SWIR), and visible spectrum (VIS), we could substantially improve over the then state-of-the-art enhancement algorithms, especially in poor visibility conditions. We proposed the following tasks: 1) Investigate the effects of applying the MSR to LWIR and SWIR images. This consisted of optimizing the algorithm in terms of surround scales, and weights for these spectral bands; 2) Fusing the LWIR and SWIR images with the VIS images using the MSR framework to determine the best possible representation of the desired features; 3) Evaluating different mixes of LWIR, SWIR and VIS bands for maximum fog and haze reduction, and low light level compensation; 4) Modifying the existing algorithms to work with video sequences. Over the course of the 3 year research period, we were able to accomplish these tasks and report on them at various internal presentations at NASA Langley Research Center, and in presentations and publications elsewhere. A description of the work performed under the tasks is provided in Section 2. The complete list of relevant publications during the research periods is provided in Section 5. This research also resulted in the generation of intellectual property."
1994 Science Information Management and Data Compression Workshop,37.892696,jpeg algorithm,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Studies and simulations of the DigiCipher system,37.49545,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this period the development of simulators for the various high definition television (HDTV) systems proposed to the FCC was continued. The FCC has indicated that it wants the various proposers to collaborate on a single system. Based on all available information this system will look very much like the advanced digital television (ADTV) system with major contributions only from the DigiCipher system. The results of our simulations of the DigiCipher system are described. This simulator was tested using test sequences from the MPEG committee. The results are extrapolated to HDTV video sequences. Once again, some caveats are in order. The sequences used for testing the simulator and generating the results are those used for testing the MPEG algorithm. The sequences are of much lower resolution than the HDTV sequences would be, and therefore the extrapolations are not totally accurate. One would expect to get significantly higher compression in terms of bits per pixel with sequences that are of higher resolution. However, the simulator itself is a valid one, and should HDTV sequences become available, they could be used directly with the simulator. A brief overview of the DigiCipher system is given. Some coding results obtained using the simulator are looked at. These results are compared to those obtained using the ADTV system. These results are evaluated in the context of the CCSDS specifications and make some suggestions as to how the DigiCipher system could be implemented in the NASA network. Simulations such as the ones reported can be biased depending on the particular source sequence used. In order to get more complete information about the system one needs to obtain a reasonable set of models which mirror the various kinds of sources encountered during video coding. A set of models which can be used to effectively model the various possible scenarios is provided. As this is somewhat tangential to the other work reported, the results are included as an appendix."
Software Reduces Radio-Interference Effects in Radar Data,37.390522,jpeg algorithm,['Computer Programming and Software'],"A computer program suppresses the effects of narrow-band radio-frequency interference (RFI) on the data collected by a wide-band radar system. The need for this program arises because some advanced wide-band synthetic-aperture radar systems utilize frequency bands that include frequencies used by other radio services. In this program, the RFI environment is represented by an auto-regressive process, the frequency band of which is narrow relative to that of the radar. Most of the RFI signals, both narrow- and wide-band, are estimated in one pass of a least-mean-square (LMS) adaptive filter. The program implements three popular LMS algorithms: the time-domain LMS, the frequency-domain LMS, and the filter-bank LMS adaptive-filter algorithms. The program can be run in a manual or automatic mode. In the manual mode, the user selects the filter parameters prior to execution. In the automatic mode, the program utilizes median-filter and spectral-estimation techniques plus the variable-step-size LMS algorithm for automatic determination of filter parameters, and the parameters are adaptively changed as functions of the inputs, resulting in better overall performance."
Bandwidth characteristics of multimedia data traffic on a local area network,37.10254,jpeg algorithm,"['SPACE COMMUNICATIONS, SPACECRAFT COMMUNICATIONS, COMMAND AND TRACKING']","Limited spacecraft communication links call for users to investigate the potential use of video compression and multimedia technologies to optimize bandwidth allocations. The objective was to determine the transmission characteristics of multimedia data - motion video, text or bitmap graphics, and files transmitted independently and simultaneously over an ethernet local area network. Commercial desktop video teleconferencing hardware and software and Intel's proprietary Digital Video Interactive (DVI) video compression algorithm were used, and typical task scenarios were selected. The transmission time, packet size, number of packets, and network utilization of the data were recorded. Each data type - compressed motion video, text and/or bitmapped graphics, and a compressed image file - was first transmitted independently and its characteristics recorded. The results showed that an average bandwidth of 7.4 kilobits per second (kbps) was used to transmit graphics; an average bandwidth of 86.8 kbps was used to transmit an 18.9-kilobyte (kB) image file; a bandwidth of 728.9 kbps was used to transmit compressed motion video at 15 frames per second (fps); and a bandwidth of 75.9 kbps was used to transmit compressed motion video at 1.5 fps. Average packet sizes were 933 bytes for graphics, 498.5 bytes for the image file, 345.8 bytes for motion video at 15 fps, and 341.9 bytes for motion video at 1.5 fps. Simultaneous transmission of multimedia data types was also characterized. The multimedia packets used transmission bandwidths of 341.4 kbps and 105.8kbps. Bandwidth utilization varied according to the frame rate (frames per second) setting for the transmission of motion video. Packet size did not vary significantly between the data types. When these characteristics are applied to Space Station Freedom (SSF), the packet sizes fall within the maximum specified by the Consultative Committee for Space Data Systems (CCSDS). The uplink of imagery to SSF may be performed at minimal frame rates and/or within seconds of delay, depending on the user's allocated bandwidth. Further research to identify the acceptable delay interval and its impact on human performance is required. Additional studies in network performance using various video compression algorithms and integrated multimedia techniques are needed to determine the optimal design approach for utilizing SSF's data communications system."
The Suomi National Polar-Orbiting Partnership (SNPP): Continuing NASA Research and Applications,35.456688,jpeg algorithm,['Earth Resources and Remote Sensing']," The Suomi National Polar-orbiting Partnership (SNPP) satellite was successfully launched into a polar orbit on October 28, 2011 carrying 5 remote sensing instruments designed to provide data to improve weather forecasts and to increase understanding of long-term climate change. SNPP provides operational continuity of satellite-based observations for NOAA's Polar-orbiting Operational Environmental Satellites (POES) and continues the long-term record of climate quality observations established by NASA's Earth Observing System (EOS) satellites. In the 2003 to 2011 pre-launch timeframe, NASA's SNPP Science Team assessed the adequacy of the operational Raw Data Records (RDRs), Sensor Data Records (SDRs), and Environmental Data Records (EDRs) from the SNPP instruments for use in NASA Earth Science research, examined the operational algorithms used to produce those data records, and proposed a path forward for the production of climate quality products from SNPP. In order to perform these tasks, a distributed data system, the NASA Science Data Segment (SDS), ingested RDRs, SDRs, and EDRs from the NOAA Archive and Distribution and Interface Data Processing Segments, ADS and IDPS, respectively. The SDS also obtained operational algorithms for evaluation purposes from the NOAA Government Resource for Algorithm Verification, Independent Testing and Evaluation (GRAVITE). Within the NASA SDS, five Product Evaluation and Test Elements (PEATEs) received, ingested, and stored data and performed NASA's data processing, evaluation, and analysis activities. The distributed nature of this data distribution system was established by physically housing each PEATE within one of five Climate Analysis Research Systems (CARS) located at either at a NASA or a university institution. The CARS were organized around 5 key EDRs directly in support of the following NASA Earth Science focus areas: atmospheric sounding, ocean, land, ozone, and atmospheric composition products. The PEATES provided the system level interface with members of the NASA SNPP Science Team and other science investigators within each CARS. A sixth Earth Radiation Budget CARS was established at NASA Langley Research Center (NASA LaRC) to support instrument performance, data evaluation, and analysis for the SNPP Clouds and the Earth's Radiant Budget Energy System (CERES) instrument. Following the 2011 launch of SNPP, spacecraft commissioning, and instrument activation, the NASA SNPP Science Team evaluated the operational RDRs, SDRs, and EDRs produced by the NOAA ADS and IDPS. A key part in that evaluation was the NASA Science Team's independent processing of operational RDRs and SDRs to EDRs using the latest NASA science algorithms. The NASA science evaluation was completed in the December 2012 to April 2014 timeframe with the release of a series of NASA Science Team Discipline Reports. In summary, these reports indicated that the RDRs produced by the SNPP instruments were of sufficiently high quality to be used to create data products suitable for NASA Earth System science and applications. However, the quality of the SDRs and EDRs were found to vary greatly when considering suitability for NASA science. The need for improvements in operational algorithms, adoption of different algorithmic approaches, greater monitoring of on-orbit instrument calibration, greater attention to data product validation, and data reprocessing were prominent findings in the reports. In response to these findings, NASA, in late 2013, directed the NASA SNPP Science Team to use SNPP instrument data to develop data products of sufficiently high quality to enable the continuation of EOS time series data records and to develop innovative, practical applications of SNPP data. This direction necessitated a transition of the SDS data system from its pre-launch assessment mode to one of full data processing and production. To do this, the PEATES, which served as NASA's data product testing environment during the prelaunch and early on-orbit periods, were transitioned to Science Investigator-led Processing Systems (SIPS). The distributed data architecture was maintained in this new system by locating the SIPS at the same institutions at which the CARS and PEATES were located. The SIPS acquire raw SNPP instrument Level 0 (i.e. RDR) data over the full SNPP mission from the NOAA ADS and IDPS through the NASA SDS Data Distribution and Depository Element (SD3E). The SIPS process those data into NASA Level 1, Level 2, and global, gridded Level 3 standard products using peer-reviewed algorithms provided by members of the NASA Science Team. The SIPS work with the NASA SNPP Science Team in obtaining enhanced, refined, or alternate real-time algorithms to support the capabilities of the Direct Readout Laboratory (DRL). All data products, algorithm source codes, coefficients, and auxiliary data used in product generation are archived in an assigned NASA Distributed Active Archive Center (DAAC). "
High-performance compression of astronomical images,35.139427,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Astronomical images have some rather unusual characteristics that make many existing image compression techniques either ineffective or inapplicable. A typical image consists of a nearly flat background sprinkled with point sources and occasional extended sources. The images are often noisy, so that lossless compression does not work very well; furthermore, the images are usually subjected to stringent quantitative analysis, so any lossy compression method must be proven not to discard useful information, but must instead discard only the noise. Finally, the images can be extremely large. For example, the Space Telescope Science Institute has digitized photographic plates covering the entire sky, generating 1500 images each having 14000 x 14000 16-bit pixels. Several astronomical groups are now constructing cameras with mosaics of large CCD's (each 2048 x 2048 or larger); these instruments will be used in projects that generate data at a rate exceeding 100 MBytes every 5 minutes for many years. An effective technique for image compression may be based on the H-transform (Fritze et al. 1977). The method that we have developed can be used for either lossless or lossy compression. The digitized sky survey images can be compressed by at least a factor of 10 with no noticeable losses in the astrometric and photometric properties of the compressed images. The method has been designed to be computationally efficient: compression or decompression of a 512 x 512 image requires only 4 seconds on a Sun SPARCstation 1. The algorithm uses only integer arithmetic, so it is completely reversible in its lossless mode, and it could easily be implemented in hardware for space applications."
Toward Better Modeling of Supercritical Turbulent Mixing,34.460587,jpeg algorithm,['Man/System Technology and Life Support'],"study was done as part of an effort to develop computational models representing turbulent mixing under thermodynamic supercritical (here, high pressure) conditions. The question was whether the large-eddy simulation (LES) approach, developed previously for atmospheric-pressure compressible-perfect-gas and incompressible flows, can be extended to real-gas non-ideal (including supercritical) fluid mixtures. [In LES, the governing equations are approximated such that the flow field is spatially filtered and subgrid-scale (SGS) phenomena are represented by models.] The study included analyses of results from direct numerical simulation (DNS) of several such mixing layers based on the Navier-Stokes, total-energy, and conservation- of-chemical-species governing equations. Comparison of LES and DNS results revealed the need to augment the atmospheric- pressure LES equations with additional SGS momentum and energy terms. These new terms are the direct result of high-density-gradient-magnitude regions found in the DNS and observed experimentally under fully turbulent flow conditions. A model has been derived for the new term in the momentum equation and was found to perform well at small filter size but to deteriorate with increasing filter size. Several alternative models were derived for the new SGS term in the energy equation that would need further investigations to determine if they are too computationally intensive in LES."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,34.42514,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
The 1995 Science Information Management and Data Compression Workshop,34.418804,jpeg algorithm,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
The Space and Earth Science Data Compression Workshop,34.073456,jpeg algorithm,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
Compression of color-mapped images,34.028236,jpeg algorithm,['EARTH RESOURCES AND REMOTE SENSING'],"In a standard image coding scenario, pixel-to-pixel correlation nearly always exists in the data, especially if the image is a natural scene. This correlation is what allows predictive coding schemes (e.g., DPCM) to perform efficient compression. In a color-mapped image, the values stored in the pixel array are no longer directly related to the pixel intensity. Two color indices which are numerically adjacent (close) may point to two very different colors. The correlation still exists, but only via the colormap. This fact can be exploited by sorting the color map to reintroduce the structure. The sorting of colormaps is studied and it is shown how the resulting structure can be used in both lossless and lossy compression of images."
Planetary Transmission Diagnostics,33.884727,jpeg algorithm,['Mechanical Engineering'],"This report presents a methodology for detecting and diagnosing gear faults in the planetary stage of a helicopter transmission. This diagnostic technique is based on the constrained adaptive lifting algorithm. The lifting scheme, developed by Wim Sweldens of Bell Labs, is a time domain, prediction-error realization of the wavelet transform that allows for greater flexibility in the construction of wavelet bases. Classic lifting analyzes a given signal using wavelets derived from a single fundamental basis function. A number of researchers have proposed techniques for adding adaptivity to the lifting scheme, allowing the transform to choose from a set of fundamental bases the basis that best fits the signal. This characteristic is desirable for gear diagnostics as it allows the technique to tailor itself to a specific transmission by selecting a set of wavelets that best represent vibration signals obtained while the gearbox is operating under healthy-state conditions. However, constraints on certain basis characteristics are necessary to enhance the detection of local wave-form changes caused by certain types of gear damage. The proposed methodology analyzes individual tooth-mesh waveforms from a healthy-state gearbox vibration signal that was generated using the vibration separation (synchronous signal-averaging) algorithm. Each waveform is separated into analysis domains using zeros of its slope and curvature. The bases selected in each analysis domain are chosen to minimize the prediction error, and constrained to have the same-sign local slope and curvature as the original signal. The resulting set of bases is used to analyze future-state vibration signals and the lifting prediction error is inspected. The constraints allow the transform to effectively adapt to global amplitude changes, yielding small prediction errors. However, local wave-form changes associated with certain types of gear damage are poorly adapted, causing a significant change in the prediction error. The constrained adaptive lifting diagnostic algorithm is validated using data collected from the University of Maryland Transmission Test Rig and the results are discussed."
STRIPE: Remote Driving Using Limited Image Data,33.858673,jpeg algorithm,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
Conditional Entropy-Constrained Residual VQ with Application to Image Coding,33.67449,jpeg algorithm,['Computer Programming and Software'],"This paper introduces an extension of entropy-constrained residual vector quantization (VQ) where intervector dependencies are exploited. The method, which we call conditional entropy-constrained residual VQ, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. When applied to coding images, the proposed method is shown to achieve better rate-distortion performance than that of entropy-constrained residual vector quantization with less computational complexity and lower memory requirements. Moreover, it can be designed to support progressive transmission in a natural way. It is also shown to outperform some of the best predictive and finite-state VQ techniques reported in the literature. This is due partly to the joint optimization between the residual vector quantizer and a high-order conditional entropy coder as well as the efficiency of the multistage residual VQ structure and the dynamic nature of the prediction."
Studies on image compression and image reconstruction,33.19482,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Techniques for video compression,32.937443,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"In this report, we present our study on multiprocessor implementation of a MPEG2 encoding algorithm. First, we compare two approaches to implementing video standards, VLSI technology and multiprocessor processing, in terms of design complexity, applications, and cost. Then we evaluate the functional modules of MPEG2 encoding process in terms of their computation time. Two crucial modules are identified based on this evaluation. Then we present our experimental study on the multiprocessor implementation of the two crucial modules. Data partitioning is used for job assignment. Experimental results show that high speedup ratio and good scalability can be achieved by using this kind of job assignment strategy."
The 1993 Space and Earth Science Data Compression Workshop,32.69173,jpeg algorithm,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
"NASA Tech Briefs, October 2008",32.611168,jpeg algorithm,['Man/System Technology and Life Support'],"Topics covered include: Control Architecture for Robotic Agent Command and Sensing; Algorithm for Wavefront Sensing Using an Extended Scene; CO2 Sensors Based on Nanocrystalline SnO2 Doped with CuO; Improved Airborne System for Sensing Wildfires; VHF Wide-Band, Dual-Polarization Microstrip-Patch Antenna; Onboard Data Processor for Change-Detection Radar Imaging; Using LDPC Code Constraints to Aid Recovery of Symbol Timing; System for Measuring Flexing of a Large Spaceborne Structure; Integrated Formation Optical Communication and Estimation System; Making Superconducting Welds between Superconducting Wires; Method for Thermal Spraying of Coatings Using Resonant-Pulsed Combustion; Coating Reduces Ice Adhesion; Hybrid Multifoil Aerogel Thermal Insulation; SHINE Virtual Machine Model for In-flight Updates of Critical Mission Software; Mars Image Collection Mosaic Builder; Providing Internet Access to High-Resolution Mars Images; Providing Internet Access to High-Resolution Lunar Images; Expressions Module for the Satellite Orbit Analysis Program Virtual Satellite; Small-Body Extensions for the Satellite Orbit Analysis Program (SOAP); Scripting Module for the Satellite Orbit Analysis Program (SOAP); XML-Based SHINE Knowledge Base Interchange Language; Core Technical Capability Laboratory Management System; MRO SOW Daily Script; Tool for Inspecting Alignment of Twinaxial Connectors; An ATP System for Deep-Space Optical Communication; Polar Traverse Rover Instrument; Expert System Control of Plant Growth in an Enclosed Space; Detecting Phycocyanin-Pigmented Microbes in Reflected Light; DMAC and NMP as Electrolyte Additives for Li-Ion Cells; Mass Spectrometer Containing Multiple Fixed Collectors; Waveguide Harmonic Generator for the SIM; Whispering Gallery Mode Resonator with Orthogonally Reconfigurable Filter Function; Stable Calibration of Raman Lidar Water-Vapor Measurements; Bimaterial Thermal Compensators for WGM Resonators; Root Source Analysis/ValuStream[Trade Mark] - A Methodology for Identifying and Managing Risks; Ensemble: an Architecture for Mission-Operations Software; Object Recognition Using Feature-and Color-Based Methods; On-Orbit Multi-Field Wavefront Control with a Kalman Filter; and The Interplanetary Overlay Networking Protocol Accelerator."
Introduction to Image Processing,32.59979,jpeg algorithm,['Instrumentation and Photography'],No abstract available
Compressed/reconstructed test images for CRAF/Cassini,32.5733,jpeg algorithm,['ASTROPHYSICS'],"A set of compressed, then reconstructed, test images submitted to the Comet Rendezvous Asteroid Flyby (CRAF)/Cassini project is presented as part of its evaluation of near lossless high compression algorithms for representing image data. A total of seven test image files were provided by the project. The seven test images were compressed, then reconstructed with high quality (root mean square error of approximately one or two gray levels on an 8 bit gray scale), using discrete cosine transforms or Hadamard transforms and efficient entropy coders. The resulting compression ratios varied from about 2:1 to about 10:1, depending on the activity or randomness in the source image. This was accomplished without any special effort to optimize the quantizer or to introduce special postprocessing to filter the reconstruction errors. A more complete set of measurements, showing the relative performance of the compression algorithms over a wide range of compression ratios and reconstruction errors, shows that additional compression is possible at a small sacrifice in fidelity."
SEDSAT-1 Technology Development,32.529907,jpeg algorithm,"['Spacecraft Design, Testing and Performance']","The Students for the Exploration and Development of Space Satellite (SEDSAT-1) is an ambitious project to design, build, and fly a generally-accessible low-cost satellite which will 1) act as a technology demonstration to verify the suitability of novel optical, battery, microprocessor, and memory hardware for space flight environments, (2) to advance the understanding of tether dynamics and environmental science through the development of advanced imaging experiments, (3) to act as a communication link for radio amateurs, and (4) to provide graduate and undergraduate students with a unique multi-disciplinary experience in designing complex real-world hardware/software. This report highlights the progress made on this project during the time period from January 2, 1996 to June 1, 1996 at the end of which time the SEASIS 0.7 version software was completed and integrated on the SEASIS breadboard, a functional prototype of the Panoramic Annual Lenses (PAL) camera was developed, the preferred image compression technique was selected, the layout of the SEASIS board was begun, porting of the SCOS operating system to the command data system (CDS) board was begun, a new design for a tether release mechanism was developed, safety circuitry to inhibit tether cutting was developed and prototyped, material was prepared to support a comprehensive safety review of the project which was held at Johnson Space Center (JSC) (which was personally attended by one of the Principal Investigators), and prototype ground software was developed."
The effects of video compression on acceptability of images for monitoring life sciences' experiments,32.501873,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Current plans indicate that there will be a large number of life science experiments carried out during the thirty year-long mission of the Biological Flight Research Laboratory (BFRL) on board Space Station Freedom (SSF). Non-human life science experiments will be performed in the BFRL. Two distinct types of activities have already been identified for this facility: (1) collect, store, distribute, analyze and manage engineering and science data from the Habitats, Glovebox and Centrifuge, (2) perform a broad range of remote science activities in the Glovebox and Habitat chambers in conjunction with the remotely located principal investigator (PI). These activities require extensive video coverage, viewing and/or recording and distribution to video displays on board SSF and to the ground. This paper concentrates mainly on the second type of activity. Each of the two BFRL habitat racks are designed to be configurable for either six rodent habitats per rack, four plant habitats per rack, or a combination of the above. Two video cameras will be installed in each habitat with a spare attachment for a third camera when needed. Therefore, a video system that can accommodate up to 12-18 camera inputs per habitat rack must be considered."
The importance of robust error control in data compression applications,32.333313,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
The CCDS Data Compression Recommendations: Development and Status,32.166733,jpeg algorithm,"['Spacecraft Design, Testing and Performance']","The Consultative Committee for Space Data Systems (CCSDS) has been engaging in recommending data compression standards for space applications. The first effort focused on a lossless scheme that was adopted in 1997. Since then, space missions benefiting from this recommendation range from deep space probes to near Earth observatories. The cost savings result not only from reduced onboard storage and reduced bandwidth, but also in ground archive of mission data. In many instances, this recommendation also enables more science data to be collected for added scientific value. Since 1998, the compression sub-panel of CCSDS has been investigating lossy image compression schemes and is currently working towards a common solution for a single recommendation. The recommendation will fulfill the requirements for remote sensing conducted on space platforms."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",32.116013,jpeg algorithm,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
COxSwAIN: Compressive Sensing for Advanced Imaging and Navigation,32.06535,jpeg algorithm,"['Mathematical and Computer Sciences (General)', 'Communications and Radar']","The COxSwAIN project focuses on building an image and video compression scheme that can be implemented in a small or low-power satellite. To do this, we used Compressive Sensing, where the compression is performed by matrix multiplications on the satellite and reconstructed on the ground. Our paper explains our methodology and demonstrates the results of the scheme, being able to achieve high quality image compression that is robust to noise and corruption."
Compression of regions in the global advanced very high resolution radiometer 1-km data set,32.01834,jpeg algorithm,['COMPUTER SYSTEMS'],"The global advanced very high resolution radiometer (AVHRR) 1-km data set is a 10-band image produced at USGS' EROS Data Center for the study of the world's land surfaces. The image contains masked regions for non-land areas which are identical in each band but vary between data sets. They comprise over 75 percent of this 9.7 gigabyte image. The mask is compressed once and stored separately from the land data which is compressed for each of the 10 bands. The mask is stored in a hierarchical format for multi-resolution decompression of geographic subwindows of the image. The land for each band is compressed by modifying a method that ignores fill values. This multi-spectral region compression efficiently compresses the region data and precludes fill values from interfering with land compression statistics. Results show that the masked regions in a one-byte test image (6.5 Gigabytes) compress to 0.2 percent of the 557,756,146 bytes they occupy in the original image, resulting in a compression ratio of 89.9 percent for the entire image."
Fast image decompression for telebrowsing of images,31.770033,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"Progressive image transmission (PIT) is often used to reduce the transmission time of an image telebrowsing system. A side effect of the PIT is the increase of computational complexity at the viewer's site. This effect is more serious in transform domain techniques than in other techniques. Recent attempts to reduce the side effect are futile as they create another side effect, namely, the discontinuous and unpleasant image build-up. Based on a practical assumption that image blocks to be inverse transformed are generally sparse, this paper presents a method to minimize both side effects simultaneously."
The 1992 4th NASA SERC Symposium on VLSI Design,31.539051,jpeg algorithm,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"Papers from the fourth annual NASA Symposium on VLSI Design, co-sponsored by the IEEE, are presented. Each year this symposium is organized by the NASA Space Engineering Research Center (SERC) at the University of Idaho and is held in conjunction with a quarterly meeting of the NASA Data System Technology Working Group (DSTWG). One task of the DSTWG is to develop new electronic technologies that will meet next generation electronic data system needs. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The NASA SERC is proud to offer, at its fourth symposium on VLSI design, presentations by an outstanding set of individuals from national laboratories, the electronics industry, and universities. These speakers share insights into next generation advances that will serve as a basis for future VLSI design."
Improved image decompression for reduced transform coding artifacts,31.391266,jpeg algorithm,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
The Telecommunications and Data Acquisition Report,30.843445,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"A compilation is presented of articles on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition. In space communications, radio navigation, radio science, and ground based radio and radar astronomy, activities of the Deep Space Network are reported in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA. In the search for extraterrestrial intelligence (SETI), implementation and operations are reported for searching the microwave spectrum."
Innovative Video Diagnostic Equipment for Material Science,30.682173,jpeg algorithm,['Space Processing'],"Materials science experiments under microgravity increasingly rely on advanced optical systems to determine the physical properties of the samples under investigation. This includes video systems with high spatial and temporal resolution. The acquisition, handling, storage and transmission to ground of the resulting video data are very challenging. Since the available downlink data rate is limited, the capability to compress the video data significantly without compromising the data quality is essential. We report on the development of a Digital Video System (DVS) for EML (Electro Magnetic Levitator) which provides real-time video acquisition, high compression using advanced Wavelet algorithms, storage and transmission of a continuous flow of video with different characteristics in terms of image dimensions and frame rates. The DVS is able to operate with the latest generation of high-performance cameras acquiring high resolution video images up to 4Mpixels@60 fps or high frame rate video images up to about 1000 fps@512x512pixels."
Concept report: Experimental vector magnetograph (EXVM) operational configuration balloon flight assembly,30.671144,jpeg algorithm,['AERODYNAMICS'],"The observational limitations of earth bound solar studies has prompted a great deal of interest in recent months in being able to gain new scientific perspectives through, what should prove to be, relatively low cost flight of the magnetograph system. The ground work done by TBE for the solar balloon missions (originally planned for SOUP and GRID) as well as the rather advanced state of assembly of the EXVM has allowed the quick formulation of a mission concept for the 30 cm system currently being assembled. The flight system operational configuration will be discussed as it is proposed for short duration flight (on the order of one day) over the continental United States. Balloon hardware design requirements used in formulation of the concept are those set by the National Science Balloon Facility (NSBF), the support agency under NASA contract for flight services. The concept assumes that the flight hardware assembly would come together from three development sources: the scientific investigator package, the integration contractor package, and the NSBF support system. The majority of these three separate packages can be independently developed; however, the computer control interfaces and telemetry links would require extensive preplanning and coordination. A special section of this study deals with definition of a dedicated telemetry link to be provided by the integration contractor for video image data for pointing system performance verification. In this study the approach has been to capitalize to the maximum extent possible on existing hardware and system design. This is the most prudent step that can be taken to reduce eventual program cost for long duration flights. By fielding the existing EXVM as quickly as possible, experience could be gained from several short duration flight tests before it became necessary to commit to major upgrades for long duration flights of this system or of the larger 60 cm version being considered for eventual development."
Generation of Precision Stimuli for Web-based and At-home Psychophysics,30.652285,jpeg algorithm,['Air Transportation and Safety'],"This oral presentation will present methods for linearizing a display to enable the delivery of precisely controlled stimuli in vision experiments, performing colorimetric calibrations using common objects, and testing for spatial and temporal nonlinearities."
Voice and video transmission using XTP and FDDI,30.34584,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"The use of Xpress Transfer Protocol (XTP) and Fiber Distributed Data Interface (FDDI) provides a high speed and high performance network solution to multimedia transmission that requires high bandwidth. FDDI is an ANSI and ISO standard for a MAC and physical layer protocol that provides a signaling rate of 100 Mbits/sec and fault tolerance. XTP is a transport and network layer protocol designed for high performance and efficiency and is the heart of the SAFENET Lightweight Suite for systems that require performance or realtime communications. The testbed consists of several commercially available Intel based i486 PC's containing off-the-shelf FDDI cards, audio analog-digital converter cards, video interface cards, and XTP software. Unicast, multicast, and duplex audio transmission experiments have been performed using XTP software. Unicast and multicast video transmission is in progress. Several potential commercial applications are described."
Video Compression Study: h.265 vs h.264,30.32109,jpeg algorithm,['Instrumentation and Photography'],"H.265 video compression (also known as High Efficiency Video Encoding (HEVC)) promises to provide double the video quality at half the bandwidth, or the same quality at half the bandwidth of h.264 video compression [1]. This study uses a Tektronix PQA500 to determine the video quality gains by using h.265 encoding. This study also compares two video encoders to see how different implementations of h.264 and h.265 impact video quality at various bandwidths. "
Technology Transfer Report,30.051735,jpeg algorithm,['Technology Utilization and Surface Transportation'],"Since its inception, Goddard has pursued a commitment to technology transfer and commercialization. For every space technology developed, Goddard strives to identify secondary applications. Goddard then provides the technologies, as well as NASA expertise and facilities, to U.S. companies, universities, and government agencies. These efforts are based in Goddard's Technology Commercialization Office. This report presents new technologies, commercialization success stories, and other Technology Commercialization Office activities in 1999."
Image compression system and method having optimized quantization tables,29.99894,jpeg algorithm,['Instrumentation and Photography'],"A digital image compression preprocessor for use in a discrete cosine transform-based digital image compression device is provided. The preprocessor includes a gathering mechanism for determining discrete cosine transform statistics from input digital image data. A computing mechanism is operatively coupled to the gathering mechanism to calculate a image distortion array and a rate of image compression array based upon the discrete cosine transform statistics for each possible quantization value. A dynamic programming mechanism is operatively coupled to the computing mechanism to optimize the rate of image compression array against the image distortion array such that a rate-distortion-optimal quantization table is derived. In addition, a discrete cosine transform-based digital image compression device and a discrete cosine transform-based digital image compression and decompression system are provided. Also, a method for generating a rate-distortion-optimal quantization table, using discrete cosine transform-based digital image compression, and operating a discrete cosine transform-based digital image compression and decompression system are provided."
"Robo-line storage: Low latency, high capacity storage systems over geographically distributed networks",29.944862,jpeg algorithm,['COMPUTER OPERATIONS AND HARDWARE'],"Rapid advances in high performance computing are making possible more complete and accurate computer-based modeling of complex physical phenomena, such as weather front interactions, dynamics of chemical reactions, numerical aerodynamic analysis of airframes, and ocean-land-atmosphere interactions. Many of these 'grand challenge' applications are as demanding of the underlying storage system, in terms of their capacity and bandwidth requirements, as they are on the computational power of the processor. A global view of the Earth's ocean chlorophyll and land vegetation requires over 2 terabytes of raw satellite image data. In this paper, we describe our planned research program in high capacity, high bandwidth storage systems. The project has four overall goals. First, we will examine new methods for high capacity storage systems, made possible by low cost, small form factor magnetic and optical tape systems. Second, access to the storage system will be low latency and high bandwidth. To achieve this, we must interleave data transfer at all levels of the storage system, including devices, controllers, servers, and communications links. Latency will be reduced by extensive caching throughout the storage hierarchy. Third, we will provide effective management of a storage hierarchy, extending the techniques already developed for the Log Structured File System. Finally, we will construct a protototype high capacity file server, suitable for use on the National Research and Education Network (NREN). Such research must be a Cornerstone of any coherent program in high performance computing and communications."
"Adjustable lossless image compression based on a natural splitting of an image into drawing, shading, and fine-grained components",29.838116,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"The compression, or efficient coding, of single band or multispectral still images is becoming an increasingly important topic. While lossy compression approaches can produce reconstructions that are visually close to the original, many scientific and engineering applications require exact (lossless) reconstructions. However, the most popular and efficient lossless compression techniques do not fully exploit the two-dimensional structural links existing in the image data. We describe here a general approach to lossless data compression that effectively exploits two-dimensional structural links of any length. After describing in detail two main variants on this scheme, we discuss experimental results."
Planning/scheduling techniques for VQ-based image compression,29.821598,jpeg algorithm,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
Data compression for full motion video transmission,29.752346,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
Study and simulation of low rate video coding schemes,29.688877,jpeg algorithm,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
GIF Animation of Mode Shapes and Other Data on the Internet,29.630161,jpeg algorithm,['Structural Mechanics'],"The World Wide Web abounds with animated cartoons and advertisements competing for our attention. Most of these figures are animated Graphics Interchange Format (GIF) files. These files contain a series of ordinary GIF images plus control information, and they provide an exceptionally simple, effective way to animate on the Internet. To date, however, this format has rarely been used for technical data, although there is no inherent reason not to do so. This paper describes a procedure for creating high-resolution animated GIFs of mode shapes and other types of structural dynamics data with readily available software. The paper shows three example applications using recent modal test data and video footage of a high-speed sled run. A fairly detailed summary of the GIF file format is provided in the appendix. All of the animations discussed in the paper are posted on the Internet available through the following address: http://sdb-www.larc.nasa.gov/."
Learning random networks for compression of still and moving images,29.594994,jpeg algorithm,['CYBERNETICS'],"Image compression for both still and moving images is an extremely important area of investigation, with numerous applications to videoconferencing, interactive education, home entertainment, and potential applications to earth observations, medical imaging, digital libraries, and many other areas. We describe work on a neural network methodology to compress/decompress still and moving images. We use the 'point-process' type neural network model which is closer to biophysical reality than standard models, and yet is mathematically much more tractable. We currently achieve compression ratios of the order of 120:1 for moving grey-level images, based on a combination of motion detection and compression. The observed signal-to-noise ratio varies from values above 25 to more than 35. The method is computationally fast so that compression and decompression can be carried out in real-time. It uses the adaptive capabilities of a set of neural networks so as to select varying compression ratios in real-time as a function of quality achieved. It also uses a motion detector which will avoid retransmitting portions of the image which have varied little from the previous frame. Further improvements can be achieved by using on-line learning during compression, and by appropriate compensation of nonlinearities in the compression/decompression scheme. We expect to go well beyond the 250:1 compression level for color images with good quality levels."
Landsat Pathfinder tropical forest information management system,29.540148,jpeg algorithm,['DOCUMENTATION AND INFORMATION SCIENCE'],"A Tropical Forest Information Management System_(TFIMS) has been designed to fulfill the needs of HTFIP in such a way that it tracks all aspects of the generation and analysis of the raw satellite data and the derived deforestation dataset. The system is broken down into four components: satellite image selection, processing, data management and archive management. However, as we began to think of how the TFIMS could also be used to make the data readily accessible to all user communities we realized that the initial system was too project oriented and could only be accessed locally. The new system needed development in the areas of data ingest and storage, while at the same time being implemented on a server environment with a network interface accessible via Internet. This paper summarizes the overall design of the existing prototype (version 0) information management system and then presents the design of the new system (version 1). The development of version 1 of the TFIMS is ongoing. There are no current plans for a gradual transition from version 0 to version 1 because the significant changes are in how the data within the HTFIP will be made accessible to the extended community of scientists, policy makers, educators, and students and not in the functionality of the basic system."
Photogrammetry of a 5m Inflatable Space Antenna With Consumer Digital Cameras,29.530537,jpeg algorithm,['Structural Mechanics'],"This paper discusses photogrammetric measurements of a 5m-diameter inflatable space antenna using four Kodak DC290 (2.1 megapixel) digital cameras. The study had two objectives: 1) Determine the photogrammetric measurement precision obtained using multiple consumer-grade digital cameras and 2) Gain experience with new commercial photogrammetry software packages, specifically PhotoModeler Pro from Eos Systems, Inc. The paper covers the eight steps required using this hardware/software combination. The baseline data set contained four images of the structure taken from various viewing directions. Each image came from a separate camera. This approach simulated the situation of using multiple time-synchronized cameras, which will be required in future tests of vibrating or deploying ultra-lightweight space structures. With four images, the average measurement precision for more than 500 points on the antenna surface was less than 0.020 inches in-plane and approximately 0.050 inches out-of-plane."
Reduction of blocking effects for the JPEG baseline image compression standard,134.66074,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"Transform coding has been chosen for still image compression in the Joint Photographic Experts Group (JPEG) standard. Although transform coding performs superior to many other image compression methods and has fast algorithms for implementation, it is limited by a blocking effect at low bit rates. The blocking effect is inherent in all nonoverlapping transforms. This paper presents a technique for reducing blocking while remaining compatible with the JPEG standard. Simulations show that the system results in subjective performance improvements, sacrificing only a marginal increase in bit rate."
Effects of Digitization and JPEG Compression on Land Cover Classification Using Astronaut-Acquired Orbital Photographs,109.859085,jpeg estimate,['Instrumentation and Photography'],"Studies that utilize astronaut-acquired orbital photographs for visual or digital classification require high-quality data to ensure accuracy. The majority of images available must be digitized from film and electronically transferred to scientific users. This study examined the effect of scanning spatial resolution (1200, 2400 pixels per inch [21.2 and 10.6 microns/pixel]), scanning density range option (Auto, Full) and compression ratio (non-lossy [TIFF], and lossy JPEG 10:1, 46:1, 83:1) on digital classification results of an orbital photograph from the NASA - Johnson Space Center archive. Qualitative results suggested that 1200 ppi was acceptable for visual interpretive uses for major land cover types. Moreover, Auto scanning density range was superior to Full density range. Quantitative assessment of the processing steps indicated that, while 2400 ppi scanning spatial resolution resulted in more classified polygons as well as a substantially greater proportion of polygons < 0.2 ha, overall agreement between 1200 ppi and 2400 ppi was quite high. JPEG compression up to approximately 46:1 also did not appear to have a major impact on quantitative classification characteristics. We conclude that both 1200 and 2400 ppi scanning resolutions are acceptable options for this level of land cover classification, as well as a compression ratio at or below approximately 46:1. Auto range density should always be used during scanning because it acquires more of the information from the film. The particular combination of scanning spatial resolution and compression level will require a case-by-case decision and will depend upon memory capabilities, analytical objectives and the spatial properties of the objects in the image."
Estimated spectrum adaptive postfilter and the iterative prepost filtering algirighms,85.84439,jpeg estimate,['Instrumentation and Photography'],The invention presents The Estimated Spectrum Adaptive Postfilter (ESAP) and the Iterative Prepost Filter (IPF) algorithms. These algorithms model a number of image-adaptive post-filtering and pre-post filtering methods. They are designed to minimize Discrete Cosine Transform (DCT) blocking distortion caused when images are highly compressed with the Joint Photographic Expert Group (JPEG) standard. The ESAP and the IPF techniques of the present invention minimize the mean square error (MSE) to improve the objective and subjective quality of low-bit-rate JPEG gray-scale images while simultaneously enhancing perceptual visual quality with respect to baseline JPEG images.
A visual detection model for DCT coefficient quantization,75.29311,jpeg estimate,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
BOREAS RSS-3 Imagery and Snapshots from a Helicopter-Mounted Video Camera,64.14432,jpeg estimate,['Earth Resources and Remote Sensing'],"The BOREAS RSS-3 team collected helicopter-based video coverage of forested sites acquired during BOREAS as well as single-frame ""snapshots"" processed to still images. Helicopter data used in this analysis were collected during all three 1994 IFCs (24-May to 16-Jun, 19-Jul to 10-Aug, and 30-Aug to 19-Sep), at numerous tower and auxiliary sites in both the NSA and the SSA. The VHS-camera observations correspond to other coincident helicopter measurements. The field of view of the camera is unknown. The video tapes are in both VHS and Beta format. The still images are stored in JPEG format."
BOREAS Level-0 C-130 Aerial Photography,63.98207,jpeg estimate,['Earth Resources and Remote Sensing'],"For BOReal Ecosystem-Atmosphere Study (BOREAS), C-130 and other aerial photography was collected to provide finely detailed and spatially extensive documentation of the condition of the primary study sites. The NASA C-130 Earth Resources aircraft can accommodate two mapping cameras during flight, each of which can be fitted with 6- or 12-inch focal-length lenses and black-and-white, natural-color, or color-IR film, depending upon requirements. Both cameras were often in operation simultaneously, although sometimes only the lower resolution camera was deployed. When both cameras were in operation, the higher resolution camera was often used in a more limited fashion. The acquired photography covers the period of April to September 1994. The aerial photography was delivered as rolls of large format (9 x 9 inch) color transparency prints, with imagery from multiple missions (hundreds of prints) often contained within a single roll. A total of 1533 frames were collected from the C-130 platform for BOREAS in 1994. Note that the level-0 C-130 transparencies are not contained on the BOREAS CD-ROM set. An inventory file is supplied on the CD-ROM to inform users of all the data that were collected. Some photographic prints were made from the transparencies. In addition, BORIS staff digitized a subset of the tranparencies and stored the images in JPEG format. The CD-ROM set contains a small subset of the collected aerial photography that were the digitally scanned and stored as JPEG files for most tower and auxiliary sites in the NSA and SSA. See Section 15 for information about how to acquire additional imagery."
High performance compression of science data,60.513634,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Two papers make up the body of this report. One presents a single-pass adaptive vector quantization algorithm that learns a codebook of variable size and shape entries; the authors present experiments on a set of test images showing that with no training or prior knowledge of the data, for a given fidelity, the compression achieved typically equals or exceeds that of the JPEG standard. The second paper addresses motion compensation, one of the most effective techniques used in the interframe data compression. A parallel block-matching algorithm for estimating interframe displacement of blocks with minimum error is presented. The algorithm is designed for a simple parallel architecture to process video in real time."
System considerations for efficient communication and storage of MSTI image data,53.598206,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Ballistic Missile Defense Organization has been developing the capability to evaluate one or more high-rate sensor/hardware combinations by incorporating them as payloads on a series of Miniature Seeker Technology Insertion (MSTI) flights. This publication represents the final report of a 1993 study to analyze the potential impact f data compression and of related communication system technologies on post-MSTI 3 flights. Lossless compression is considered alone and in conjunction with various spatial editing modes. Additionally, JPEG and Fractal algorithms are examined in order to bound the potential gains from the use of lossy compression. but lossless compression is clearly shown to better fit the goals of the MSTI investigations. Lossless compression factors of between 2:1 and 6:1 would provide significant benefits to both on-board mass memory and the downlink. for on-board mass memory, the savings could range from $5 million to $9 million. Such benefits should be possible by direct application of recently developed NASA VLSI microcircuits. It is shown that further downlink enhancements of 2:1 to 3:1 should be feasible thorough use of practical modifications to the existing modulation system and incorporation of Reed-Solomon channel coding. The latter enhancement could also be achieved by applying recently developed VLSI microcircuits."
Visual optimization of DCT quantization matrices for individual images,52.89908,jpeg estimate,['CYBERNETICS'],"Many image compression standards (JPEG, MPEG, H.261) are based on the Discrete Cosine Transform (DCT). However, these standards do not specify the actual DCT quantization matrix. We have previously provided mathematical formulae to compute a perceptually lossless quantization matrix. Here I show how to compute a matrix that is optimized for a particular image. The method treats each DCT coefficient as an approximation to the local response of a visual 'channel'. For a given quantization matrix, the DCT quantization errors are adjusted by contrast sensitivity, light adaptation, and contrast masking, and are pooled non-linearly over the blocks of the image. This yields an 8x8 'perceptual error matrix'. A second non-linear pooling over the perceptual error matrix yields total perceptual error. With this model we may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
An efficient system for reliably transmitting image and video data over low bit rate noisy channels,52.83451,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"This research project is intended to develop an efficient system for reliably transmitting image and video data over low bit rate noisy channels. The basic ideas behind the proposed approach are the following: employ statistical-based image modeling to facilitate pre- and post-processing and error detection, use spare redundancy that the source compression did not remove to add robustness, and implement coded modulation to improve bandwidth efficiency and noise rejection. Over the last six months, progress has been made on various aspects of the project. Through our studies of the integrated system, a list-based iterative Trellis decoder has been developed. The decoder accepts feedback from a post-processor which can detect channel errors in the reconstructed image. The error detection is based on the Huber Markov random field image model for the compressed image. The compression scheme used here is that of JPEG (Joint Photographic Experts Group). Experiments were performed and the results are quite encouraging. The principal ideas here are extendable to other compression techniques. In addition, research was also performed on unequal error protection channel coding, subband vector quantization as a means of source coding, and post processing for reducing coding artifacts. Our studies on unequal error protection (UEP) coding for image transmission focused on examining the properties of the UEP capabilities of convolutional codes. The investigation of subband vector quantization employed a wavelet transform with special emphasis on exploiting interband redundancy. The outcome of this investigation included the development of three algorithms for subband vector quantization. The reduction of transform coding artifacts was studied with the aid of a non-Gaussian Markov random field model. This results in improved image decompression. These studies are summarized and the technical papers included in the appendices."
KRESKA: A compression system for small and very large images,51.878597,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"An effective lossless compression system for grayscale images is presented using finite context variable order Markov models. A new method to accurately estimate the probability of the escape symbol is proposed. The choice of the best model order and rules for selecting context pixels are discussed. Two context precision and two symbol precision techniques to handle noisy image data with Markov models are introduced. Results indicate that finite context variable order Markov models lead to effective lossless compression systems for small and very large images. The system achieves higher compression ratios than some of the better known image compression techniques such as lossless JPEG, JBIG, or FELICS."
"NASA Tech Briefs, September 2008",50.660526,jpeg estimate,['Man/System Technology and Life Support'],"Topics covered include: Nanotip Carpets as Antireflection Surfaces; Nano-Engineered Catalysts for Direct Methanol Fuel Cells; Capillography of Mats of Nanofibers; Directed Growth of Carbon Nanotubes Across Gaps; High-Voltage, Asymmetric-Waveform Generator; Magic-T Junction Using Microstrip/Slotline Transitions; On-Wafer Measurement of a Silicon-Based CMOS VCO at 324 GHz; Group-III Nitride Field Emitters; HEMT Amplifiers and Equipment for their On-Wafer Testing; Thermal Spray Formation of Polymer Coatings; Improved Gas Filling and Sealing of an HC-PCF; Making More-Complex Molecules Using Superthermal Atom/Molecule Collisions; Nematic Cells for Digital Light Deflection; Improved Silica Aerogel Composite Materials; Microgravity, Mesh-Crawling Legged Robots; Advanced Active-Magnetic-Bearing Thrust- Measurement System; Thermally Actuated Hydraulic Pumps; A New, Highly Improved Two-Cycle Engine; Flexible Structural-Health-Monitoring Sheets; Alignment Pins for Assembling and Disassembling Structures; Purifying Nucleic Acids from Samples of Extremely Low Biomass; Adjustable-Viewing-Angle Endoscopic Tool for Skull Base and Brain Surgery; UV-Resistant Non-Spore-Forming Bacteria From Spacecraft-Assembly Facilities; Hard-X-Ray/Soft-Gamma-Ray Imaging Sensor Assembly for Astronomy; Simplified Modeling of Oxidation of Hydrocarbons; Near-Field Spectroscopy with Nanoparticles Deposited by AFM; Light Collimator and Monitor for a Spectroradiometer; Hyperspectral Fluorescence and Reflectance Imaging Instrument; Improving the Optical Quality Factor of the WGM Resonator; Ultra-Stable Beacon Source for Laboratory Testing of Optical Tracking; Transmissive Diffractive Optical Element Solar Concentrators; Delaying Trains of Short Light Pulses in WGM Resonators; Toward Better Modeling of Supercritical Turbulent Mixing; JPEG 2000 Encoding with Perceptual Distortion Control; Intelligent Integrated Health Management for a System of Systems; Delay Banking for Managing Air Traffic; and Spline-Based Smoothing of Airfoil Curvatures."
"Denoising with Three Dimensional Fourier Transform for Three Dimensional Images, Including Image Sequences",44.124977,jpeg estimate,['Instrumentation and Photography'],"A method of mitigating noise in source image data representing pixels of a 3-D image. The ""3-D image"" may be any type of 3-D image, regardless of whether the third dimension is spatial, temporal, or some other parameter. The 3-D image is divided into three-dimensional chunks of pixels. These chunks are apodized and a three-dimensional Fourier transform is performed on each chunk, thereby producing a three-dimensional spectrum of each chunk. The transformed chunks are processed to estimate a noise floor based on spectral values of the pixels within each chunk. A noise threshold is then determined, and the spectrum of each chunk is filtered with a denoising filter based on the noise threshold. The chunks are then inverse transformed, and recombined into a denoised 3-D image."
Improved image decompression for reduced transform coding artifacts,41.349426,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
Photographic Volume Estimation of CPAS Main Parachutes,40.127,jpeg estimate,['Aerodynamics'],"Capsule Parachute Assembly System (CPAS) flight tests regularly stage a helicopter to observe inflation of 116 ft D o ringsail Main parachutes. These side views can be used to generate 3-D models of inflating canopies to estimate enclosed volume. Assuming a surface of revolution is inadequate because reefed canopies in a cluster are elongated due to mutual aerodynamic interference. A method was developed to combine the side views with upward looking HD video to account for non-circular cross sections. Approximating the cross sections as elliptical greatly improves accuracy. But since that correction requires manually tracing projected outlines, the actual irregular shapes can be used to generate high fidelity models. Compensation is also made for apparent tilt angle. Validation was accomplished by comparing perimeter and projected area with known line lengths and/or high quality photogrammetry. "
"The Spatial Vision Tree: A Generic Pattern Recognition Engine- Scientific Foundations, Design Principles, and Preliminary Tree Design",38.638504,jpeg estimate,['Instrumentation and Photography'],"New foundational ideas are used to define a novel approach to generic visual pattern recognition. These ideas proceed from the starting point of the intrinsic equivalence of noise reduction and pattern recognition when noise reduction is taken to its theoretical limit of explicit matched filtering. This led us to think of the logical extension of sparse coding using basis function transforms for both de-noising and pattern recognition to the full pattern specificity of a lexicon of matched filter pattern templates. A key hypothesis is that such a lexicon can be constructed and is, in fact, a generic visual alphabet of spatial vision. Hence it provides a tractable solution for the design of a generic pattern recognition engine. Here we present the key scientific ideas, the basic design principles which emerge from these ideas, and a preliminary design of the Spatial Vision Tree (SVT). The latter is based upon a cryptographic approach whereby we measure a large aggregate estimate of the frequency of occurrence (FOO) for each pattern. These distributions are employed together with Hamming distance criteria to design a two-tier tree. Then using information theory, these same FOO distributions are used to define a precise method for pattern representation. Finally the experimental performance of the preliminary SVT on computer generated test images and complex natural images is assessed."
Clementine High Resolution Camera Mosaicking Project,38.243275,jpeg estimate,['Lunar and Planetary Science and Exploration'],"This report constitutes the final report for NASA Contract NASW-5054. This project processed Clementine I high resolution images of the Moon, mosaicked these images together, and created a 22-disk set of compact disk read-only memory (CD-ROM) volumes. The mosaics were produced through semi-automated registration and calibration of the high resolution (HiRes) camera's data against the geometrically and photometrically controlled Ultraviolet/Visible (UV/Vis) Basemap Mosaic produced by the US Geological Survey (USGS). The HiRes mosaics were compiled from non-uniformity corrected, 750 nanometer (""D"") filter high resolution nadir-looking observations. The images were spatially warped using the sinusoidal equal-area projection at a scale of 20 m/pixel for sub-polar mosaics (below 80 deg. latitude) and using the stereographic projection at a scale of 30 m/pixel for polar mosaics. Only images with emission angles less than approximately 50 were used. Images from non-mapping cross-track slews, which tended to have large SPICE errors, were generally omitted. The locations of the resulting image population were found to be offset from the UV/Vis basemap by up to 13 km (0.4 deg.). Geometric control was taken from the 100 m/pixel global and 150 m/pixel polar USGS Clementine Basemap Mosaics compiled from the 750 nm Ultraviolet/Visible Clementine imaging system. Radiometric calibration was achieved by removing the image nonuniformity dominated by the HiRes system's light intensifier. Also provided are offset and scale factors, achieved by a fit of the HiRes data to the corresponding photometrically calibrated UV/Vis basemap, that approximately transform the 8-bit HiRes data to photometric units. The sub-polar mosaics are divided into tiles that cover approximately 1.75 deg. of latitude and span the longitude range of the mosaicked frames. Images from a given orbit are map projected using the orbit's nominal central latitude. Polar mosaics are tiled into squares 2250 pixels on a side, which spans approximately 2.2 deg. Two mosaics are provided for each pole: one corresponding to data acquired while periapsis was in the south, the other while periapsis was in the north. The CD-ROMs also contain ancillary data files that support the HiRes mosaic. These files include browse images with UV/Vis context stored in a Joint Photographic Experts Group (JPEG) format, index files ('imgindx.tab' and 'srcindx.tab') that tabulate the contents of the CD, and documentation files."
A Method for Rapid Measurement of Contrast Sensitivity on Mobile Touch-Screens,36.760414,jpeg estimate,"['Space Transportation and Safety', 'Air Transportation and Safety']","Touch-screen displays in cell phones and tablet computers are now pervasive, making them an attractive option for vision testing outside of the laboratory or clinic. Here we de- scribe a novel method in which subjects use a finger swipe to indicate the transition from visible to invisible on a grating which is swept in both contrast and frequency. Because a single image can be swiped in about a second, it is practical to use a series of images to zoom in on particular ranges of contrast or frequency, both to increase the accuracy of the measurements and to obtain an estimate of the reliability of the subject. Sensitivities to chromatic and spatio-temporal modulations are easily measured using the same method. A proto- type has been developed for Apple Computer's iPad/iPod/iPhone family of devices, implemented using an open-source scripting environment known as QuIP (QUick Image Processing, http://hsi.arc.nasa.gov/groups/scanpath/research.php). Preliminary data show good agreement with estimates obtained from traditional psychophysical methods as well as newer rapid estimation techniques. Issues relating to device calibration are also discussed."
1994 Science Information Management and Data Compression Workshop,31.966263,jpeg estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on September 26-27, 1994, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival and retrieval of large quantities of data in future Earth and space science missions. It consisted of eleven presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
Low-Complexity Lossless Compression of Hyperspectral Imagery via Adaptive Filtering,31.957754,jpeg estimate,['Optics'],"A low-complexity, adaptive predictive technique for lossless compression of hyperspectral data is presented. The technique relies on the sign algorithm from the repertoire of adaptive filtering. The compression effectiveness obtained with the technique is competitive with that of the best of previously described techniques with similar complexity."
Concept report: Experimental vector magnetograph (EXVM) operational configuration balloon flight assembly,31.236643,jpeg estimate,['AERODYNAMICS'],"The observational limitations of earth bound solar studies has prompted a great deal of interest in recent months in being able to gain new scientific perspectives through, what should prove to be, relatively low cost flight of the magnetograph system. The ground work done by TBE for the solar balloon missions (originally planned for SOUP and GRID) as well as the rather advanced state of assembly of the EXVM has allowed the quick formulation of a mission concept for the 30 cm system currently being assembled. The flight system operational configuration will be discussed as it is proposed for short duration flight (on the order of one day) over the continental United States. Balloon hardware design requirements used in formulation of the concept are those set by the National Science Balloon Facility (NSBF), the support agency under NASA contract for flight services. The concept assumes that the flight hardware assembly would come together from three development sources: the scientific investigator package, the integration contractor package, and the NSBF support system. The majority of these three separate packages can be independently developed; however, the computer control interfaces and telemetry links would require extensive preplanning and coordination. A special section of this study deals with definition of a dedicated telemetry link to be provided by the integration contractor for video image data for pointing system performance verification. In this study the approach has been to capitalize to the maximum extent possible on existing hardware and system design. This is the most prudent step that can be taken to reduce eventual program cost for long duration flights. By fielding the existing EXVM as quickly as possible, experience could be gained from several short duration flight tests before it became necessary to commit to major upgrades for long duration flights of this system or of the larger 60 cm version being considered for eventual development."
Sub-band/transform compression of video sequences,29.836075,jpeg estimate,['COMMUNICATIONS AND RADAR'],"The progress on compression of video sequences is discussed. The overall goal of the research was the development of data compression algorithms for high-definition television (HDTV) sequences, but most of our research is general enough to be applicable to much more general problems. We have concentrated on coding algorithms based on both sub-band and transform approaches. Two very fundamental issues arise in designing a sub-band coder. First, the form of the signal decomposition must be chosen to yield band-pass images with characteristics favorable to efficient coding. A second basic consideration, whether coding is to be done in two or three dimensions, is the form of the coders to be applied to each sub-band. Computational simplicity is of essence. We review the first portion of the year, during which we improved and extended some of the previous grant period's results. The pyramid nonrectangular sub-band coder limited to intra-frame application is discussed. Perhaps the most critical component of the sub-band structure is the design of bandsplitting filters. We apply very simple recursive filters, which operate at alternating levels on rectangularly sampled, and quincunx sampled images. We will also cover the techniques we have studied for the coding of the resulting bandpass signals. We discuss adaptive three-dimensional coding which takes advantage of the detection algorithm developed last year. To this point, all the work on this project has been done without the benefit of motion compensation (MC). Motion compensation is included in many proposed codecs, but adds significant computational burden and hardware expense. We have sought to find a lower-cost alternative featuring a simple adaptation to motion in the form of the codec. In sequences of high spatial detail and zooming or panning, it appears that MC will likely be necessary for the proposed quality and bit rates."
"Investigation of solar active regions at high resolution by balloon flights of the solar optical universal polarimeter, extended definition phase",29.582726,jpeg estimate,['SOLAR PHYSICS'],"Technical studies of the feasibility of balloon flights of the former Spacelab instrument, the Solar Optical Universal Polarimeter, with a modern charge-coupled device (CCD) camera, to study the structure and evolution of solar active regions at high resolution, are reviewed. In particular, different CCD cameras were used at ground-based solar observatories with the SOUP filter, to evaluate their performance and collect high resolution images. High resolution movies of the photosphere and chromosphere were successfully obtained using four different CCD cameras. Some of this data was collected in coordinated observations with the Yohkoh satellite during May-July, 1992, and they are being analyzed scientifically along with simultaneous X-ray observations."
Conditional Entropy-Constrained Residual VQ with Application to Image Coding,29.408306,jpeg estimate,['Computer Programming and Software'],"This paper introduces an extension of entropy-constrained residual vector quantization (VQ) where intervector dependencies are exploited. The method, which we call conditional entropy-constrained residual VQ, employs a high-order entropy conditioning strategy that captures local information in the neighboring vectors. When applied to coding images, the proposed method is shown to achieve better rate-distortion performance than that of entropy-constrained residual vector quantization with less computational complexity and lower memory requirements. Moreover, it can be designed to support progressive transmission in a natural way. It is also shown to outperform some of the best predictive and finite-state VQ techniques reported in the literature. This is due partly to the joint optimization between the residual vector quantizer and a high-order conditional entropy coder as well as the efficiency of the multistage residual VQ structure and the dynamic nature of the prediction."
Studies on image compression and image reconstruction,28.9571,jpeg estimate,['COMMUNICATIONS AND RADAR'],"During this six month period our works concentrated on three, somewhat different areas. We looked at and developed a number of error concealment schemes for use in a variety of video coding environments. This work is described in an accompanying (draft) Masters thesis. In the thesis we describe application of this techniques to the MPEG video coding scheme. We felt that the unique frame ordering approach used in the MPEG scheme would be a challenge to any error concealment/error recovery technique. We continued with our work in the vector quantization area. We have also developed a new type of vector quantizer, which we call a scan predictive vector quantization. The scan predictive VQ was tested on data processed at Goddard to approximate Landsat 7 HRMSI resolution and compared favorably with existing VQ techniques. A paper describing this work is included. The third area is concerned more with reconstruction than compression. While there is a variety of efficient lossless image compression schemes, they all have a common property that they use past data to encode future data. This is done either via taking differences, context modeling, or by building dictionaries. When encoding large images, this common property becomes a common flaw. When the user wishes to decode just a portion of the image, the requirement that the past history be available forces the decoding of a significantly larger portion of the image than desired by the user. Even with intelligent partitioning of the image dataset, the number of pixels decoded may be four times the number of pixels requested. We have developed an adaptive scanning strategy which can be used with any lossless compression scheme and which lowers the additional number of pixels to be decoded to about 7 percent of the number of pixels requested! A paper describing these results is included."
Transform coding for space applications,28.685259,jpeg estimate,['COMMUNICATIONS AND RADAR'],"Data compression coding requirements for aerospace applications differ somewhat from the compression requirements for entertainment systems. On the one hand, entertainment applications are bit rate driven with the goal of getting the best quality possible with a given bandwidth. Science applications are quality driven with the goal of getting the lowest bit rate for a given level of reconstruction quality. In the past, the required quality level has been nothing less than perfect allowing only the use of lossless compression methods (if that). With the advent of better, faster, cheaper missions, an opportunity has arisen for lossy data compression methods to find a use in science applications as requirements for perfect quality reconstruction runs into cost constraints. This paper presents a review of the data compression problem from the space application perspective. Transform coding techniques are described and some simple, integer transforms are presented. The application of these transforms to space-based data compression problems is discussed. Integer transforms have an advantage over conventional transforms in computational complexity. Space applications are different from broadcast or entertainment in that it is desirable to have a simple encoder (in space) and tolerate a more complicated decoder (on the ground) rather than vice versa. Energy compaction with new transforms are compared with the Walsh-Hadamard (WHT), Discrete Cosine (DCT), and Integer Cosine (ICT) transforms."
The 1995 Science Information Management and Data Compression Workshop,28.51416,jpeg estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from the 'Science Information Management and Data Compression Workshop,' which was held on October 26-27, 1995, at the NASA Goddard Space Flight Center, Greenbelt, Maryland. The Workshop explored promising computational approaches for handling the collection, ingestion, archival, and retrieval of large quantities of data in future Earth and space science missions. It consisted of fourteen presentations covering a range of information management and data compression approaches that are being or have been integrated into actual or prototypical Earth or space science data information systems, or that hold promise for such an application. The Workshop was organized by James C. Tilton and Robert F. Cromp of the NASA Goddard Space Flight Center."
The 1992 4th NASA SERC Symposium on VLSI Design,28.148228,jpeg estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"Papers from the fourth annual NASA Symposium on VLSI Design, co-sponsored by the IEEE, are presented. Each year this symposium is organized by the NASA Space Engineering Research Center (SERC) at the University of Idaho and is held in conjunction with a quarterly meeting of the NASA Data System Technology Working Group (DSTWG). One task of the DSTWG is to develop new electronic technologies that will meet next generation electronic data system needs. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The NASA SERC is proud to offer, at its fourth symposium on VLSI design, presentations by an outstanding set of individuals from national laboratories, the electronics industry, and universities. These speakers share insights into next generation advances that will serve as a basis for future VLSI design."
Searching for patterns in remote sensing image databases using neural networks,28.01353,jpeg estimate,['CYBERNETICS'],"We have investigated a method, based on a successful neural network multispectral image classification system, of searching for single patterns in remote sensing databases. While defining the pattern to search for and the feature to be used for that search (spectral, spatial, temporal, etc.) is challenging, a more difficult task is selecting competing patterns to train against the desired pattern. Schemes for competing pattern selection, including random selection and human interpreted selection, are discussed in the context of an example detection of dense urban areas in Landsat Thematic Mapper imagery. When applying the search to multiple images, a simple normalization method can alleviate the problem of inconsistent image calibration. Another potential problem, that of highly compressed data, was found to have a minimal effect on the ability to detect the desired pattern. The neural network algorithm has been implemented using the PVM (Parallel Virtual Machine) library and nearly-optimal speedups have been obtained that help alleviate the long process of searching through imagery."
ICER-3D: A Progressive Wavelet-Based Compressor for Hyperspectral Images,27.76403,jpeg estimate,['Earth Resources and Remote Sensing'],"ICER-3D is a progressive, wavelet-based compressor for hyperspectral images. ICER-3D is derived from the ICER image compressor. ICER-3D can provide lossless and lossy compression, and incorporates an error-containment scheme to limit the effects of data loss during transmission. The three-dimensional wavelet decomposition structure used by ICER-3D exploits correlations in all three dimensions of hyperspectral data sets, while facilitating elimination of spectral ringing artifacts. Correlation is further exploited by a context modeler that effectively exploits spectral dependencies in the wavelet-transformed hyperspectral data. Performance results illustrating the benefits of these features are presented."
The Telecommunications and Data Acquisition Report,27.694109,jpeg estimate,['COMMUNICATIONS AND RADAR'],"A compilation is presented of articles on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition. In space communications, radio navigation, radio science, and ground based radio and radar astronomy, activities of the Deep Space Network are reported in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA. In the search for extraterrestrial intelligence (SETI), implementation and operations are reported for searching the microwave spectrum."
Fast Lossless Compression of Multispectral-Image Data,27.617449,jpeg estimate,['Computer Programming and Software'],"An algorithm that effects fast lossless compression of multispectral-image data is based on low-complexity, proven adaptive-filtering algorithms. This algorithm is intended for use in compressing multispectral-image data aboard spacecraft for transmission to Earth stations. Variants of this algorithm could be useful for lossless compression of three-dimensional medical imagery and, perhaps, for compressing image data in general."
The importance of robust error control in data compression applications,27.593184,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Data compression has become an increasingly popular option as advances in information technology have placed further demands on data storage capabilities. With compression ratios as high as 100:1 the benefits are clear; however, the inherent intolerance of many compression formats to error events should be given careful consideration. If we consider that efficiently compressed data will ideally contain no redundancy, then the introduction of a channel error must result in a change of understanding from that of the original source. While the prefix property of codes such as Huffman enables resynchronisation, this is not sufficient to arrest propagating errors in an adaptive environment. Arithmetic, Lempel-Ziv, discrete cosine transform (DCT) and fractal methods are similarly prone to error propagating behaviors. It is, therefore, essential that compression implementations provide sufficient combatant error control in order to maintain data integrity. Ideally, this control should be derived from a full understanding of the prevailing error mechanisms and their interaction with both the system configuration and the compression schemes in use."
Image-adapted visually weighted quantization matrices for digital image compression,27.221014,jpeg estimate,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
The 1993 Space and Earth Science Data Compression Workshop,27.200758,jpeg estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"The Earth Observing System Data and Information System (EOSDIS) is described in terms of its data volume, data rate, and data distribution requirements. Opportunities for data compression in EOSDIS are discussed."
Fast image decompression for telebrowsing of images,27.127798,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Progressive image transmission (PIT) is often used to reduce the transmission time of an image telebrowsing system. A side effect of the PIT is the increase of computational complexity at the viewer's site. This effect is more serious in transform domain techniques than in other techniques. Recent attempts to reduce the side effect are futile as they create another side effect, namely, the discontinuous and unpleasant image build-up. Based on a practical assumption that image blocks to be inverse transformed are generally sparse, this paper presents a method to minimize both side effects simultaneously."
COxSwAIN: Compressive Sensing for Advanced Imaging and Navigation,27.127798,jpeg estimate,"['Mathematical and Computer Sciences (General)', 'Communications and Radar']","The COxSwAIN project focuses on building an image and video compression scheme that can be implemented in a small or low-power satellite. To do this, we used Compressive Sensing, where the compression is performed by matrix multiplications on the satellite and reconstructed on the ground. Our paper explains our methodology and demonstrates the results of the scheme, being able to achieve high quality image compression that is robust to noise and corruption."
The Space and Earth Science Data Compression Workshop,26.974224,jpeg estimate,['MATHEMATICAL AND COMPUTER SCIENCES (GENERAL)'],"This document is the proceedings from a Space and Earth Science Data Compression Workshop, which was held on March 27, 1992, at the Snowbird Conference Center in Snowbird, Utah. This workshop was held in conjunction with the 1992 Data Compression Conference (DCC '92), which was held at the same location, March 24-26, 1992. The workshop explored opportunities for data compression to enhance the collection and analysis of space and Earth science data. The workshop consisted of eleven papers presented in four sessions. These papers describe research that is integrated into, or has the potential of being integrated into, a particular space and/or Earth science data information system. Presenters were encouraged to take into account the scientists's data requirements, and the constraints imposed by the data collection, transmission, distribution, and archival system."
Automatic Assessment and Reduction of Noise using Edge Pattern Analysis in Non-Linear Image Enhancement,26.374592,jpeg estimate,['Instrumentation and Photography'],"Noise is the primary visibility limit in the process of non-linear image enhancement, and is no longer a statistically stable additive noise in the post-enhancement image. Therefore novel approaches are needed to both assess and reduce spatially variable noise at this stage in overall image processing. Here we will examine the use of edge pattern analysis both for automatic assessment of spatially variable noise and as a foundation for new noise reduction methods."
Technology Transfer Report,26.340588,jpeg estimate,['Technology Utilization and Surface Transportation'],"Since its inception, Goddard has pursued a commitment to technology transfer and commercialization. For every space technology developed, Goddard strives to identify secondary applications. Goddard then provides the technologies, as well as NASA expertise and facilities, to U.S. companies, universities, and government agencies. These efforts are based in Goddard's Technology Commercialization Office. This report presents new technologies, commercialization success stories, and other Technology Commercialization Office activities in 1999."
Image data compression having minimum perceptual error,26.002726,jpeg estimate,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Data compression for full motion video transmission,25.840971,jpeg estimate,['COMMUNICATIONS AND RADAR'],"Clearly transmission of visual information will be a major, if not dominant, factor in determining the requirements for, and assessing the performance of the Space Exploration Initiative (SEI) communications systems. Projected image/video requirements which are currently anticipated for SEI mission scenarios are presented. Based on this information and projected link performance figures, the image/video data compression requirements which would allow link closure are identified. Finally several approaches which could satisfy some of the compression requirements are presented and possible future approaches which show promise for more substantial compression performance improvement are discussed."
ViDI: Virtual Diagnostics Interface,25.47473,jpeg estimate,['Instrumentation and Photography'],"The desire to revolutionize the aircraft design cycle from its currently lethargic pace to a fast turn-around operation enabling the optimization of non-traditional configurations is a critical challenge facing the aeronautics industry. In response, a large scale effort is underway to not only advance the state of the art in wind tunnel testing, computational modeling, and information technology, but to unify these often disparate elements into a cohesive design resource. This paper will address Seamless Data Transfer, the critical central nervous system that will enable a wide variety of varied components to work together."
Photogrammetry of a 5m Inflatable Space Antenna With Consumer Digital Cameras,25.47473,jpeg estimate,['Structural Mechanics'],"This paper discusses photogrammetric measurements of a 5m-diameter inflatable space antenna using four Kodak DC290 (2.1 megapixel) digital cameras. The study had two objectives: 1) Determine the photogrammetric measurement precision obtained using multiple consumer-grade digital cameras and 2) Gain experience with new commercial photogrammetry software packages, specifically PhotoModeler Pro from Eos Systems, Inc. The paper covers the eight steps required using this hardware/software combination. The baseline data set contained four images of the structure taken from various viewing directions. Each image came from a separate camera. This approach simulated the situation of using multiple time-synchronized cameras, which will be required in future tests of vibrating or deploying ultra-lightweight space structures. With four images, the average measurement precision for more than 500 points on the antenna surface was less than 0.020 inches in-plane and approximately 0.050 inches out-of-plane."
Planning/scheduling techniques for VQ-based image compression,25.461168,jpeg estimate,['COMPUTER SYSTEMS'],"The enormous size of the data holding and the complexity of the information system resulting from the EOS system pose several challenges to computer scientists, one of which is data archival and dissemination. More than ninety percent of the data holdings of NASA is in the form of images which will be accessed by users across the computer networks. Accessing the image data in its full resolution creates data traffic problems. Image browsing using a lossy compression reduces this data traffic, as well as storage by factor of 30-40. Of the several image compression techniques, VQ is most appropriate for this application since the decompression of the VQ compressed images is a table lookup process which makes minimal additional demands on the user's computational resources. Lossy compression of image data needs expert level knowledge in general and is not straightforward to use. This is especially true in the case of VQ. It involves the selection of appropriate codebooks for a given data set and vector dimensions for each compression ratio, etc. A planning and scheduling system is described for using the VQ compression technique in the data access and ingest of raw satellite data."
"Robo-line storage: Low latency, high capacity storage systems over geographically distributed networks",25.31154,jpeg estimate,['COMPUTER OPERATIONS AND HARDWARE'],"Rapid advances in high performance computing are making possible more complete and accurate computer-based modeling of complex physical phenomena, such as weather front interactions, dynamics of chemical reactions, numerical aerodynamic analysis of airframes, and ocean-land-atmosphere interactions. Many of these 'grand challenge' applications are as demanding of the underlying storage system, in terms of their capacity and bandwidth requirements, as they are on the computational power of the processor. A global view of the Earth's ocean chlorophyll and land vegetation requires over 2 terabytes of raw satellite image data. In this paper, we describe our planned research program in high capacity, high bandwidth storage systems. The project has four overall goals. First, we will examine new methods for high capacity storage systems, made possible by low cost, small form factor magnetic and optical tape systems. Second, access to the storage system will be low latency and high bandwidth. To achieve this, we must interleave data transfer at all levels of the storage system, including devices, controllers, servers, and communications links. Latency will be reduced by extensive caching throughout the storage hierarchy. Third, we will provide effective management of a storage hierarchy, extending the techniques already developed for the Log Structured File System. Finally, we will construct a protototype high capacity file server, suitable for use on the National Research and Education Network (NREN). Such research must be a Cornerstone of any coherent program in high performance computing and communications."
A Survey of Complex Object Technologies for Digital Libraries,25.206398,jpeg estimate,['Documentation and Information Science'],"Many early web-based digital libraries (DLs) had implicit assumptions reflected in their architecture that the unit of focus in the DL (frequently ""reports"" or ""e-prints"") would only be manifested in a single, or at most a few, common file formats such as PDF or PostScript. DLs have now matured to the point where their contents are commonly no longer simple files. Complex objects in DLs have emerged from in response to various requirements, including: simple aggregation of formats and supporting files, bundling additional information to aid digital preservation, creating opaque digital objects for e-commerce applications, and the incorporation of dynamic services with the traditional data files. We examine a representative (but not necessarily exhaustive) number of current and recent historical web-based complex object technologies and projects that are applicable to DLs: Aurora, Buckets, ComMentor, Cryptolopes, Digibox, Document Management Alliance, FEDORA, Kahn-Wilensky Framework Digital Objects, Metadata Encoding & Transmission Standard, Multivalent Documents, Open eBooks, VERS Encapsulated Objects, and the Warwick Framework."
GIF Animation of Mode Shapes and Other Data on the Internet,25.1223,jpeg estimate,['Structural Mechanics'],"The World Wide Web abounds with animated cartoons and advertisements competing for our attention. Most of these figures are animated Graphics Interchange Format (GIF) files. These files contain a series of ordinary GIF images plus control information, and they provide an exceptionally simple, effective way to animate on the Internet. To date, however, this format has rarely been used for technical data, although there is no inherent reason not to do so. This paper describes a procedure for creating high-resolution animated GIFs of mode shapes and other types of structural dynamics data with readily available software. The paper shows three example applications using recent modal test data and video footage of a high-speed sled run. A fairly detailed summary of the GIF file format is provided in the appendix. All of the animations discussed in the paper are posted on the Internet available through the following address: http://sdb-www.larc.nasa.gov/."
Imaging for Hypersonic Experimental Aeroheating Testing (IHEAT) Version 4.0: User Manual,25.073933,jpeg estimate,"['Fluid Mechanics and Thermodynamics', 'Computer Programming and Software']","The IHEAT v4.0 software is a data reduction code for global thermography data acquired in the NASA Langley Aerothermodynamics Laboratory (LAL) hypersonic wind tunnels. IHEAT uses red and green color-intensity data from two-dimensional images of wind tunnel models to compute temperatures and heat-transfer rates using a semi-infinite, one-dimensional heat transfer approximation at each image pixel. Multiple automated tools in IHEAT v4.0 decrease the time required to reduce the data from a phosphor thermography wind tunnel run. Data at one or all of the image pixel locations can be exported to computer files for further analysis. The prior version of IHEAT, v3.2, was written in PV-WAVE (now owned by Rogue Wave Software) in 1994 and was limited in functionality to fit within the memory constraints of the available computers at the time. IHEAT v4.0 is written in MATLAB by MathWorks and contains several new features that leverage the increase in available memory of the current computers. A Piecewise tool permits the user to extract data along a segmented line cut that can follow interesting features in the image better than the single, straight line cuts that were possible with the legacy Length and Profile tools. The new Load Run and Batch tools facilitate batch processing by loading in all of the input files and images for a run at the same time. Load Run permits the user to process the available run images manually, while Batch automatically saves heat transfer data from all of the images based on the analysis previously performed on a single frame. IHEAT v4.0 also can automatically calculate the temporal collapse of reference line cuts from the time history heating data for a run to indicate the appropriate frame to reduce for each run. The IHEAT v4.0 source code was compiled into a standalone executable file that can be accessed remotely from several computers with different operating systems, simultaneously. The software is run through the MATLAB Compiler Runtime engine, and therefore, IHEAT does not require a software license to run. Any software commands executed in the IHEAT v4.0 code will not affect other similar applications running on the same machine. Similarly, changes to the parent software do not affect a compiled code. These features of IHEAT v4.0 are improvements over the legacy v3.2 code, which required regular maintenance to avoid losing functionality as the PVWAVE  programming language was upgraded."
"A Scalable, Out-of-Band Diagnostics Architecture for International Space Station Systems Support",24.859776,jpeg estimate,"['Spacecraft Design, Testing and Performance']","The computational infrastructure of the International Space Station (ISS) is a dynamic system that supports multiple vehicle subsystems such as Caution and Warning, Electrical Power Systems and Command and Data Handling (C&DH), as well as scientific payloads of varying size and complexity. The dynamic nature of the ISS configuration coupled with the increased demand for payload support places a significant burden on the inherently resource constrained computational infrastructure of the ISS. Onboard system diagnostics applications are hosted on computers that are elements of the avionics network while ground-based diagnostic applications receive only a subset of available telemetry, down-linked via S-band communications. In this paper we propose a scalable, out-of-band diagnostics architecture for ISS systems support that uses a read-only connection for C&DH data acquisition, which provides a lower cost of deployment and maintenance (versus a higher criticality readwrite connection). The diagnostics processing burden is off-loaded from the avionics network to elements of the on-board LAN that have a lower overall cost of operation and increased computational capacity. A superset of diagnostic data, richer in content than the configured telemetry, is made available to Advanced Diagnostic System (ADS) clients running on wireless handheld devices, affording the crew greater mobility for troubleshooting and providing improved insight into vehicle state. The superset of diagnostic data is made available to the ground in near real-time via an out-of band downlink, providing a high level of fidelity between vehicle state and test, training and operational facilities on the ground."
Development of a Prototype Model-Form Uncertainty Knowledge Base,24.648949,jpeg estimate,['Statistics and Probability'],"Uncertainties are generally classified as either aleatory or epistemic. Aleatory uncertainties are those attributed to random variation, either naturally or through manufacturing processes. Epistemic uncertainties are generally attributed to a lack of knowledge. One type of epistemic uncertainty is called model-form uncertainty. The term model-form means that among the choices to be made during a design process within an analysis, there are different forms of the analysis process, which each give different results for the same configuration at the same flight conditions. Examples of model-form uncertainties include the grid density, grid type, and solver type used within a computational fluid dynamics code, or the choice of the number and type of model elements within a structures analysis. The objectives of this work are to identify and quantify a representative set of model-form uncertainties and to make this information available to designers through an interactive knowledge base (KB). The KB can then be used during probabilistic design sessions, so as to enable the possible reduction of uncertainties in the design process through resource investment. An extensive literature search has been conducted to identify and quantify typical model-form uncertainties present within aerospace design. An initial attempt has been made to assemble the results of this literature search into a searchable KB, usable in real time during probabilistic design sessions. A concept of operations and the basic structure of a model-form uncertainty KB are described. Key operations within the KB are illustrated. Current limitations in the KB, and possible workarounds are explained."
Advances in Remote Sensing for Vegetation Dynamics and Agricultural Management,24.531477,jpeg estimate,['Earth Resources and Remote Sensing'],"Spaceborne remote sensing has led to great advances in the global monitoring of vegetation. For example, the NASA Global Inventory Modeling and Mapping Studies (GIMMS) group has developed widely used datasets from the Advanced Very High Resolution Radiometer (AVHRR) sensors as well as the Moderate Resolution Imaging Spectroradiometer (MODIS) map imagery and normalized difference vegetation index datasets. These data are valuable for analyzing vegetation trends and variability at the regional and global levels. Numerous studies have investigated such trends and variability for both natural vegetation (e.g., re-greening of the Sahel, shifts in the Eurasian boreal forest, Amazonian drought sensitivity) and crops (e.g., impacts of extremes on agricultural production). Here, a critical overview is presented on recent developments and opportunities in the use of remote sensing for monitoring vegetation and crop dynamics.


"
Image compression system and method having optimized quantization tables,24.42151,jpeg estimate,['Instrumentation and Photography'],"A digital image compression preprocessor for use in a discrete cosine transform-based digital image compression device is provided. The preprocessor includes a gathering mechanism for determining discrete cosine transform statistics from input digital image data. A computing mechanism is operatively coupled to the gathering mechanism to calculate a image distortion array and a rate of image compression array based upon the discrete cosine transform statistics for each possible quantization value. A dynamic programming mechanism is operatively coupled to the computing mechanism to optimize the rate of image compression array against the image distortion array such that a rate-distortion-optimal quantization table is derived. In addition, a discrete cosine transform-based digital image compression device and a discrete cosine transform-based digital image compression and decompression system are provided. Also, a method for generating a rate-distortion-optimal quantization table, using discrete cosine transform-based digital image compression, and operating a discrete cosine transform-based digital image compression and decompression system are provided."
Learning random networks for compression of still and moving images,24.421028,jpeg estimate,['CYBERNETICS'],"Image compression for both still and moving images is an extremely important area of investigation, with numerous applications to videoconferencing, interactive education, home entertainment, and potential applications to earth observations, medical imaging, digital libraries, and many other areas. We describe work on a neural network methodology to compress/decompress still and moving images. We use the 'point-process' type neural network model which is closer to biophysical reality than standard models, and yet is mathematically much more tractable. We currently achieve compression ratios of the order of 120:1 for moving grey-level images, based on a combination of motion detection and compression. The observed signal-to-noise ratio varies from values above 25 to more than 35. The method is computationally fast so that compression and decompression can be carried out in real-time. It uses the adaptive capabilities of a set of neural networks so as to select varying compression ratios in real-time as a function of quality achieved. It also uses a motion detector which will avoid retransmitting portions of the image which have varied little from the previous frame. Further improvements can be achieved by using on-line learning during compression, and by appropriate compensation of nonlinearities in the compression/decompression scheme. We expect to go well beyond the 250:1 compression level for color images with good quality levels."
A Numerical Investigation of Turbine Noise Source Hierarchy and Its Acoustic Transmission Characteristics: Proof-of-Concept Progress,24.380146,jpeg estimate,['Acoustics'],A CFD-based simulation of single-stage turbine was done using the TURBO code to assess its viability for determining acoustic transmission through blade rows. Temporal and spectral analysis of the unsteady pressure data from the numerical simulations showed the allowable Tyler-Sofrin modes that are consistent with expectations. This indicated that high-fidelity acoustic transmission calculations are feasible with TURBO.
SOLAR-B Mission Extreme Ultraviolet (EUV) Imaging Spectrometer (EIS) Instrument Components,24.131031,jpeg estimate,['Spacecraft Instrumentation and Astrionics'],"This Monthly Progress Report covers the reporting period through June 2001, Phase C/D, Detailed Design and Development Through Launch Plus Thirty Days, for selected components and subsystems of the Extreme ultraviolet Imaging Spectrometer (EIS) instrument, hereafter referred to as EIS Instrument Components. This document contains the program status through the reporting period and forecasts the status for the upcoming reporting period."
Using Open and Interoperable Ways to Publish and Access LANCE AIRS Near-Real Time Data,24.078257,jpeg estimate,['Computer Systems'],"The Atmospheric Infrared Sounder (AIRS) Near-Real Time (NRT) data from the Land Atmosphere Near real-time Capability for EOS (LANCE) element at the Goddard Earth Sciences Data and Information Services Center (GES DISC) provides information on the global and regional atmospheric state, with very low temporal latency, to support climate research and improve weather forecasting. An open and interoperable platform is useful to facilitate access to, and integration of, LANCE AIRS NRT data. As Web services technology has matured in recent years, a new scalable Service-Oriented Architecture (SOA) is emerging as the basic platform for distributed computing and large networks of interoperable applications. Following the provide-register-discover-consume SOA paradigm, this presentation discusses how to use open-source geospatial software components to build Web services for publishing and accessing AIRS NRT data, explore the metadata relevant to registering and discovering data and services in the catalogue systems, and implement a Web portal to facilitate users' consumption of the data and services."
Study and simulation of low rate video coding schemes,23.941696,jpeg estimate,['COMMUNICATIONS AND RADAR'],"The semiannual report is included. Topics covered include communication, information science, data compression, remote sensing, color mapped images, robust coding scheme for packet video, recursively indexed differential pulse code modulation, image compression technique for use on token ring networks, and joint source/channel coder design."
Low Resolution Picture Transmission (LRPT) Demonstration System,23.929184,jpeg estimate,['Communications and Radar'],"Low-Resolution Picture Transmission (LRPT) is a proposed standard for direct broadcast transmission of satellite weather images. This standard is a joint effort by the European Organization for the Exploitation of Meteorological Satellites (EUMETSAT) and NOAA. As a digital transmission scheme, its purpose is to replace the current analog Automatic Picture Transmission (APT) system for use in the Meteorological Operational (METOP) satellites. GSFC has been tasked to build an LRPT Demonstration System (LDS). Its main objective is to develop or demonstrate the feasibility of a low-cost receiver utilizing a PC as the primary processing component and determine the performance of the protocol in the simulated Radio Frequency (RF) environment. The approach would consist of two phases."
Remote sensing and the Mississippi high accuracy reference network,23.834843,jpeg estimate,['EARTH RESOURCES AND REMOTE SENSING'],"Since 1986, NASA's Commercial Remote Sensing Program (CRSP) at Stennis Space Center has supported commercial remote sensing partnerships with industry. CRSP's mission is to maximize U.S. market exploitation of remote sensing and related space-based technologies and to develop advanced technical solutions for spatial information requirements. Observation, geolocation, and communications technologies are converging and their integration is critical to realize the economic potential for spatial informational needs. Global positioning system (GPS) technology enables a virtual revolution in geopositionally accurate remote sensing of the earth. A majority of states are creating GPS-based reference networks, or high accuracy reference networks (HARN). A HARN can be defined for a variety of local applications and tied to aerial or satellite observations to provide an important contribution to geographic information systems (GIS). This paper details CRSP's experience in the design and implementation of a HARN in Mississippi and the design and support of future applications of integrated earth observations, geolocation, and communications technology."
The Fifth NASA Symposium on VLSI Design,23.397087,jpeg estimate,['ELECTRONICS AND ELECTRICAL ENGINEERING'],"The fifth annual NASA Symposium on VLSI Design had 13 sessions including Radiation Effects, Architectures, Mixed Signal, Design Techniques, Fault Testing, Synthesis, Signal Processing, and other Featured Presentations. The symposium provides insights into developments in VLSI and digital systems which can be used to increase data systems performance. The presentations share insights into next generation advances that will serve as a basis for future VLSI design."
"Power, Avionics and Software - Phase 1.0:",23.152046,jpeg estimate,"['Space Communications, Spacecraft Communications, Command and Tracking', 'Computer Programming and Software']","This report describes Power, Avionics and Software (PAS) 1.0 subsystem integration testing and test results that occurred in August and September of 2013. This report covers the capabilities of each PAS assembly to meet integration test objectives for non-safety critical, non-flight, non-human-rated hardware and software development. This test report is the outcome of the first integration of the PAS subsystem and is meant to provide data for subsequent designs, development and testing of the future PAS subsystems. The two main objectives were to assess the ability of the PAS assemblies to exchange messages and to perform audio testing of both inbound and outbound channels. This report describes each test performed, defines the test, the data, and provides conclusions and recommendations."
Studies and simulations of the DigiCipher system,23.04088,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"During this period the development of simulators for the various high definition television (HDTV) systems proposed to the FCC was continued. The FCC has indicated that it wants the various proposers to collaborate on a single system. Based on all available information this system will look very much like the advanced digital television (ADTV) system with major contributions only from the DigiCipher system. The results of our simulations of the DigiCipher system are described. This simulator was tested using test sequences from the MPEG committee. The results are extrapolated to HDTV video sequences. Once again, some caveats are in order. The sequences used for testing the simulator and generating the results are those used for testing the MPEG algorithm. The sequences are of much lower resolution than the HDTV sequences would be, and therefore the extrapolations are not totally accurate. One would expect to get significantly higher compression in terms of bits per pixel with sequences that are of higher resolution. However, the simulator itself is a valid one, and should HDTV sequences become available, they could be used directly with the simulator. A brief overview of the DigiCipher system is given. Some coding results obtained using the simulator are looked at. These results are compared to those obtained using the ADTV system. These results are evaluated in the context of the CCSDS specifications and make some suggestions as to how the DigiCipher system could be implemented in the NASA network. Simulations such as the ones reported can be biased depending on the particular source sequence used. In order to get more complete information about the system one needs to obtain a reasonable set of models which mirror the various kinds of sources encountered during video coding. A set of models which can be used to effectively model the various possible scenarios is provided. As this is somewhat tangential to the other work reported, the results are included as an appendix."
Distant Operational Care Centre: Design Project Report,22.902481,jpeg estimate,['Astronautics (General)'],"The goal of this project is to outline the design of the Distant Operational Care Centre (DOCC), a modular medical facility to maintain human health and performance in space, that is adaptable to a range of remote human habitats. The purpose of this project is to outline a design, not to go into a complete technical specification of a medical facility for space. This project involves a process to produce a concise set of requirements, addressing the fundamental problems and issues regarding all aspects of a space medical facility for the future. The ideas presented here are at a high level, based on existing, researched, and hypothetical technologies. Given the long development times for space exploration, the outlined concepts from this project embodies a collection of identified problems, and corresponding proposed solutions and ideas, ready to contribute to future space exploration efforts. In order to provide a solid extrapolation and speculation in the context of the future of space medicine, the extent of this project's vision is roughly within the next two decades. The Distant Operational Care Centre (DOCC) is a modular medical facility for space. That is, its function is to maintain human health and performance in space environments, through prevention, diagnosis, and treatment. Furthermore, the DOCC must be adaptable to meet the environmental requirements of different remote human habitats, and support a high quality of human performance. To meet a diverse range of remote human habitats, the DOCC concentrates on a core medical capability that can then be adapted. Adaptation would make use of the DOCC's functional modularity, providing the ability to replace, add, and modify core functions of the DOCC by updating hardware, operations, and procedures. Some of the challenges to be addressed by this project include what constitutes the core medical capability in terms of hardware, operations, and procedures, and how DOCC can be adapted to different remote habitats."
Progressive transmission and compression images,22.19231,jpeg estimate,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
"NASAwide electronic publishing system: Prototype STI electronic document distribution, stage-4 evaluation report",21.976841,jpeg estimate,['Documentation and Information Science'],"This evaluation report contains an introduction, seven chapters, and five appendices. The Introduction describes the purpose, conceptual frame work, functional description, and technical report server of the STI Electronic Document Distribution (EDD) project. Chapter 1 documents the results of the prototype STI EDD in actual operation. Chapter 2 documents each NASA center's post processing publication processes. Chapter 3 documents each center's STI software, hardware, and communications configurations. Chapter 7 documents STI EDD policy, practices, and procedures. The appendices, which arc contained in Part 2 of this document, consist of (1) STI EDD Project Plan, (2) Team members, (3) Phasing Schedules, (4) Accessing On-line Reports, and (5) Creating an HTML File and Setting Up an xTRS. In summary, Stage 4 of the NASAwide Electronic Publishing System is the final phase of its implementation through the prototyping and gradual integration of each NASA center's electronic printing systems, desktop publishing systems, and technical report servers to be able to provide to NASA's engineers, researchers, scientists, and external users the widest practicable and appropriate dissemination of information concerning its activities and the result thereof to their work stations."
Progressive Transmission and Compression of Images,21.763512,jpeg estimate,['Communications and Radar'],"We describe an image data compression strategy featuring progressive transmission. The method exploits subband coding and arithmetic coding for compression. We analyze the Laplacian probability density, which closely approximates the statistics of individual subbands, to determine a strategy for ordering the compressed subband data in a way that improves rate-distortion performance. Results are presented for a test image."
Manufacturing Planning Guide,21.706276,jpeg estimate,['Administration and Management'],"Manufacturing process, milestones and inputs are unknowns to first-time users of the manufacturing facilities. The Manufacturing Planning Guide aids in establishing expectations for both NASA and non-NASA facility customers. The potential audience for this guide includes both internal and commercial spaceflight hardware/software developers. It is intended to assist their project engineering personnel in manufacturing planning and execution. Material covered includes a roadmap of the manufacturing process, roles and responsibilities of facility and user, major milestones, facility capabilities, and inputs required by the facility. Samples of deliverables, products, and inputs necessary to define test scope, cost, and schedule are included as an appendix to the guide."
LSP Composite Susbtrate Destructive Evaluation Test Assessment Manual,21.655067,jpeg estimate,"['Composite Materials', 'Aircraft Design, Testing and Performance']","This document specifies the processes to perform post-strike destructive damage evaluation of tested CFRP panels.It is recognized that many factors besides lightning damage protection are involved in the selection of an appropriate Lightning Strike Protection (LSP) for a particular system (e.g., cost, weight, corrosion resistance, shielding effectiveness, etc.). This document strives primarily to address the standardized generation of damage protection performance data."
Microhard MHX 2420 Orbital Performance Evaluation Using RT Logic T400CS,21.655067,jpeg estimate,['Communications and Radar'],"A major upfront cost of building low cost Nanosatellites is the communications sub-system. Most radios built for space missions cost over $4,000 per unit. This exceeds many budgets. One possible cost effective solution is the Microhard MHX2420, a commercial off-the-shelf transceiver with a unit cost under $1000. This paper aims to support the Nanosatellite community seeking an inexpensive radio by characterizing Microhard's performance envelope. Though not intended for space operations, the ability to test edge cases and increase average data transfer speeds through optimization positions this radio as a solution for Nanosatellite communications by expanding usage to include more missions. The second objective of this paper is to test and verify the optimal radio settings for the most common cases to improve downlinking. All tests were conducted with the aid of the RT Logic T400CS, a hardware-in-the-loop channel simulator designed to emulate real-world radio frequency (RF) link effects. This study provides recommended settings to optimize the downlink speed as well as the environmental parameters that cause the link to fail."
A Review of the New AVIRIS Data Processing System,21.42942,jpeg estimate,['Documentation and Information Science'],"The processing of AVIRIS data - from the Metrum Very Large Data Store (VLDS) flight tape to delivered data products - has traditionally been performed in essentially the same way, from the beginning of the AVIRIS project up to and including the 1996 flight season. Starting with the 1997 flight season, a drastically different paradigm has been used for the processing of AVIRIS data. This change was made possible by the recent development of and related availability of affordable data storage devices."
The Telecommunications and Data Acquisition Report,21.336662,jpeg estimate,['COMMUNICATIONS AND RADAR'],"This quarterly publication provides archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA). In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, supporting research and technology, implementation, and operations. Also included are standards activity at JPL for space data and information systems and reimbursable DSN work performed for other space agencies through NASA."
Synthetic aperture radar signal data compression using block adaptive quantization,21.246735,jpeg estimate,['COMPUTER SYSTEMS'],"This paper describes the design and testing of an on-board SAR signal data compression algorithm for ESA's ENVISAT satellite. The Block Adaptive Quantization (BAQ) algorithm was selected, and optimized for the various operational modes of the ASAR instrument. A flexible BAQ scheme was developed which allows a selection of compression ratio/image quality trade-offs. Test results show the high quality of the SAR images processed from the reconstructed signal data, and the feasibility of on-board implementation using a single ASIC."
The Telecommunications and Data Acquisition Report,21.233543,jpeg estimate,['COMMUNICATIONS AND RADAR'],"Archival reports on developments in programs managed by JPL's Office of Telecommunications and Data Acquisition (TDA) are provided. In space communications, radio navigation, radio science, and ground-based radio and radar astronomy, it reports on activities of the Deep Space Network (DSN) in planning, in supporting research and technology, in implementation, and in operations. Also included is standards activity at JPL for space data and information. In the search for extraterrestrial intelligence (SETI), the TDA Progress Report reports on implementation and operations for searching the microwave spectrum. Topics covered include tracking and ground-based navigation; communications, spacecraft-ground; station control and system technology; capabilities for new projects; network upgrade and sustaining; network operations and operations support; and TDA program management and analysis."
Compressing subbanded image data with Lempel-Ziv-based coders,21.103119,jpeg estimate,['COMMUNICATIONS AND RADAR'],"A method of improving the compression of image data using Lempel-Ziv-based coding is presented. Image data is first processed with a simple transform, such as the Walsh Hadamard Transform, to produce subbands. The subbanded data can be rounded to eight bits or it can be quantized for higher compression at the cost of some reduction in the quality of the reconstructed image. The data is then run-length coded to take advantage of the large runs of zeros produced by quantization. Compression results are presented and contrasted with a subband compression method using quantization followed by run-length coding and Huffman coding. The Lempel-Ziv-based coding in conjunction with run-length coding produces the best compression results at the same reconstruction quality (compared with the Huffman-based coding) on the image data used."
"Recent Advances in Registration, Integration and Fusion of Remotely Sensed Data: Redundant Representations and Frames",21.077686,jpeg estimate,"['Earth Resources and Remote Sensing', 'Mathematical and Computer Sciences (General)']","In recent years, sophisticated mathematical techniques have been successfully applied to the field of remote sensing to produce significant advances in applications such as registration, integration and fusion of remotely sensed data. Registration, integration and fusion of multiple source imagery are the most important issues when dealing with Earth Science remote sensing data where information from multiple sensors, exhibiting various resolutions, must be integrated. Issues ranging from different sensor geometries, different spectral responses, differing illumination conditions, different seasons, and various amounts of noise need to be dealt with when designing an image registration, integration or fusion method. This tutorial will first define the problems and challenges associated with these applications and then will review some mathematical techniques that have been successfully utilized to solve them. In particular, we will cover topics on geometric multiscale representations, redundant representations and fusion frames, graph operators, diffusion wavelets, as well as spatial-spectral and operator-based data fusion. All the algorithms will be illustrated using remotely sensed data, with an emphasis on current and operational instruments."
"NASA Tech Briefs, September 2012",21.052322,jpeg estimate,['Man/System Technology and Life Support'],"Topics covered include: Beat-to-Beat Blood Pressure Monitor; Measurement Techniques for Clock Jitter; Lightweight, Miniature Inertial Measurement System; Optical Density Analysis of X-Rays Utilizing Calibration Tooling to Estimate Thickness of Parts; Fuel Cell/Electrochemical Cell Voltage Monitor; Anomaly Detection Techniques with Real Test Data from a Spinning Turbine Engine-Like Rotor; Measuring Air Leaks into the Vacuum Space of Large Liquid Hydrogen Tanks; Antenna Calibration and Measurement Equipment; Glass Solder Approach for Robust, Low-Loss, Fiber-to-Waveguide Coupling; Lightweight Metal Matrix Composite Segmented for Manufacturing High-Precision Mirrors; Plasma Treatment to Remove Carbon from Indium UV Filters; Telerobotics Workstation (TRWS) for Deep Space Habitats; Single-Pole Double-Throw MMIC Switches for a Microwave Radiometer; On Shaft Data Acquisition System (OSDAS); ASIC Readout Circuit Architecture for Large Geiger Photodiode Arrays; Flexible Architecture for FPGAs in Embedded Systems; Polyurea-Based Aerogel Monoliths and Composites; Resin-Impregnated Carbon Ablator: A New Ablative Material for Hyperbolic Entry Speeds; Self-Cleaning Particulate Prefilter Media; Modular, Rapid Propellant Loading System/Cryogenic Testbed; Compact, Low-Force, Low-Noise Linear Actuator; Loop Heat Pipe with Thermal Control Valve as a Variable Thermal Link; Process for Measuring Over-Center Distances; Hands-Free Transcranial Color Doppler Probe; Improving Balance Function Using Low Levels of Electrical Stimulation of the Balance Organs; Developing Physiologic Models for Emergency Medical Procedures Under Microgravity; PMA-Linked Fluorescence for Rapid Detection of Viable Bacterial Endospores; Portable Intravenous Fluid Production Device for Ground Use; Adaptation of a Filter Assembly to Assess Microbial Bioburden of Pressurant Within a Propulsion System; Multiplexed Force and Deflection Sensing Shell Membranes for Robotic Manipulators; Whispering Gallery Mode Optomechanical Resonator; Vision-Aided Autonomous Landing and Ingress of Micro Aerial Vehicles; Self-Sealing Wet Chemistry Cell for Field Analysis; General MACOS Interface for Modeling and Analysis for Controlled Optical Systems; Mars Technology Rover with Arm-Mounted Percussive Coring Tool, Microimager, and Sample-Handling Encapsulation Containerization Subsystem; Fault-Tolerant, Real-Time, Multi-Core Computer System; Water Detection Based on Object Reflections; SATPLOT for Analysis of SECCHI Heliospheric Imager Data; Plug-in Plan Tool v3.0.3.1; Frequency Correction for MIRO Chirp Transformation Spectroscopy Spectrum; Nonlinear Estimation Approach to Real-Time Georegistration from Aerial Images; Optimal Force Control of Vibro-Impact Systems for Autonomous Drilling Applications; Low-Cost Telemetry System for Small/Micro Satellites; Operator Interface and Control Software for the Reconfigurable Surface System Tri-ATHLETE; and Algorithms for Determining Physical Responses of Structures Under Load."
Video transmission on ATM networks,20.782452,jpeg estimate,['COMMUNICATIONS AND RADAR'],"The broadband integrated services digital network (B-ISDN) is expected to provide high-speed and flexible multimedia applications. Multimedia includes data, graphics, image, voice, and video. Asynchronous transfer mode (ATM) is the adopted transport techniques for B-ISDN and has the potential for providing a more efficient and integrated environment for multimedia. It is believed that most broadband applications will make heavy use of visual information. The prospect of wide spread use of image and video communication has led to interest in coding algorithms for reducing bandwidth requirements and improving image quality. The major results of a study on the bridging of network transmission performance and video coding are: Using two representative video sequences, several video source models are developed. The fitness of these models are validated through the use of statistical tests and network queuing performance. A dual leaky bucket algorithm is proposed as an effective network policing function. The concept of the dual leaky bucket algorithm can be applied to a prioritized coding approach to achieve transmission efficiency. A mapping of the performance/control parameters at the network level into equivalent parameters at the video coding level is developed. Based on that, a complete set of principles for the design of video codecs for network transmission is proposed."
Aeronautics and Aviation Science: Careers and Opportunities Project,20.709354,jpeg estimate,['Social Sciences (General)'],"The National Aeronautics and Space Administration funded project, Aeronautics and Aviation Science: Careers and Opportunities has been in operation since July, 1995. This project operated as a collaboration with Massachusetts Corporation for Educational Telecommunications, the Federal Aviation Administration, Bridgewater State College and four targeted ""core sites"" in the greater Boston area. In its first and second years, a video series on aeronautics and aviation science was developed and broadcast via ""live, interactive"" satellite feed. Accompanying teacher and student supplementary instructional materials for grades 6-9 were produced and disseminated by the Massachusetts Corporation for Educational Telecommunications (MCET). In the MCET grant application it states that project Take Off! in its initial phase would recruit and train teachers at ""core"" sites in the greater Boston area, as well as opening participation to other on-line users of MCET's satellite feeds. ""Core site"" classrooms would become equipped so that teachers and students might become engaged in an interactive format which aimed at not only involving the students during the ""live"" broadcast of the instructional video series, but which would encourage participation in electronic information gathering and sharing among participants. As a Take Off! project goal, four schools with a higher than average proportion of minority and underrepresented youth were invited to become involved with the project to give these students the opportunity to consider career exploration and development in the field of science aviation and aeronautics. The four sites chosen to participate in this project were: East Boston High School, Dorchester High School, Randolph Junior-Senior High School and Malden High School. In year 3 Dorchester was unable to continue to fully participate and exited out. Danvers was added to the ""core site"" list in year 3. In consideration of Goals 2000, the National Science Foundation standards for quality of teaching, and an educational agenda that promotes high standards for all students, Aeronautics and Aviation Science: Careers and Opportunities had as its aim to deliver products to schools, both in and outside the project sites, which attempt to incorporate multi-disciplined approaches in the presentation of a curriculum which would be appropriate in any classroom, while also aiming to appeal to young women and minorities. The curriculum was developed to provide students with fundamentals of aeronautics and aviation science. The curriculum also encouraged involving students and teachers in research projects, and further information gathering via electronic bulletin boards and internet capabilities. Though not entirely prescriptive, the curriculum was designed to guide teachers through recommended activities to supplement MCET's live telecast video presentations. Classroom teachers were encouraged to invite local pilots, meteorologists, and others from the field of aviation and aeronautics, particularly women and minorities to visit schools and to field questions from the students."
NASA Earth Science Technology Office (ESTO) Advanced Information Systems Technology (AIST) New Observing Strategies (NOS) Annual Technical Reviews,20.58889,jpeg estimate,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']",
"Buckets: Aggregative, Intelligent Agents for Publishing",20.497185,jpeg estimate,['Documentation and Information Science'],"Buckets are an aggregative, intelligent construct for publishing in digital libraries. The goal of research projects is to produce information. This information is often instantiated in several forms, differentiated by semantic types (report, software, video, datasets, etc.). A given semantic type can be further differentiated by syntactic representations as well (PostScript version, PDF version, Word version, etc.). Although the information was created together and subtle relationships can exist between them, different semantic instantiations are generally segregated along currently obsolete media boundaries. Reports are placed in report archives, software might go into a software archive, but most of the data and supporting materials are likely to be kept in informal personal archives or discarded altogether. Buckets provide an archive-independent container construct in which all related semantic and syntactic data types and objects can be logically grouped together, archived, and manipulated as a single object. Furthermore, buckets are active archival objects and can communicate with each other, people, or arbitrary network services."
"More Than the Sum of the Parts: Satellite Aerosol Remote Sensing, and Its Relationship to Sub-Orbital Measurements and Models",20.202974,jpeg estimate,"['Earth Resources and Remote Sensing', 'Meteorology and Climatology']","Space-borne instruments are providing increasing amounts of data relating to global aerosol spectral optical depth, horizontal and vertical distribution, and very loose, but spatially and temporally extensive, constraints on particle micro-physical properties. The data sets, and many of the underlying techniques, are evolving rapidly. They represent a vast amount of information, potentially useful to the AAAR community. However, there are also issues, some quite subtle, that scientific users must take into consideration. This tutorial will provide one view of the answers to the following four questions: 1) What satellite-derived aerosol products are available? 2) What are their strengths and limitations? 3) How are they being used now? 4) How might they be used in conjunction with each other, with sub-orbital measurements, and with models to address cutting-edge aerosol questions?"
Overview of research in progress at the Center of Excellence,20.086563,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"The Center of Excellence (COE) was created nine years ago to facilitate active collaboration between the scientists at Ames Research Center and the Stanford Psychology Department. Significant interchange of ideas and personnel continues between Stanford and participating groups at NASA-Ames; the COE serves its function well. This progress report is organized into sections divided by project. Each section contains a list of investigators, a background statement, progress report, and a proposal for work during the coming year. The projects are: Algorithms for development and calibration of visual systems, Visually optimized image compression, Evaluation of advanced piloting displays, Spectral representations of color, Perception of motion in man and machine, Automation and decision making, and Motion information used for navigation and control."
Applied Information Systems Research Program (AISRP) Workshop 3 meeting proceedings,20.0629,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],The third Workshop of the Applied Laboratory Systems Research Program (AISRP) met at the Univeristy of Colorado's Laboratory for Atmospheric and Space Physics in August of 1993. The presentations were organized into four sessions: Artificial Intelligence Techniques; Scientific Visualization; Data Management and Archiving; and Research and Technology.
Embedded Web Technology: Applying World Wide Web Standards to Embedded Systems,20.049585,jpeg estimate,['Computer Systems'],"Embedded Systems have traditionally been developed in a highly customized manner. The user interface hardware and software along with the interface to the embedded system are typically unique to the system for which they are built, resulting in extra cost to the system in terms of development time and maintenance effort. World Wide Web standards have been developed in the passed ten years with the goal of allowing servers and clients to intemperate seamlessly. The client and server systems can consist of differing hardware and software platforms but the World Wide Web standards allow them to interface without knowing about the details of system at the other end of the interface. Embedded Web Technology is the merging of Embedded Systems with the World Wide Web. Embedded Web Technology decreases the cost of developing and maintaining the user interface by allowing the user to interface to the embedded system through a web browser running on a standard personal computer. Embedded Web Technology can also be used to simplify an Embedded System's internal network."
The Geophysical Fluid Flow Cell Experiment,20.008684,jpeg estimate,['Fluid Mechanics and Heat Transfer'],"The Geophysical Fluid Flow Cell (GFFC) experiment performed visualizations of thermal convection in a rotating differentially heated spherical shell of fluid. In these experiments dielectric polarization forces are used to generate a radially directed buoyancy force. This enables the laboratory simulation of a number of geophysically and astrophysically important situations in which sphericity and rotation both impose strong constraints on global scale fluid motions. During USML-2 a large set of experiments with spherically symmetric heating were carried out. These enabled the determination of critical points for the transition to various forms of nonaxisymmetric convection and, for highly turbulent flows, the transition latitudes separating the different modes of motion. This paper presents a first analysis of these experiments as well as data on the general performance of the instrument during the USML-2 flight."
"NASA Tech Briefs, October 2008",19.909403,jpeg estimate,['Man/System Technology and Life Support'],"Topics covered include: Control Architecture for Robotic Agent Command and Sensing; Algorithm for Wavefront Sensing Using an Extended Scene; CO2 Sensors Based on Nanocrystalline SnO2 Doped with CuO; Improved Airborne System for Sensing Wildfires; VHF Wide-Band, Dual-Polarization Microstrip-Patch Antenna; Onboard Data Processor for Change-Detection Radar Imaging; Using LDPC Code Constraints to Aid Recovery of Symbol Timing; System for Measuring Flexing of a Large Spaceborne Structure; Integrated Formation Optical Communication and Estimation System; Making Superconducting Welds between Superconducting Wires; Method for Thermal Spraying of Coatings Using Resonant-Pulsed Combustion; Coating Reduces Ice Adhesion; Hybrid Multifoil Aerogel Thermal Insulation; SHINE Virtual Machine Model for In-flight Updates of Critical Mission Software; Mars Image Collection Mosaic Builder; Providing Internet Access to High-Resolution Mars Images; Providing Internet Access to High-Resolution Lunar Images; Expressions Module for the Satellite Orbit Analysis Program Virtual Satellite; Small-Body Extensions for the Satellite Orbit Analysis Program (SOAP); Scripting Module for the Satellite Orbit Analysis Program (SOAP); XML-Based SHINE Knowledge Base Interchange Language; Core Technical Capability Laboratory Management System; MRO SOW Daily Script; Tool for Inspecting Alignment of Twinaxial Connectors; An ATP System for Deep-Space Optical Communication; Polar Traverse Rover Instrument; Expert System Control of Plant Growth in an Enclosed Space; Detecting Phycocyanin-Pigmented Microbes in Reflected Light; DMAC and NMP as Electrolyte Additives for Li-Ion Cells; Mass Spectrometer Containing Multiple Fixed Collectors; Waveguide Harmonic Generator for the SIM; Whispering Gallery Mode Resonator with Orthogonally Reconfigurable Filter Function; Stable Calibration of Raman Lidar Water-Vapor Measurements; Bimaterial Thermal Compensators for WGM Resonators; Root Source Analysis/ValuStream[Trade Mark] - A Methodology for Identifying and Managing Risks; Ensemble: an Architecture for Mission-Operations Software; Object Recognition Using Feature-and Color-Based Methods; On-Orbit Multi-Field Wavefront Control with a Kalman Filter; and The Interplanetary Overlay Networking Protocol Accelerator."
"NASA Tech Briefs, August 2006",19.906166,jpeg estimate,['Man/System Technology and Life Support'],"Topics covered include: Measurement and Controls Data Acquisition System IMU/GPS System Provides Position and Attitude Data Using Artificial Intelligence to Inform Pilots of Weather Fast Lossless Compression of Multispectral-Image Data Developing Signal-Pattern-Recognition Programs Implementing Access to Data Distributed on Many Processors Compact, Efficient Drive Circuit for a Piezoelectric Pump; Dual Common Planes for Time Multiplexing of Dual-Color QWIPs; MMIC Power Amplifier Puts Out 40 mW From 75 to 110 GHz; 2D/3D Visual Tracker for Rover Mast; Adding Hierarchical Objects to Relational Database General-Purpose XML-Based Information Managements; Vaporizable Scaffolds for Fabricating Thermoelectric Modules; Producing Quantum Dots by Spray Pyrolysis; Mobile Robot for Exploring Cold Liquid/Solid Environments; System Would Acquire Core and Powder Samples of Rocks; Improved Fabrication of Lithium Films Having Micron Features; Manufacture of Regularly Shaped Sol-Gel Pellets; Regulating Glucose and pH, and Monitoring Oxygen in a Bioreactor; Satellite Multiangle Spectropolarimetric Imaging of Aerosols; Interferometric System for Measuring Thickness of Sea Ice; Microscale Regenerative Heat Exchanger Protocols for Handling Messages Between Simulation Computers Statistical Detection of Atypical Aircraft Flights NASA's Aviation Safety and Modeling Project Multimode-Guided-Wave Ultrasonic Scanning of Materials Algorithms for Maneuvering Spacecraft Around Small Bodies Improved Solar-Radiation-Pressure Models for GPS Satellites Measuring Attitude of a Large, Flexible, Orbiting Structure"
Evolution of Scientific and Technical Information Distribution,19.781097,jpeg estimate,['Documentation and Information Science'],"World Wide Web (WWW) and related information technologies are transforming the distribution of scientific and technical information (STI). We examine 11 recent, functioning digital libraries focusing on the distribution of STI publications, including journal articles, conference papers, and technical reports. We introduce 4 main categories of digital library projects: based on the architecture (distributed vs. centralized) and the contributor (traditional publisher vs. authoring individual/organization). Many digital library prototypes merely automate existing publishing practices or focus solely on the digitization of the publishing cycle output, not sampling and capturing elements of the input. Still others do not consider for distribution the large body of ""gray literature."" We address these deficiencies in the current model of STI exchange by suggesting methods for expanding the scope and target of digital libraries by focusing on a greater source of technical publications and using ""buckets,"" an object-oriented construct for grouping logically related information objects, to include holdings other than technical publications."
Telecommunications issues of intelligent database management for ground processing systems in the EOS era,19.633692,jpeg estimate,['DOCUMENTATION AND INFORMATION SCIENCE'],"Future NASA earth science missions, including the Earth Observing System (EOS), will be generating vast amounts of data that must be processed and stored at various locations around the world. Here we present a stepwise-refinement of the intelligent database management (IDM) of the distributed active archive center (DAAC - one of seven regionally-located EOSDIS archive sites) architecture, to showcase the telecommunications issues involved. We develop this architecture into a general overall design. We show that the current evolution of protocols is sufficient to support IDM at Gbps rates over large distances. We also show that network design can accommodate a flexible data ingestion storage pipeline and a user extraction and visualization engine, without interference between the two."
An investigative study of multispectral data compression for remotely-sensed images using vector quantization and difference-mapped shift-coding,19.562235,jpeg estimate,['COMPUTER PROGRAMMING AND SOFTWARE'],"A study is conducted to investigate the effects and advantages of data compression techniques on multispectral imagery data acquired by NASA's airborne scanners at the Stennis Space Center. The first technique used was vector quantization. The vector is defined in the multispectral imagery context as an array of pixels from the same location from each channel. The error obtained in substituting the reconstructed images for the original set is compared for different compression ratios. Also, the eigenvalues of the covariance matrix obtained from the reconstructed data set are compared with the eigenvalues of the original set. The effects of varying the size of the vector codebook on the quality of the compression and on subsequent classification are also presented. The output data from the Vector Quantization algorithm was further compressed by a lossless technique called Difference-mapped Shift-extended Huffman coding. The overall compression for 7 channels of data acquired by the Calibrated Airborne Multispectral Scanner (CAMS), with an RMS error of 15.8 pixels was 195:1 (0.41 bpp) and with an RMS error of 3.6 pixels was 18:1 (.447 bpp). The algorithms were implemented in software and interfaced with the help of dedicated image processing boards to an 80386 PC compatible computer. Modules were developed for the task of image compression and image analysis. Also, supporting software to perform image processing for visual display and interpretation of the compressed/classified images was developed."
An Innovative Solution to NASA's NEO Impact Threat Mitigation Grand Challenge and Flight Validation Mission Architecture Development,19.442747,jpeg estimate,"['Astrodynamics', 'Lunar and Planetary Science and Exploration']","This paper presents the results of a NASA Innovative Advanced Concept (NIAC) Phase 2 study entitled ""An Innovative Solution to NASA's Near-Earth Object (NEO) Impact Threat Mitigation Grand Challenge and Flight Validation Mission Architecture Development."" This NIAC Phase 2 study was conducted at the Asteroid Deflection Research Center (ADRC) of Iowa State University in 2012-2014. The study objective was to develop an innovative yet practically implementable mitigation strategy for the most probable impact threat of an asteroid or comet with short warning time (< 5 years). The mitigation strategy described in this paper is intended to optimally reduce the severity and catastrophic damage of the NEO impact event, especially when we don't have sufficient warning times for non-disruptive deflection of a hazardous NEO. This paper provides an executive summary of the NIAC Phase 2 study results. Detailed technical descriptions of the study results are provided in a separate final technical report, which can be downloaded from the ADRC website (www.adrc.iastate.edu)."
Declassified Intelligence Satellite Photography (DISP) Coverage of Antarctica,19.376476,jpeg estimate,['Earth Resources and Remote Sensing'],"This report summarizes the results of a nine-week summer project examining all Declassified Intelligence Satellite Photography (DISP) of Antarctica. It was discovered that the data were collected in three separate missions during 1962 and 1963. The first two missions covered only the coastal areas, while the third mission covered the entire continent. Many of the 1782 frames collected were cloudy. This is especially true of West Antarctica. An optimal set of photographs covering the entire Antarctic coastline is identified along with some examples that show changes in the coastline which have occurred since the early 1960s."
Aircraft Conceptual Design Using Vehicle Sketch Pad,19.327625,jpeg estimate,['Aerodynamics'],"Vehicle Sketch Pad (VSP) is a parametric geometry modeling tool that is intended for use in the conceptual design of aircraft. The intent of this software is to rapidly model aircraft configurations without expending the expertise and time that is typically required for modeling with traditional Computer Aided Design (CAD) packages. VSP accomplishes this by using parametrically defined components, such as a wing that is defined by span, area, sweep, taper ratio, thickness to cord, and so on. During this phase of frequent design builds, changes to the model can be rapidly visualized along with the internal volumetric layout. Using this geometry-based approach, parameters such as wetted areas and cord lengths can be easily extracted for rapid external performance analyses, such as a parasite drag buildup. At the completion of the conceptual design phase, VSP can export its geometry to higher fidelity tools. This geometry tool was developed by NASA and is freely available to U.S. companies and universities. It has become integral to conceptual design in the Aeronautics Systems Analysis Branch (ASAB) here at NASA Langley Research Center and is currently being used at over 100 universities, aerospace companies, and other government agencies. This paper focuses on the use of VSP in recent NASA conceptual design studies to facilitate geometry-centered design methodology. Such a process is shown to promote greater levels of creativity, more rapid assessment of critical design issues, and improved ability to quickly interact with higher order analyses. A number of VSP vehicle model examples are compared to CAD-based conceptual design, from a designer perspective; comparisons are also made of the time and expertise required to build the geometry representations as well."
Method and system for efficient video compression with low-complexity encoder,19.327148,jpeg estimate,['Instrumentation and Photography'],"Disclosed are a method and system for video compression, wherein the video encoder has low computational complexity and high compression efficiency. The disclosed system comprises a video encoder and a video decoder, wherein the method for encoding includes the steps of converting a source frame into a space-frequency representation; estimating conditional statistics of at least one vector of space-frequency coefficients; estimating encoding rates based on the said conditional statistics; and applying Slepian-Wolf codes with the said computed encoding rates. The preferred method for decoding includes the steps of; generating a side-information vector of frequency coefficients based on previously decoded source data, encoder statistics, and previous reconstructions of the source frequency vector; and performing Slepian-Wolf decoding of at least one source frequency vector based on the generated side-information, the Slepian-Wolf code bits and the encoder statistics."
"Technology 2003: The Fourth National Technology Transfer Conference and Exposition, volume 2",19.29162,jpeg estimate,['GENERAL'],"Proceedings from symposia of the Technology 2003 Conference and Exposition, Dec. 7-9, 1993, Anaheim, CA, are presented. Volume 2 features papers on artificial intelligence, CAD&E, computer hardware, computer software, information management, photonics, robotics, test and measurement, video and imaging, and virtual reality/simulation."
MPEG-2 Over Asynchronous Transfer Mode (ATM) Over Satellite Quality of Service (QoS) Experiments: Laboratory Tests,19.241598,jpeg estimate,"['Space Communications, Spacecraft Communications, Command and Tracking']","Asynchronous transfer mode (ATM) quality of service (QoS) experiments were performed using MPEG-2 (ATM application layer 5, AAL5) over ATM over an emulated satellite link. The purpose of these experiments was to determine the free-space link quality necessary to transmit high-quality multimedia information by using the ATM protocol. The detailed test plan and test configuration are described herein as are the test results. MPEG-2 transport streams were baselined in an errored environment, followed by a series of tests using, MPEG-2 over ATM. Errors were created both digitally as well as in an IF link by using a satellite modem and commercial gaussian noise test set for two different MPEG-2 decoder implementations. The results show that ITU-T Recommendation 1.356 Class 1, stringent ATM applications will require better link quality than currently specified; in particular, cell loss ratios of better than 1.0 x 10(exp -8) and cell error ratios of better than 1.0 x 10(exp -7) are needed. These tests were conducted at the NASA Lewis Research Center in support of satellite-ATM interoperability research."
Advanced Shipboard Communications Demonstrations with ACTS,19.241598,jpeg estimate,['Communications and Radar'],"For ships at sea. satellites provide the only option for high data rate (HDR), long haul communications. Furthermore the demand for HDR satellite communications (SATCOM) for military and commercial ships. and other offshore platforms is increasing. Presently the bulk of this maritime HDR SATCOM connectivity is provided via C-band and X-band. However, the shipboard antenna sizes required to achieve a data rate of, say T 1 (1.544 Mbps) with present C-/X-band SATCOM systems range from seven to ten feet in diameter. This limits the classes of ships to which HDR services can be provided to those which are large enough to accommodate the massive antennas. With its high powered K/Ka-band spot beams, the National Aeronautics and Space Administration's (NASA) Advanced Communications Technology Satellite (ACTS) was able to provide T I and higher rate services to ships at sea using much smaller shipboard antennas. This paper discusses three shipboard HDR SATCOM demonstrations that were conducted with ACTS between 1996 and 1998. The first demonstration involved a 2 Mbps link provided to the seismic survey ship MN Geco Diamond equipped with a 16-inch wide, 4.5-inch tall, mechanically steered slotted waveguide array antenna developed by the Jet Propulsion Laboratory. In this February 1996 demonstration ACTS allowed supercomputers ashore to process Geco Diamond's voluminous oceanographic seismic data in near real time. This capability allowed the ship to adjust its search parameters on a daily basis based on feedback from the processed data, thereby greatly increasing survey efficiency. The second demonstration was conducted on the US Navy cruiser USS Princeton (CG 59) with the same antenna used on Geco Diamond. Princeton conducted a six-month (January-July 1997) Western Hemisphere solo deployment during which time T1 connectivity via ACTS provided the ship with a range of valuable tools for operational, administrative and quality-of-life tasks. In one instance, video teleconferencing (VTC) via ACTS allowed the ship to provide life-saving emergency medical aid, assisted by specialists ashore. to a fellow mariner - the Master of a Greek cargo ship. The third demonstration set what is believed to be the all-time SATCOM data rate record to a ship at sea, 45 Mbps in October 1998. This Lake Michigan (Chicago area) demonstration employed one of ACTS' fixed beams and involved the smallest of the three vessels, the 45-foot Bayliner M/V Entropy equipped with a modified commercial-off-the-shelf one-meter antenna. A variety of multi-media services were provided to Entropy through a stressing range of sea states. These three demonstrations provided a preview of the capabilities that could be provided to future mariners on a more routine basis when K/Ka-band SATCOM systems are widely deployed."
Photogrammetry Methodology Development for Gossamer Spacecraft Structures,19.118113,jpeg estimate,['Structural Mechanics'],"Photogrammetry--the science of calculating 3D object coordinates from images--is a flexible and robust approach for measuring the static and dynamic characteristics of future ultra-lightweight and inflatable space structures (a.k.a., Gossamer structures), such as large membrane reflectors, solar sails, and thin-film solar arrays. Shape and dynamic measurements are required to validate new structural modeling techniques and corresponding analytical models for these unconventional systems. This paper summarizes experiences at NASA Langley Research Center over the past three years to develop or adapt photogrammetry methods for the specific problem of measuring Gossamer space structures. Turnkey industrial photogrammetry systems were not considered a cost-effective choice for this basic research effort because of their high purchase and maintenance costs. Instead, this research uses mainly off-the-shelf digital-camera and software technologies that are affordable to most organizations and provide acceptable accuracy."
STRIPE: Remote Driving Using Limited Image Data,19.060478,jpeg estimate,['Cybernetics'],"Driving a vehicle, either directly or remotely, is an inherently visual task. When heavy fog limits visibility, we reduce our car's speed to a slow crawl, even along very familiar roads. In teleoperation systems, an operator's view is limited to images provided by one or more cameras mounted on the remote vehicle. Traditional methods of vehicle teleoperation require that a real time stream of images is transmitted from the vehicle camera to the operator control station, and the operator steers the vehicle accordingly. For this type of teleoperation, the transmission link between the vehicle and operator workstation must be very high bandwidth (because of the high volume of images required) and very low latency (because delayed images can cause operators to steer incorrectly). In many situations, such a high-bandwidth, low-latency communication link is unavailable or even technically impossible to provide. Supervised TeleRobotics using Incremental Polyhedral Earth geometry, or STRIPE, is a teleoperation system for a robot vehicle that allows a human operator to accurately control the remote vehicle across very low bandwidth communication links, and communication links with large delays. In STRIPE, a single image from a camera mounted on the vehicle is transmitted to the operator workstation. The operator uses a mouse to pick a series of 'waypoints' in the image that define a path that the vehicle should follow. These 2D waypoints are then transmitted back to the vehicle, where they are used to compute the appropriate steering commands while the next image is being transmitted. STRIPE requires no advance knowledge of the terrain to be traversed, and can be used by novice operators with only minimal training. STRIPE is a unique combination of computer and human control. The computer must determine the 3D world path designated by the 2D waypoints and then accurately control the vehicle over rugged terrain. The human issues involve accurate path selection, and the prevention of disorientation, a common problem across all types of teleoperation systems. STRIPE is the only semi-autonomous teleoperation system that can accurately follow paths designated in monocular images on varying terrain. The thesis describes the STRIPE algorithm for tracking points using the incremental geometry model, insight into the design and redesign of the interface, an analysis of the effects of potential errors, details of the user studies, and hints on how to improve both the algorithm and interface for future designs."
Retinal Image Quality Assessment for Spaceflight-Induced Vision Impairment Study,152.16023,image quality detection,['Life Sciences (General)'],"Long-term exposure to space microgravity poses significant risks for visual impairment. Evidence suggests such vision changes are linked to cephalad fluid shifts, prompting a need to directly quantify microgravity-induced retinal vascular changes. The quality of retinal images used for such vascular remodeling analysis, however, is dependent on imaging methodology. For our exploratory study, we hypothesized that retinal images captured using fluorescein imaging methodologies would be of higher quality in comparison to images captured without fluorescein. A semi-automated image quality assessment was developed using Vessel Generation Analysis (VESGEN) software and MATLAB image analysis toolboxes. An analysis of ten images found that the fluorescein imaging modality provided a 36% increase in overall image quality (two-tailed p=0.089) in comparison to nonfluorescein imaging techniques. "
A comparison of Image Quality Models and Metrics Predicting Object Detection,149.65079,image quality detection,['Optics'],"Many models and metrics for image quality predict image discriminability, the visibility of the difference between a pair of images. Some image quality applications, such as the quality of imaging radar displays, are concerned with object detection and recognition. Object detection involves looking for one of a large set of object sub-images in a large set of background images and has been approached from this general point of view. We find that discrimination models and metrics can predict the relative detectability of objects in different images, suggesting that these simpler models may be useful in some object detection and recognition applications. Here we compare three alternative measures of image discrimination, a multiple frequency channel model, a single filter model, and RMS error."
A system to monitor stellar image quality,131.79073,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],Stellar image quality monitoring system
A study of image quality for radar image processing,131.13081,image quality detection,['COMMUNICATIONS AND RADAR'],Methods developed for image quality metrics are reviewed with focus on basic interpretation or recognition elements including: tone or color; shape; pattern; size; shadow; texture; site; association or context; and resolution. Seven metrics are believed to show promise as a way of characterizing the quality of an image: (1) the dynamic range of intensities in the displayed image; (2) the system signal-to-noise ratio; (3) the system spatial bandwidth or bandpass; (4) the system resolution or acutance; (5) the normalized-mean-square-error as a measure of geometric fidelity; (6) the perceptual mean square error; and (7) the radar threshold quality factor. Selective levels of degradation are being applied to simulated synthetic radar images to test the validity of these metrics.
Detection of Obstacles in Monocular Image Sequences,127.308044,image quality detection,['Optics'],"The ability to detect and locate runways/taxiways and obstacles in images captured using on-board sensors is an essential first step in the automation of low-altitude flight, landing, takeoff, and taxiing phase of aircraft navigation. Automation of these functions under different weather and lighting situations, can be facilitated by using sensors of different modalities. An aircraft-based Synthetic Vision System (SVS), with sensors of different modalities mounted on-board, complements the current ground-based systems in functions such as detection and prevention of potential runway collisions, airport surface navigation, and landing and takeoff in all weather conditions. In this report, we address the problem of detection of objects in monocular image sequences obtained from two types of sensors, a Passive Millimeter Wave (PMMW) sensor and a video camera mounted on-board a landing aircraft. Since the sensors differ in their spatial resolution, and the quality of the images obtained using these sensors is not the same, different approaches are used for detecting obstacles depending on the sensor type. These approaches are described separately in two parts of this report. The goal of the first part of the report is to develop a method for detecting runways/taxiways and objects on the runway in a sequence of images obtained from a moving PMMW sensor. Since the sensor resolution is low and the image quality is very poor, we propose a model-based approach for detecting runways/taxiways. We use the approximate runway model and the position information of the camera provided by the Global Positioning System (GPS) to define regions of interest in the image plane to search for the image features corresponding to the runway markers. Once the runway region is identified, we use histogram-based thresholding to detect obstacles on the runway and regions outside the runway. This algorithm is tested using image sequences simulated from a single real PMMW image."
Geometric assessment of image quality using digital image registration techniques,124.11915,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"Image registration techniques were developed to perform a geometric quality assessment of multispectral and multitemporal image pairs. Based upon LANDSAT tapes, accuracies to a small fraction of a pixel were demonstrated. Because it is insensitive to the choice of registration areas, the technique is well suited to performance in an automatic system. It may be implemented at megapixel-per-second rates using a commercial minicomputer in combination with a special purpose digital preprocessor."
A conceptual study of automatic and semi-automatic quality assurance techniques for round image processing,122.795074,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"This report summarizes the results of a study conducted by Engineering and Economics Research (EER), Inc. under NASA Contract Number NAS5-27513. The study involved the development of preliminary concepts for automatic and semiautomatic quality assurance (QA) techniques for ground image processing. A distinction is made between quality assessment and the more comprehensive quality assurance which includes decision making and system feedback control in response to quality assessment."
Image processing system performance prediction and product quality evaluation,121.36667,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. A new technique for image processing system performance prediction and product quality evaluation was developed. It was entirely objective, quantitative, and general, and should prove useful in system design and quality control. The technique and its application to determination of quality control procedures for the Earth Resources Technology Satellite NASA Data Processing Facility are described."
Image Discrimination Models for Object Detection in Natural Backgrounds,117.94881,image quality detection,['Earth Resources and Remote Sensing'],"This paper reviews work accomplished and in progress at NASA Ames relating to visual target detection. The focus is on image discrimination models, starting with Watson's pioneering development of a simple spatial model and progressing through this model's descendents and extensions. The application of image discrimination models to target detection will be described and results reviewed for Rohaly's vehicle target data and the Search 2 data. The paper concludes with a description of work we have done to model the process by which observers learn target templates and methods for elucidating those templates."
Retinal Image Quality Assessment for Spaceflight-Induced Visual Impairment Study,116.17044,image quality detection,['Aerospace Medicine'],"Medical reports have identified visual impairments as a risk associated with extended exposure to microgravity. Etiology of these ocular changes is currently unknown. Current hypotheses propose cephalad fluid shifts resulting from microgravity as the primary cause of ocular damage. One approach to studying ocular response to microgravity is by examining possible changes in retinal blood vessels using a NASA model of microgravity, the head-down tilt (HDT) of human subjects undergoing prolonged bed rest (BR). Retinal vessels in astronauts and BR subjects are monitored by Heidelberg Spectralis infrared (IR) imaging, in which retinal image quality is limited by insufficient resolution of small vessels. Yet small vessels respond and remodel most actively to physiological stress.For our NASA study of BR subjects, we identify retinal image quality as thecapability to capture vascular detail to acceptable resolution of small vessels. We therefore are analyzing Spectralis images acquired with fluorescein angiography(FA), where increased contrast significantly improves image resolution. The FA images are of normal subjects participating in a clinical study on diabeticretinopathy (US National Institutes of Health). We hypothesize that FA Spectralis images are of superior quality compared to non-FA Spectralis IR images."
"Synthetic aperture radar target detection, feature extraction, and image formation techniques",116.158806,image quality detection,['COMMUNICATIONS AND RADAR'],"This report presents new algorithms for target detection, feature extraction, and image formation with the synthetic aperture radar (SAR) technology. For target detection, we consider target detection with SAR and coherent subtraction. We also study how the image false alarm rates are related to the target template false alarm rates when target templates are used for target detection. For feature extraction from SAR images, we present a computationally efficient eigenstructure-based 2D-MODE algorithm for two-dimensional frequency estimation. For SAR image formation, we present a robust parametric data model for estimating high resolution range signatures of radar targets and for forming high resolution SAR images."
Image Registration Workshop Proceedings,115.15748,image quality detection,['Mathematical and Computer Sciences (General)'],"Automatic image registration has often been considered as a preliminary step for higher-level processing, such as object recognition or data fusion. But with the unprecedented amounts of data which are being and will continue to be generated by newly developed sensors, the very topic of automatic image registration has become and important research topic. This workshop presents a collection of very high quality work which has been grouped in four main areas: (1) theoretical aspects of image registration; (2) applications to satellite imagery; (3) applications to medical imagery; and (4) image registration for computer vision research."
Image processing developments and applications for water quality monitoring and trophic state determination,113.923744,image quality detection,['ENVIRONMENT POLLUTION'],"Remote sensing data analysis of water quality monitoring is evaluated. Data anaysis and image processing techniques are applied to LANDSAT remote sensing data to produce an effective operational tool for lake water quality surveying and monitoring. Digital image processing and analysis techniques were designed, developed, tested, and applied to LANDSAT multispectral scanner (MSS) data and conventional surface acquired data. Utilization of these techniques facilitates the surveying and monitoring of large numbers of lakes in an operational manner. Supervised multispectral classification, when used in conjunction with surface acquired water quality indicators, is used to characterize water body trophic status. Unsupervised multispectral classification, when interpreted by lake scientists familiar with a specific water body, yields classifications of equal validity with supervised methods and in a more cost effective manner. Image data base technology is used to great advantage in characterizing other contributing effects to water quality. These effects include drainage basin configuration, terrain slope, soil, precipitation and land cover characteristics."
"Image Station Matching, Preprocessing, Spatial Registration and Change Detection with Multi-Temporal Remotely-Sensed Imagery",113.84938,image quality detection,['Instrumentation and Photography'],"A method for collecting and processing remotely sensed imagery in order to achieve precise spatial co-registration (e.g., matched alignment) between multi-temporal image sets is presented. Such precise alignment or spatial co-registration of imagery can be used for change detection, image fusion, and temporal analysis/modeling. Further, images collected in this manner may be further processed in such a way that image frames or line arrays from corresponding photo stations are matched, co-aligned and if desired merged into a single image and/or subjected to the same processing sequence. A second methodology for automated detection of moving objects within a scene using a time series of remotely sensed imagery is also presented. Specialized image collection and preprocessing procedures are utilized to obtain precise spatial co-registration (image registration) between multitemporal image frame sets. In addition, specialized change detection techniques are employed in order to automate the detection of moving objects."
Image gathering and coding for digital restoration: Information efficiency and visual quality,113.13455,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"Image gathering and coding are commonly treated as tasks separate from each other and from the digital processing used to restore and enhance the images. The goal is to develop a method that allows us to assess quantitatively the combined performance of image gathering and coding for the digital restoration of images with high visual quality. Digital restoration is often interactive because visual quality depends on perceptual rather than mathematical considerations, and these considerations vary with the target, the application, and the observer. The approach is based on the theoretical treatment of image gathering as a communication channel (J. Opt. Soc. Am. A2, 1644(1985);5,285(1988). Initial results suggest that the practical upper limit of the information contained in the acquired image data range typically from approximately 2 to 4 binary information units (bifs) per sample, depending on the design of the image-gathering system. The associated information efficiency of the transmitted data (i.e., the ratio of information over data) ranges typically from approximately 0.3 to 0.5 bif per bit without coding to approximately 0.5 to 0.9 bif per bit with lossless predictive compression and Huffman coding. The visual quality that can be attained with interactive image restoration improves perceptibly as the available information increases to approximately 3 bifs per sample. However, the perceptual improvements that can be attained with further increases in information are very subtle and depend on the target and the desired enhancement."
Improving biomedical image quality with computers,108.672356,image quality detection,['BIOSCIENCES'],Computerized image enhancement techniques used on biomedical radiographs and photomicrographs
Image Science and Analysis Group Spacecraft Damage Detection/Characterization,106.033455,image quality detection,"['Spacecraft Design, Testing and Performance']","This project consisted of several tasks that could be served by an intern to assist the ISAG in detecting damage to spacecrafts during missions. First, this project focused on supporting the Micrometeoroid Orbital Debris (MMOD) damage detection and assessment for the Hubble Space Telescope (HST) using imagery from the last two HST Shuttle servicing missions. In this project, we used coordinates of two windows on the Shuttle Aft flight deck from where images were taken and the coordinates of three ID points in order to calculate the distance from each window to the three points. Then, using the specifications from the camera used, we calculated the image scale in pixels per inch for planes parallel to and planes in the z-direction to the image plane (shown in Table 1). This will help in the future for calculating measurements of objects in the images. Next, tabulation and statistical analysis were conducted for screening results (shown in Table 2) of imagery with Orion Thermal Protection System (TPS) damage. Using the Microsoft Excel CRITBINOM function and Goal Seek, the probabilities of detection of damage to different shuttle tiles were calculated as shown in Table 3. Using developed measuring tools, volume and area measurements will be created from 3D models of Orion TPS damage. Last, mathematical expertise was provided to the Photogrammetry Team. These mathematical tasks consisted of developing elegant image space error equations for observations along 3D lines, circles, planes, etc. and checking proofs for minimal sets of sufficient multi-linear constraints. Some of the processes and resulting equations are displayed in Figure 1."
Calibration and Change Detection of [Alaska Satellite Facility] ASF/ERS-1 SAR Image Data,105.26355,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"The results of a calibration analysis performed on ERS-1 synthetic aperture radar (SAR) images produced by the Alaska SAR facility (ASF) are presented, together with some preliminary results on change detection on the Alaskan north slope derived from the same images. Image quality, geometric and radiometric fidelity, and repeat pass radiometric stability have all been determined to be satisfactory. A calibration workstation has been designed and implemented for use in operational data quality analysis of ASF data products. Higher-level data analysis programs have been developed for interferometry, texture analysis, and change detection."
Characterization of LANDSAT-4 TM and MSS Image Quality for the Interpretation of California's Agricultural Resources,103.040085,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"The quality of LANDSAT-4 MSS and TM data was determined by analyzing TM spectral and spatial performance in terms of spectral variability of natural targets and the TM-ground instantaneous field-of-view (IFOV) variability in level and mountainous terrain; and by assessing the suitability of TM and MSS image products for characterizing renewable resourse features. The TM data should be extremelly valuable for crop type and area proportion estimation; undating agricultural land use survey maps at 1:24,000 scale and smaller, field boundary definition; and determining the size and location of individual farmsteads. Ongoing research activities are focused on making spectral and spatial analyses of both MSS and TM analytical film products. The improved spectral, spatial, and radiometric quality of the TM data, should promote a renewed emphasis and interest in direct visual interpretation of these image products, both for updating and improving land stratification in support of resource inventory and for enhancing the image analyst's contribution to computer-assisted analysis procedures."
Automation of disbond detection in aircraft fuselage through thermal image processing,102.53611,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"A procedure for interpreting thermal images obtained during the nondestructive evaluation of aircraft bonded joints is presented. The procedure operates on time-derivative thermal images and resulted in a disbond image with disbonds highlighted. The size of the 'black clusters' in the output disbond image is a quantitative measure of disbond size. The procedure is illustrated using simulation data as well as data obtained through experimental testing of fabricated samples and aircraft panels. Good results are obtained, and, except in pathological cases, 'false calls' in the cases studied appeared only as noise in the output disbond image which was easily filtered out. The thermal detection technique coupled with an automated image interpretation capability will be a very fast and effective method for inspecting bonded joints in an aircraft structure."
A New Tool for Quality Control,101.58259,image quality detection,['Quality Assurance and Reliability'],"Diffracto, Ltd. is now offering a new product inspection system that allows detection of minute flaws previously difficult or impossible to observe. Called D-Sight, it represents a revolutionary technique for inspection of flat or curved surfaces to find such imperfections as dings, dents and waviness. System amplifies defects, making them highly visible to simplify decision making as to corrective measures or to identify areas that need further study. CVA 3000 employs a camera, high intensity lamps and a special reflective screen to produce a D- Sight image of light reflected from a surface. Image is captured and stored in a computerized vision system then analyzed by a computer program. A live image of surface is projected onto a video display and compared with a stored master image to identify imperfections. Localized defects measuring less than 1/1000 of an inch are readily detected."
WELDSMART: A vision-based expert system for quality control,99.540184,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"This work was aimed at exploring means for utilizing computer technology in quality inspection and evaluation. Inspection of metallic welds was selected as the main application for this development and primary emphasis was placed on visual inspection, as opposed to other inspection methods, such as radiographic techniques. Emphasis was placed on methodologies with the potential for use in real-time quality control systems. Because quality evaluation is somewhat subjective, despite various efforts to classify discontinuities and standardize inspection methods, the task of using a computer for both inspection and evaluation was not trivial. The work started out with a review of the various inspection techniques that are used for quality control in welding. Among other observations from this review was the finding that most weld defects result in abnormalities that may be seen by visual inspection. This supports the approach of emphasizing visual inspection for this work. Quality control consists of two phases: (1) identification of weld discontinuities (some of which may be severe enough to be classified as defects), and (2) assessment or evaluation of the weld based on the observed discontinuities. Usually the latter phase results in a pass/fail judgement for the inspected piece. It is the conclusion of this work that the first of the above tasks, identification of discontinuities, is the most challenging one. It calls for sophisticated image processing and image analysis techniques, and frequently ad hoc methods have to be developed to identify specific features in the weld image. The difficulty of this task is generally not due to limited computing power. In most cases it was found that a modest personal computer or workstation could carry out most computations in a reasonably short time period. Rather, the algorithms and methods necessary for identifying weld discontinuities were in some cases limited. The fact that specific techniques were finally developed and successfully demosntrated to work illustrates that the general approach taken here appears to be promising for commercial development of computerized quality inspection systems. Inspection based on these techniques may be used to supplement or substitute more elaborate inspection methods, such as x-ray inspections."
Land use change detection with LANDSAT-2 data for monitoring and predicting regional water quality degradation,99.31558,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"The author has identified the following significant results. Comparison between LANDSAT 1 and 2 imagery of Arkansas provided evidence of significant land use changes during the 1972-75 time period. Analysis of Arkansas historical water quality information has shown conclusively that whereas point source pollution generally can be detected by use of water quality data collected by state and federal agencies, sampling methodologies for nonpoint source contamination attributable to surface runoff are totally inadequate. The expensive undertaking of monitoring all nonpoint sources for numerous watersheds can be lessened by implementing LANDSAT change detection analyses."
Detection and Estimation of an Optical Image by Photon-Counting Techniques,98.81333,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"Statistical description of a photoelectric detector is given. The photosensitive surface of the detector is divided into many small areas, and the moment generating function of the photo-counting statistic is derived for large time-bandwidth product. The detection of a specified optical image in the presence of the background light by using the hypothesis test is discussed. The ideal detector based on the likelihood ratio from a set of numbers of photoelectrons ejected from many small areas of the photosensitive surface is studied and compared with the threshold detector and a simple detector which is based on the likelihood ratio by counting the total number of photoelectrons from a finite area of the surface. The intensity of the image is assumed to be Gaussian distributed spatially against the uniformly distributed background light. The numerical approximation by the method of steepest descent is used, and the calculations of the reliabilities for the detectors are carried out by a digital computer."
Image-adapted visually weighted quantization matrices for digital image compression,98.15251,image quality detection,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is presented. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Development of Software to Model AXAF-I Image Quality,96.93468,image quality detection,['Computer Programming and Software'],"This draft final report describes the work performed under the delivery order number 145 from May 1995 through August 1996. The scope of work included a number of software development tasks for the performance modeling of AXAF-I. A number of new capabilities and functions have been added to the GT software, which is the command mode version of the GRAZTRACE software, originally developed by MSFC. A structural data interface has been developed for the EAL (old SPAR) finite element analysis FEA program, which is being used by MSFC Structural Analysis group for the analysis of AXAF-I. This interface utility can read the structural deformation file from the EAL and other finite element analysis programs such as NASTRAN and COSMOS/M, and convert the data to a suitable format that can be used for the deformation ray-tracing to predict the image quality for a distorted mirror. There is a provision in this utility to expand the data from finite element models assuming 180 degrees symmetry. This utility has been used to predict image characteristics for the AXAF-I HRMA, when subjected to gravity effects in the horizontal x-ray ground test configuration. The development of the metrology data processing interface software has also been completed. It can read the HDOS FITS format surface map files, manipulate and filter the metrology data, and produce a deformation file, which can be used by GT for ray tracing for the mirror surface figure errors. This utility has been used to determine the optimum alignment (axial spacing and clocking) for the four pairs of AXAF-I mirrors. Based on this optimized alignment, the geometric images and effective focal lengths for the as built mirrors were predicted to cross check the results obtained by Kodak."
Multiple image overlay storage methods,96.799934,image quality detection,['GENERAL'],Image quality achievable with optical multiplexing techniques - overlay storage methods
Measuring Image Navigation and Registration Performance at the 3-Sigma Level Using Platinum Quality Landmarks,95.82202,image quality detection,"['Spacecraft Design, Testing and Performance']","Geostationary Operational Environmental Satellite (GOES) Image Navigation and Registration (INR) performance is specified at the 3- level, meaning that 99.7% of a collection of individual measurements must comply with specification thresholds. Landmarks are measured by the Replacement Product Monitor (RPM), part of the operational GOES ground system, to assess INR performance and to close the INR loop. The RPM automatically discriminates between valid and invalid measurements enabling it to run without human supervision. In general, this screening is reliable, but a small population of invalid measurements will be falsely identified as valid. Even a small population of invalid measurements can create problems when assessing performance at the 3-sigma level. This paper describes an additional layer of quality control whereby landmarks of the highest quality (""platinum"") are identified by their self-consistency. The platinum screening criteria are not simple statistical outlier tests against sigma values in populations of INR errors. In-orbit INR performance metrics for GOES-12 and GOES-13 are presented using the platinum landmark methodology."
Total Quality Leadership,94.91472,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"More than 750 NASA, government, contractor, and academic representatives attended the Seventh Annual NASA/Contractors Conference on Quality and Productivity. The panel presentations and Keynote speeches revolving around the theme of total quality leadership provided a solid base of understanding of the importance, benefits, and principles of total quality management (TQM). The presentations from the conference are summarized."
Analysis of the quality of image data acquired by the LANDSAT-4 thematic mapper and multispectral scanners,94.89457,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"Image products and numeric data were extracted from both TM and MSS data in an effort to evaluate the quality of these data for interpreting major agricultural resources and conditions in California's Central Valley. The utility of TM data appears excellent for meeting most of the inventory objectives of the agricultural resource specialist. These data should be extremely valuable for crop type and area proportion estimation, for updating agricultural land use survey maps at 1:24,000-scale and smaller, for field boundary definition, and for determining the size and location of individual farmsteads."
MEDICAL APPLICATIONS OF IMAGE INTENSIFICATION,94.77197,image quality detection,['NAVIGATION'],Medical applications of image intensification
Inventory of forest and rangeland and detection of forest stress,94.75999,image quality detection,['GEOPHYSICS'],"The author has identified the following significant results. Disturbances in a forest environment that cause reductions in forest area, timber volume, and timber growth can be detected on ERTS-1 combined color composites. However, detection depends on comparing a conventional aerial photograph taken at some base year with an ERTS-1 image taken in some subsequent year. In a test made on the Atlanta site, 1:63,360 scale aerial photo index sheets made in 1966 were compared with ERTS-1 image 1264-15445 (April 1973). Five factors were found important to detection reliability: (1) the quality of the imagery; (2) the season of the imagery; (3) the size of the disturbed area; (4) the number of years since the disturbances; and (5) the type of cutting treatment. Of 209 disturbances verified on aerial photography, 165 (or approximately 80%) were detected on the ERTS-1 image by one independent interpreter. Improved training and additional experience in using this low resolution imagery should improve detection. Of the two seasons of data studies (fall and early spring), early spring is the best for detecting land use changes. Generally speaking, winter, early spring, and early summer are the best times of year for detecting forest disturbances."
Improved image decompression for reduced transform coding artifacts,94.19227,image quality detection,['COMPUTER PROGRAMMING AND SOFTWARE'],"The perceived quality of images reconstructed from low bit rate compression is severely degraded by the appearance of transform coding artifacts. This paper proposes a method for producing higher quality reconstructed images based on a stochastic model for the image data. Quantization (scalar or vector) partitions the transform coefficient space and maps all points in a partition cell to a representative reconstruction point, usually taken as the centroid of the cell. The proposed image estimation technique selects the reconstruction point within the quantization partition cell which results in a reconstructed image which best fits a non-Gaussian Markov random field (MRF) image model. This approach results in a convex constrained optimization problem which can be solved iteratively. At each iteration, the gradient projection method is used to update the estimate based on the image model. In the transform domain, the resulting coefficient reconstruction points are projected to the particular quantization partition cells defined by the compressed image. Experimental results will be shown for images compressed using scalar quantization of block DCT and using vector quantization of subband wavelet transform. The proposed image decompression provides a reconstructed image with reduced visibility of transform coding artifacts and superior perceived quality."
Image data compression having minimum perceptual error,92.35974,image quality detection,['COMPUTER OPERATIONS AND HARDWARE'],"A method for performing image compression that eliminates redundant and invisible image components is described. The image compression uses a Discrete Cosine Transform (DCT) and each DCT coefficient yielded by the transform is quantized by an entry in a quantization matrix which determines the perceived image quality and the bit rate of the image being compressed. The present invention adapts or customizes the quantization matrix to the image being compressed. The quantization matrix comprises visual masking by luminance and contrast techniques and by an error pooling technique all resulting in a minimum perceptual error for any given bit rate, or minimum bit rate for a given perceptual error."
Image Information Mining Utilizing Hierarchical Segmentation,92.09344,image quality detection,"['Cybernetics, Artificial Intelligence and Robotics']","The Hierarchical Segmentation (HSEG) algorithm is an approach for producing high quality, hierarchically related image segmentations. The VisiMine image information mining system utilizes clustering and segmentation algorithms for reducing visual information in multispectral images to a manageable size. The project discussed herein seeks to enhance the VisiMine system through incorporating hierarchical segmentations from HSEG into the VisiMine system."
A model-based approach for detection of runways and other objects in image sequences acquired using an on-board camera,91.132484,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],This research was initiated as a part of the Advanced Sensor and Imaging System Technology (ASSIST) program at NASA Langley Research Center. The primary goal of this research is the development of image analysis algorithms for the detection of runways and other objects using an on-board camera. Initial effort was concentrated on images acquired using a passive millimeter wave (PMMW) sensor. The images obtained using PMMW sensors under poor visibility conditions due to atmospheric fog are characterized by very low spatial resolution but good image contrast compared to those images obtained using sensors operating in the visible spectrum. Algorithms developed for analyzing these images using a model of the runway and other objects are described in Part 1 of this report. Experimental verification of these algorithms was limited to a sequence of images simulated from a single frame of PMMW image. Subsequent development and evaluation of algorithms was done using video image sequences. These images have better spatial and temporal resolution compared to PMMW images. Algorithms for reliable recognition of runways and accurate estimation of spatial position of stationary objects on the ground have been developed and evaluated using several image sequences. These algorithms are described in Part 2 of this report. A list of all publications resulting from this work is also included.
The influence of image position on urban place detection,90.302345,image quality detection,['GEOPHYSICS'],"The author has identified the following significant results. The ability of ERTS-1 MSS imagery to detect small urban places appears to vary with the position of the place in the image, as well as from band to band. Urban places of smallest size (approximately 2000 population) seem more detectable in the westernmost 3.5 degree scan segment. A relationship may exist between shadowing of vertical features and detectability."
LANDSAT-4/5 image data quality analysis,89.86579,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"A LANDSAT Thematic Mapper (TM) quality evaluation study was conducted to identify geometric and radiometric sensor errors in the post-launch environment. The study began with the launch of LANDSAT-4. Several error conditions were found, including band-to-band misregistration and detector-to detector radiometric calibration errors. Similar analysis was made for the LANDSAT-5 Thematic Mapper and compared with results for LANDSAT-4. Remaining band-to-band misregistration was found to be within tolerances and detector-to-detector calibration errors were not severe. More coherent noise signals were observed in TM-5 than in TM-4, although the amplitude was generally less. The scan direction differences observed in TM-4 were still evident in TM-5. The largest effect was in Band 4 where nearly a one digital count difference was observed. Resolution estimation was carried out using roads in TM-5 for the primary focal plane bands rather than field edges as in TM-4. Estimates using roads gave better resolution. Thermal IR band calibration studies were conducted and new nonlinear calibration procedures were defined for TM-5. The overall conclusion is that there are no first order errors in TM-5 and any remaining problems are second or third order."
The effect of lossy image compression on image classification,89.82757,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"We have classified four different images, under various levels of JPEG compression, using the following classification algorithms: minimum-distance, maximum-likelihood, and neural network. The training site accuracy and percent difference from the original classification were tabulated for each image compression level, with maximum-likelihood showing the poorest results. In general, as compression ratio increased, the classification retained its overall appearance, but much of the pixel-to-pixel detail was eliminated. We also examined the effect of compression on spatial pattern detection using a neural network."
Evaluation of image quality in a Cassegrain-type telescope with an oscillating secondary mirror,89.71409,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"A ray-trace analysis is described of aberrations and extreme rays of a Cassegrain-type telescope with a tilted secondary mirror. The work was motivated by the need to understand the factors limiting image quality and to assist in the design of secondary mirrors for three telescopes with oscillating secondary mirrors (OSM) used at Ames Research Center for high altitude infrared astronomy. The telescopes are a 31-cm-diameter Dall-Kirkham (elliptical primary, spherical secondary) flown aboard a Lear jet, a 71-cm balloon-borne Dall-Kirkham flown on the AIROscope gondola, and a 91-cm true Cassegrain (parabolic primary, hyperbolic secondary) flown aboard a C-141 jet transport. The optics for these telescopes were not designed specifically for OSM operation, but all have OSM's and all must be used with various detector configurations; therefore, a facility that evaluates the performance of a telescope for a given configuration is useful. The analytical expressions are summarized and results for the above systems are discussed. Details of the calculation and a discussion of the computer program are given in the appendices."
Photon-counting image sensors for the ultraviolet,89.131775,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"An investigation on specific performance details of photon counting, ultraviolet image sensors having 2-dimensional formats is reviewed. In one study, controlled experiments were performed which compare the quantum efficiencies, in pulse counting mode, of CsI photocathodes deposited on: (1) the front surface of a microchannel plate (MCP), (2) a solid surface in front of an MCP, and (3) an intensified CCD image sensor (ICCD) where a CCD is directly bombarded by accelerated photoelectrons. Tests indicated that the detection efficiency of the CsI-coated MCP at 1026 A is lower by a factor of 2.5 than that of the MCP with a separate, opaque CsI photocathode, and the detection efficiency ratio increases substantially at longer wavelengths (ratio is 5 at 1216 A and 20 at 1608 A)."
"Fuzzy geometry, entropy, and image information",88.94286,image quality detection,['CYBERNETICS'],"Presented here are various uncertainty measures arising from grayness ambiguity and spatial ambiguity in an image, and their possible applications as image information measures. Definitions are given of an image in the light of fuzzy set theory, and of information measures and tools relevant for processing/analysis e.g., fuzzy geometrical properties, correlation, bound functions and entropy measures. Also given is a formulation of algorithms along with management of uncertainties for segmentation and object extraction, and edge detection. The output obtained here is both fuzzy and nonfuzzy. Ambiguity in evaluation and assessment of membership function are also described."
Multi-spectral image dissector camera system,88.78468,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"The image dissector sensor for the Earth Resources Program is evaluated using contrast and reflectance data. The ground resolution obtainable for low contrast at the targeted signal to noise ratio of 1.8 was defined. It is concluded that the system is capable of achieving the detection of small, low contrast ground targets from satellites."
PCB Quality Metrics that Drive Reliability (PD 18),88.764046,image quality detection,['Quality Assurance and Reliability'],"Risk based technology infusion is a deliberate and systematic process which defines the analysis and communication methodology by which new technology is applied and integrated into existing and new designs, identifies technology development needs based on trends analysis and facilitates the identification of shortfalls against performance objectives. This presentation at IPC Works Asia Aerospace 2019 Events provides the audience a snapshot of quality variations in printed wiring board quality, as assessed, using experiences in processing and risk analysis of PWB structural integrity coupons. The presentation will focus on printed wiring board quality metrics used, the relative type and number of non-conformances observed and trend analysis using statistical methods. Trend analysis shows the top five non-conformances observed across PWB suppliers, the root cause(s) behind these non-conformance and suggestions of mitigation plans. The trends will then be matched with the current state of the PWB supplier base and its challenges and opportunities. The presentation further discusses the risk based SMA approaches and methods being applied at GSFC for evaluating candidate printed wiring board technologies which promote the adoption of higher throughput and faster processing technology for GSFC missions.

"
Image Sensors Enhance Camera Technologies,88.52004,image quality detection,['Man/System Technology and Life Support'],"In the 1990s, a Jet Propulsion Laboratory team led by Eric Fossum researched ways of improving complementary metal-oxide semiconductor (CMOS) image sensors in order to miniaturize cameras on spacecraft while maintaining scientific image quality. Fossum s team founded a company to commercialize the resulting CMOS active pixel sensor. Now called the Aptina Imaging Corporation, based in San Jose, California, the company has shipped over 1 billion sensors for use in applications such as digital cameras, camera phones, Web cameras, and automotive cameras. Today, one of every three cell phone cameras on the planet feature Aptina s sensor technology."
Study Thermographic Flaw Detection,88.41671,image quality detection,['Quality Assurance and Reliability'],"The development of thermographic inspection methods for use on aerospace structures is under investigation. Several different material systems, structural geometries and defect types have been included in this study so as to establish a baseline from which future Infrared Thermography(IRT) testing can be made. This study examines various thermal loading techniques in an attempt to enhance the likelihood of capturing and identifying critically sized flaws under 'non laboratory' actual working conditions. Qualification techniques and calibration standards are also being investigated to standardize the thermographic method. In conjunction with the thermographic inspections, advanced image processing techniques including digital filtering and neural networks have been investigated to increase the ability of detecting and sizing flaws. Here, the digitized thermographic images are mathematically manipulated through various filtering techniques and/or artificial neural mapping schemes to enhance its overall quality, permitting accurate flaw identification even when the signal-to-noise ratio is low."
Water quality monitor,88.264244,image quality detection,['ENVIRONMENT POLLUTION'],"The preprototype water quality monitor (WQM) subsystem was designed based on a breadboard monitor for pH, specific conductance, and total organic carbon (TOC). The breadboard equipment demonstrated the feasibility of continuous on-line analysis of potable water for a spacecraft. The WQM subsystem incorporated these breadboard features and, in addition, measures ammonia and includes a failure detection system. The sample, reagent, and standard solutions are delivered to the WQM sensing manifold where chemical operations and measurements are performed using flow through sensors for conductance, pH, TOC, and NH3. Fault monitoring flow detection is also accomplished in this manifold assembly. The WQM is designed to operate automatically using a hardwired electronic controller. In addition, automatic shutdown is incorporated which is keyed to four flow sensors strategically located within the fluid system."
Study Thermographic Flaw Detection,88.256584,image quality detection,['Quality Assurance and Reliability'],"The development of thermographic inspection methods for use on aerospace structures is under investigation. Several different material systems, structural geometries and defect types have been included in this study so as to establish a baseline from which future IRT testing can be made. This study examines various thermal loading techniques in an attempt to enhance the likelihood of capturing and identifying critically sized flaws under 'non-laboratory' actual working conditions. Qualification techniques and calibration standards are also being investigated to standardize the thermographic method. In conjunction with the thermographic inspections, advanced image processing techniques including digital filtering and neural networks have been investigated to increase the ability of 91 detecting and sizing flaws. Here, the digitized thermographic images are mathematically manipulated through various filtering techniques and/or artificial neural mapping schemes to enhance its overall quality, permitting accurate flaw identification even when the signal-to-noise ratio is low."
NASA NDE Program,88.25264,image quality detection,['Quality Assurance and Reliability'],"The current activities in the National Aeronautics and Space Administration Nondestructive Evaluation (NDE) Program are presented. The topics covered include organizational communications, orbital weld inspection, electric field imaging, fracture critical probability of detection validation, monitoring of thermal protection systems, physical and document standards, image quality indicators, integrity of composite pressure vessels, and NDE for additively manufactured components."
Smart Camera Technology Increases Quality,87.896545,image quality detection,['Instrumentation and Photography'],"When it comes to real-time image processing, everyone is an expert. People begin processing images at birth and rapidly learn to control their responses through the real-time processing of the human visual system. The human eye captures an enormous amount of information in the form of light images. In order to keep the brain from becoming overloaded with all the data, portions of an image are processed at a higher resolution than others, such as a traffic light changing colors. changing colors. In the same manner, image processing products strive to extract the information stored in light in the most efficient way possible. Digital cameras available today capture millions of pixels worth of information from incident light. However, at frame rates more than a few per second, existing digital interfaces are overwhelmed. All the user can do is store several frames to memory until that memory is full and then subsequent information is lost. New technology pairs existing digital interface technology with an off-the-shelf complementary metal oxide semiconductor (CMOS) imager to provide more than 500 frames per second of specialty image processing. The result is a cost-effective detection system unlike any other."
Quality and Control of Water Vapor Winds,87.761894,image quality detection,['Meteorology and Climatology'],"Water vapor imagery from the geostationary satellites such as GOES, Meteosat, and GMS provides synoptic views of dynamical events on a continual basis. Because the imagery represents a non-linear combination of mid- and upper-tropospheric thermodynamic parameters (three-dimensional variations in temperature and humidity), video loops of these image products provide enlightening views of regional flow fields, the movement of tropical and extratropical storm systems, the transfer of moisture between hemispheres and from the tropics to the mid- latitudes, and the dominance of high pressure systems over particular regions of the Earth. Despite the obvious larger scale features, the water vapor imagery contains significant image variability down to the single 8 km GOES pixel. These features can be quantitatively identified and tracked from one time to the next using various image processing techniques. Merrill et al. (1991), Hayden and Schmidt (1992), and Laurent (1993) have documented the operational procedures and capabilities of NOAA and ESOC to produce cloud and water vapor winds. These techniques employ standard correlation and template matching approaches to wind tracking and use qualitative and quantitative procedures to eliminate bad wind vectors from the wind data set. Techniques have also been developed to improve the quality of the operational winds though robust editing procedures (Hayden and Veldon 1991). These quality and control approaches have limitations, are often subjective, and constrain wind variability to be consistent with model derived wind fields. This paper describes research focused on the refinement of objective quality and control parameters for water vapor wind vector data sets. New quality and control measures are developed and employed to provide a more robust wind data set for climate analysis, data assimilation studies, as well as operational weather forecasting. The parameters are applicable to cloud-tracked winds as well with minor modifications. The improvement in winds through use of these new quality and control parameters is measured without the use of rawinsonde or modeled wind field data and compared with other approaches."
LANDSAT image differencing as an automated land cover change detection technique,87.29278,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"Image differencing was investigated as a technique for use with LANDSAT digital data to delineate areas of land cover change in an urban environment. LANDSAT data collected in April 1973 and April 1975 for Austin, Texas, were geometrically corrected and precisely registered to United States Geological Survey 7.5-minute quadrangle maps. At each pixel location reflectance values for the corresponding bands were subtracted to produce four difference images. Areas of major reflectance differences are isolated by thresholding each of the difference images. The resulting images are combined to obtain an image data set to total change. These areas of reflectance differences were found, in general, to correspond to areas of land cover change. Information on areas of land cover change was incorporated into a procedure to mask out all nonchange areas and perform an unsupervised classification only for data in the change areas. This procedure identified three broad categories: (1) areas of high reflectance (construction or extractive), (2) changes in agricultural areas, and (3) areas of confusion between agricultural and other areas."
Spatial vision processes: From the optical image to the symbolic structures of contour information,87.25677,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"The significance of machine and natural vision is discussed together with the need for a general approach to image acquisition and processing aimed at recognition. An exploratory scheme is proposed which encompasses the definition of spatial primitives, intrinsic image properties and sampling, 2-D edge detection at the smallest scale, the construction of spatial primitives from edges, and the isolation of contour information from textural information. Concepts drawn from or suggested by natural vision at both perceptual and physiological levels are relied upon heavily to guide the development of the overall scheme. The scheme is intended to provide a larger context in which to place the emerging technology of detector array focal-plane processors. The approach differs from many recent efforts in edge detection and image coding by emphasizing smallest scale edge detection as a foundation for multi-scale symbolic processing while diminishing somewhat the importance of image convolutions with multi-scale edge operators. Cursory treatments of information theory illustrate that the direct application of this theory to structural information in images could not be realized."
LANDSAT TM image data quality analysis for energy-related applications,86.83948,image quality detection,['EARTH RESOURCES AND REMOTE SENSING'],"This project represents a no-cost agreement between National Aeronautic Space Administration Goddard Space Flight Center (NASA GSFC) and the Pacific Northwest Laboratory (PNL). PNL is a Department of Energy (DOE) national laboratory operted by Battelle Memorial Institute at its Pacific Northwest Laboratories in Richland, Washington. The objective of this investigation is to evaluate LANDSAT's thematic mapper (TM) data quality and utility characteristics from an energy research and technological perspective. Of main interest is the extent to which repetitive TM data might support DOE efforts relating to siting, developing, and monitoring energy-related facilities, and to basic geoscientific research. The investigation utilizes existing staff and facility capabilities, and ongoing programmatic activities at PNL and other DOE national laboratories to cooperatively assess the potential usefulness of the improved experimental TM data. The investigation involves: (1) both LANDSAT 4 and 5 TM data, (2) qualitative and quantitative use consideration, and 3) NASA P (corrected) and A (uncorrected) CCT analysis for a variety of sites of DOE interest. Initial results were presented at the LANDSAT Investigator's Workshops and at specialized LANDSAT TM sessions at various conferences."
Image Sensor Technology,86.80796,image quality detection,['COMMUNICATIONS'],Video image data transmission using millimeter wave relay satellites - image sensor systems
"Proceedings of the Image Intensifier Symposium, October 24-26, 1961, Fort Belvoir, VA",86.415436,image quality detection,['NAVIGATION'],Proceedings of second image intensifier symposium
"Digital mammography, cancer screening: Factors important for image compression",86.03673,image quality detection,['AEROSPACE MEDICINE'],"The use of digital mammography for breast cancer screening poses several novel problems such as development of digital sensors, computer assisted diagnosis (CAD) methods for image noise suppression, enhancement, and pattern recognition, compression algorithms for image storage, transmission, and remote diagnosis. X-ray digital mammography using novel direct digital detection schemes or film digitizers results in large data sets and, therefore, image compression methods will play a significant role in the image processing and analysis by CAD techniques. In view of the extensive compression required, the relative merit of 'virtually lossless' versus lossy methods should be determined. A brief overview is presented here of the developments of digital sensors, CAD, and compression methods currently proposed and tested for mammography. The objective of the NCI/NASA Working Group on Digital Mammography is to stimulate the interest of the image processing and compression scientific community for this medical application and identify possible dual use technologies within the NASA centers."
Proceedings of the Third Annual Symposium on Mathematical Pattern Recognition and Image Analysis,85.96962,image quality detection,['NUMERICAL ANALYSIS'],Topics addressed include: multivariate spline method; normal mixture analysis applied to remote sensing; image data analysis; classifications in spatially correlated environments; probability density functions; graphical nonparametric methods; subpixel registration analysis; hypothesis integration in image understanding systems; rectification of satellite scanner imagery; spatial variation in remotely sensed images; smooth multidimensional interpolation; and optimal frequency domain textural edge detection filters.
High compression image and image sequence coding,85.84064,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"The digital representation of an image requires a very large number of bits. This number is even larger for an image sequence. The goal of image coding is to reduce this number, as much as possible, and reconstruct a faithful duplicate of the original picture or image sequence. Early efforts in image coding, solely guided by information theory, led to a plethora of methods. The compression ratio reached a plateau around 10:1 a couple of years ago. Recent progress in the study of the brain mechanism of vision and scene analysis has opened new vistas in picture coding. Directional sensitivity of the neurones in the visual pathway combined with the separate processing of contours and textures has led to a new class of coding methods capable of achieving compression ratios as high as 100:1 for images and around 300:1 for image sequences. Recent progress on some of the main avenues of object-based methods is presented. These second generation techniques make use of contour-texture modeling, new results in neurophysiology and psychophysics and scene analysis."
Wire Detection Algorithms for Navigation,85.66969,image quality detection,"['Cybernetics, Artificial Intelligence and Robotics']","In this research we addressed the problem of obstacle detection for low altitude rotorcraft flight. In particular, the problem of detecting thin wires in the presence of image clutter and noise was studied. Wires present a serious hazard to rotorcrafts. Since they are very thin, their detection early enough so that the pilot has enough time to take evasive action is difficult, as their images can be less than one or two pixels wide. Two approaches were explored for this purpose. The first approach involved a technique for sub-pixel edge detection and subsequent post processing, in order to reduce the false alarms. After reviewing the line detection literature, an algorithm for sub-pixel edge detection proposed by Steger was identified as having good potential to solve the considered task. The algorithm was tested using a set of images synthetically generated by combining real outdoor images with computer generated wire images. The performance of the algorithm was evaluated both, at the pixel and the wire levels. It was observed that the algorithm performs well, provided that the wires are not too thin (or distant) and that some post processing is performed to remove false alarms due to clutter. The second approach involved the use of an example-based learning scheme namely, Support Vector Machines. The purpose of this approach was to explore the feasibility of an example-based learning based approach for the task of detecting wires from their images. Support Vector Machines (SVMs) have emerged as a promising pattern classification tool and have been used in various applications. It was found that this approach is not suitable for very thin wires and of course, not suitable at all for sub-pixel thick wires. High dimensionality of the data as such does not present a major problem for SVMs. However it is desirable to have a large number of training examples especially for high dimensional data. The main difficulty in using SVMs (or any other example-based learning method) is the need for a very good set of positive and negative examples since the performance depends on the quality of the training set."
"Planetary Crater Detection and Registration Using Marked Point Processes, Multiple Birth and Death Algorithms, and Region-Based Analysis",85.552734,image quality detection,['Statistics and Probability'],"Because of the large variety of sensors and spacecraft collecting data, planetary science needs to integrate various multi-sensor and multi-temporal images. These multiple data represent a precious asset, as they allow the study of targets spectral responses and of changes in the surface structure; because of their variety, they also require accurate and robust registration. A new crater detection algorithm, used to extract features that will be integrated in an image registration framework, is presented. A marked point process-based method has been developed to model the spatial distribution of elliptical objects (i.e. the craters) and a birth-death Markov chain Monte Carlo method, coupled with a region-based scheme aiming at computational efficiency, is used to find the optimal configuration fitting the image. The extracted features are exploited, together with a newly defined fitness function based on a modified Hausdorff distance, by an image registration algorithm whose architecture has been designed to minimize the computational time."
Contamination detection NDE for cleaning process inspection,85.382706,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"In the joining of multilayer materials, and in welding, the cleanliness of the joining surface may play a large role in the quality of the resulting bond. No non-intrusive techniques are currently available for the rapid measurement of contamination on large or irregularly shaped structures prior to the joining process. An innovative technique for the measurement of contaminant levels in these structures using laser based imaging is presented. The approach uses an ultraviolet excimer laser to illuminate large and/or irregular surface areas. The UV light induces fluorescence and is scattered from the contaminants. The illuminated area is viewed by an image-intensified CCD (charge coupled device) camera interfaced to a PC-based computer. The camera measures the fluorescence and/or scattering from the contaminants for comparison with established standards. Single shot measurements of contamination levels are possible. Hence, the technique may be used for on-line NDE testing during manufacturing processes."
Algal Accessory Pigment Detection Using AVIRIS Image-Derived Spectral Radiance Data,85.35386,image quality detection,['Earth Resources and Remote Sensing'],"Visual and derivative analyses of AVIRIS spectral data can be used to detect algal accessory pigments in aquatic communities. This capability extends the use of remote sensing for the study of aquatic ecosystems by allowing detection of taxonomically significant pigment signatures which yield information about the type of algae present. Such information allows remote sensing-based assessment of aquatic ecosystem health, as in the detection of nuisance blooms of cyanobacteria or toxic blooms of dinoflagellates. Remote sensing of aquatic systems has traditionally focused on quantification of chlorophyll a, a photoreactive (and light-harvesting) pigment which is common to all algae as well as cyanobacteria (bluegreen algae). Due to the ubiquitousness of this pigment within algae, chl a is routinely measured to estimate algal biomass both during ground-truthing and using various airborne or satellite based sensors, including AVIRIS. Within the remote sensing and aquatic sciences communities, ongoing research has been performed to detect algal accessory pigments for assessment of algal population composition. This research is based on the fact that many algal accessory pigments are taxonomically significant, and all are spectrally unique. Aquatic scientists have been refining pigment analysis techniques, primarily high performance liquid chromatography, or HPLC, to detect specific pigments as a time-saving alternative to individual algal cell identifications and counts. Remote sensing scientists are investigating the use of pigment signatures to construct pigment libraries analogous to mineral spectral libraries used in geological remote sensing applications. The accessory pigment approach has been used successfully in remote sensing using data from the Thematic Mapper, low-altitude, multiple channel scanners, field spectroradiometers and the AVIRIS hyperspectral scanner. Due to spectral and spatial resolution capabilities, AVIRIS is the sensor of choice for such studies. We present here our results on detection of algal accessory pigments using AVIRIS data."
"Planetary Crater Detection and Registration Using Marked Point Processes, Multiple Birth and Death Algorithms, and Region-Based Analysis",85.23957,image quality detection,"['Cybernetics, Artificial Intelligence and Robotics']","Because of the large variety of sensors and spacecraft collecting data, planetary science needs to integrate various multi-sensor and multi-temporal images. These multiple data represent a precious asset, as they allow the study of targets spectral responses and of changes in the surface structure; because of their variety, they also require accurate and robust registration. A new crater detection algorithm, used to extract features that will be integrated in an image registration framework, is presented. A marked point process-based method has been developed to model the spatial distribution of elliptical objects (i.e. the craters) and a birth-death Markov chain Monte Carlo method, coupled with a region-based scheme aiming at computational efficiency, is used to find the optimal configuration fitting the image. The extracted features are exploited, together with a newly defined fitness function based on a modified Hausdorff distance, by an image registration algorithm whose architecture has been designed to minimize the computational time. "
Validation of the LaSRC Cloud Detection Algorithm for Landsat 8 Images,85.05701,image quality detection,['Earth Resources and Remote Sensing'],"This study aims at validating the cloud mask produced by the Land Surface Reflectance Code (LaSRC) for Landsat 8 data. To detect clouds in optical satellite imagery, LaSRC uses quality assurance (QA) layers, which are produced during the atmospheric correction process. The QA layers include a ""Cloud mask"", which is based on the estimation of a residual metric showing the quality of aerosol inversion, and ""High aerosol"", which shows the impact of aerosols on the derived surface reflectance. Validation is performed using the ""L8 Biome"" cloud validation dataset, which is produced by the US Geological Survey (USGS), and consists of 96 Landsat 8 scenes distributed globally over 12 different biomes. We show that the LaSRC cloud detection algorithm reliably identifies thick clouds with commission and omission errors less than 4%. Large cloud overdetection errors occur for thin clouds, which is due to the subjectivity of defining and extracting thin clouds in the reference dataset. We conclude this paper with recommendations on using the LaSRC QA layers, and give suggestions on reducing subjectivity, when generating cloud validation datasets."
Smart Image Enhancement Process,84.6579,image quality detection,['Instrumentation and Photography'],"Contrast and lightness measures are used to first classify the image as being one of non-turbid and turbid. If turbid, the original image is enhanced to generate a first enhanced image. If non-turbid, the original image is classified in terms of a merged contrast/lightness score based on the contrast and lightness measures. The non-turbid image is enhanced to generate a second enhanced image when a poor contrast/lightness score is associated therewith. When the second enhanced image has a poor contrast/lightness score associated therewith, this image is enhanced to generate a third enhanced image. A sharpness measure is computed for one image that is selected from (i) the non-turbid image, (ii) the first enhanced image, (iii) the second enhanced image when a good contrast/lightness score is associated therewith, and (iv) the third enhanced image. If the selected image is not-sharp, it is sharpened to generate a sharpened image. The final image is selected from the selected image and the sharpened image."
Application of an electronic image analyzer to dimensional measurements from neutron radiographs,84.498764,image quality detection,['INSTRUMENTATION AND PHOTOGRAPHY'],"Means of obtaining improved dimensional measurements from neutron radiographs of nuclear fuel elements are discussed. The use of video-electronic image analysis relative to edge definition in radiographic images is described. Based on this study, an edge definition criterion is proposed for overcoming image unsharpness effects in taking accurate diametral measurements from radiographs. An electronic density slicing method for automatic edge definition is described. Results of measurements made with video micrometry are compared with scanning microdensitometer and micrometric physical measurements. An image quality indicator for estimating photographic and geometric unsharpness is described."
Target Detection Using Fractal Geometry,84.210396,image quality detection,['COMPUTER PROGRAMMING AND SOFTWARE'],"The concepts and theory of fractal geometry were applied to the problem of segmenting a 256 x 256 pixel image so that manmade objects could be extracted from natural backgrounds. The two most important measurements necessary to extract these manmade objects were fractal dimension and lacunarity. Provision was made to pass the manmade portion to a lookup table for subsequent identification. A computer program was written to construct cloud backgrounds of fractal dimensions which were allowed to vary between 2.2 and 2.8. Images of three model space targets were combined with these backgrounds to provide a data set for testing the validity of the approach. Once the data set was constructed, computer programs were written to extract estimates of the fractal dimension and lacunarity on 4 x 4 pixel subsets of the image. It was shown that for clouds of fractal dimension 2.7 or less, appropriate thresholding on fractal dimension and lacunarity yielded a 64 x 64 edge-detected image with all or most of the cloud background removed. These images were enhanced by an erosion and dilation to provide the final image passed to the lookup table. While the ultimate goal was to pass the final image to a neural network for identification, this work shows the applicability of fractal geometry to the problems of image segmentation, edge detection and separating a target of interest from a natural background."
Computer image processing:  Geologic applications,83.97037,image quality detection,['GEOPHYSICS'],"Computer image processing of digital data was performed to support several geological studies. The specific goals were to: (1) relate the mineral content to the spectral reflectance of certain geologic materials, (2) determine the influence of environmental factors, such as atmosphere and vegetation, and (3) improve image processing techniques. For detection of spectral differences related to mineralogy, the technique of band ratioing was found to be the most useful. The influence of atmospheric scattering and methods to correct for the scattering were also studied. Two techniques were used to correct for atmospheric effects: (1) dark object subtraction, (2) normalization of use of ground spectral measurements. Of the two, the first technique proved to be the most successful for removing the effects of atmospheric scattering. A digital mosaic was produced from two side-lapping LANDSAT frames. The advantages were that the same enhancement algorithm can be applied to both frames, and there is no seam where the two images are joined."
"Detection of water bodies in Saline County, Kansas",83.89349,image quality detection,['GEOPHYSICS'],"The author has identified the following significant results. A total of 2,272 water bodies were mapped in Saline County, Kansas in 1972 using ERTS-1 imagery. A topographic map of 1955 shows 1,056 water bodies in the county. The major increase took place in farm ponds. Preliminary comparison of image and maps indicates that water bodies larger than ten acres in area proved consistently detectable. Most water areas between four and ten acres are also detectable, although occasionally image context prevents detection. Water areas less than four acres in extent are sometimes detected, but the number varies greatly depending on image context and the individual interpretor."
Software for Automated Image-to-Image Co-registration,83.829636,image quality detection,['Computer Programming and Software'],"The project objectives are: a) Develop software to fine-tune image-to-image co-registration, presuming images are orthorectified prior to input; b) Create a reusable software development kit (SDK) to enable incorporation of these tools into other software; d) provide automated testing for quantitative analysis; and e) Develop software that applies multiple techniques to achieve subpixel precision in the co-registration of image pairs."
Unsupervised Detection of Planetary Craters by a Marked Point Process,83.80218,image quality detection,['Space Sciences (General)'],"With the launch of several planetary missions in the last decade, a large amount of planetary images is being acquired. Preferably, automatic and robust processing techniques need to be used for data analysis because of the huge amount of the acquired data. Here, the aim is to achieve a robust and general methodology for crater detection. A novel technique based on a marked point process is proposed. First, the contours in the image are extracted. The object boundaries are modeled as a configuration of an unknown number of random ellipses, i.e., the contour image is considered as a realization of a marked point process. Then, an energy function is defined, containing both an a priori energy and a likelihood term. The global minimum of this function is estimated by using reversible jump Monte-Carlo Markov chain dynamics and a simulated annealing scheme. The main idea behind marked point processes is to model objects within a stochastic framework: Marked point processes represent a very promising current approach in the stochastic image modeling and provide a powerful and methodologically rigorous framework to efficiently map and detect objects and structures in an image with an excellent robustness to noise. The proposed method for crater detection has several feasible applications. One such application area is image registration by matching the extracted features. "
The 3-D image recognition based on fuzzy neural network technology,83.74273,image quality detection,['CYBERNETICS'],"Three dimensional stereoscopic image recognition system based on fuzzy-neural network technology was developed. The system consists of three parts; preprocessing part, feature extraction part, and matching part. Two CCD color camera image are fed to the preprocessing part, where several operations including RGB-HSV transformation are done. A multi-layer perception is used for the line detection in the feature extraction part. Then fuzzy matching technique is introduced in the matching part. The system is realized on SUN spark station and special image input hardware system. An experimental result on bottle images is also presented."
Obstacle Detection Algorithms for Rotorcraft Navigation,83.05456,image quality detection,['Air Transportation and Safety'],"In this research we addressed the problem of obstacle detection for low altitude rotorcraft flight. In particular, the problem of detecting thin wires in the presence of image clutter and noise was studied. Wires present a serious hazard to rotorcrafts. Since they are very thin, their detection early enough so that the pilot has enough time to take evasive action is difficult, as their images can be less than one or two pixels wide. After reviewing the line detection literature, an algorithm for sub-pixel edge detection proposed by Steger was identified as having good potential to solve the considered task. The algorithm was tested using a set of images synthetically generated by combining real outdoor images with computer generated wire images. The performance of the algorithm was evaluated both, at the pixel and the wire levels. It was observed that the algorithm performs well, provided that the wires are not too thin (or distant) and that some post processing is performed to remove false alarms due to clutter."
Assessing Reliability of NDE Flaw Detection Using Smaller Number of Demonstration Data Points,82.81573,image quality detection,['Quality Assurance and Reliability'],"The paper provides an engineering analysis approach for assessing reliability of NDE flaw detection using smaller number of demonstration data points. It explores dependence of probability of detection (POD), probability of false positive (POF), on contrast-to-noise ratio, and net decision threshold-to-noise ratio in a simulated data; and draws some generically applicable inferences to devise the approach. ASTM nondestructive evaluation standards provide requirements on signal-to-noise ratio and/or contrast-to-noise ratio in order to provide reliable flaw detection and limit false positive calls. POD analysis of inspection test data results in an estimated flaw size, denoted by 90/95. This flaw size has 90% POD and minimum 95% confidence. POF is also estimated in the analysis. POD demonstration requires specimens with flaws of known size. In many situations, it is very expensive to produce the large number of flaws required for the POD analysis. In some situations, only real flaws can truly represent the flaws for demonstration. Real flaws of correct size and location in part configuration specimen may be difficult to produce, if not impossible. Here, an engineering analysis approach is devised using simulation to assess reliability of NDE technique when a limited number of flaws are available for demonstration. In this simulation, a technique is considered reliable, if it provides flaw detectability size equal to or better than the theoretical 90 used in simulation and also provides a POF less than or equal to a chosen value. The paper uses simulated signal response versus flaw size data to devise the approach. Linear correlation is used between the signal response data and flaw size. POD software mh1823 uses generalized linear model (GLM) in POD analysis after transforming the flaw size and signal response, if needed, using logarithm. Therefore, this approach is in agreement with the linear signal correlation used in mh1823. Using the POD analysis of data, generic conditions on contrast-to-noise ratio and net decision threshold-to-noise ratio are derived for reliable flaw detection. In order to assess technique reliability using the engineering approach, signal response-to-flaw size correlation about the flaw size of concern is needed. In addition, measurement of noise is also needed. If the technique meets the above requirements, assumption of linear signal-to-flaw size correlation and conditions on noise, then the technique can be assessed using this analysis as it fits the underlying POD model used here. The approach is conservative and is designed to provide a larger flaw size compared to the POD approach. Such NDE technique assessment approach, although, not as rigorous as POD, can be cost effective if the larger flaw size can be tolerated. Typically, this is a situation for all quality control NDE inspections. Here, an NDE technique needs to be reliable and 90/95 is not estimated, but the assessed flaw size is assumed to be larger than the unknown a90 due to conservative factors or margins. Applicability of the approach for assessing reliability of flaw detection in x-ray radiography and 2D imaging in general is also explored."
Motion Estimation Utilizing Range Detection-Enhanced Visual Odometry,82.77214,image quality detection,['Instrumentation and Photography'],"A motion determination system is disclosed. The system may receive a first and a second camera image from a camera, the first camera image received earlier than the second camera image. The system may identify corresponding features in the first and second camera images. The system may receive range data comprising at least one of a first and a second range data from a range detection unit, corresponding to the first and second camera images, respectively. The system may determine first positions and the second positions of the corresponding features using the first camera image and the second camera image. The first positions or the second positions may be determined by also using the range data. The system may determine a change in position of the machine based on differences between the first and second positions, and a VO-based velocity of the machine based on the determined change in position."
A New Approach to Image Fusion Based on Cokriging,82.75761,image quality detection,['Earth Resources and Remote Sensing'],We consider the image fusion problem involving remotely sensed data. We introduce cokriging as a method to perform fusion. We investigate the advantages of fusing Hyperion with ALI. The evaluation is performed by comparing the classification of the fused data with that of input images and by calculating well-chosen quantitative fusion quality metrics. We consider the Invasive Species Forecasting System (ISFS) project as our fusion application. The fusion of ALI with Hyperion data is studies using PCA and wavelet-based fusion. We then propose utilizing a geostatistical based interpolation method called cokriging as a new approach for image fusion.
Introduction to computer image processing,82.72554,image quality detection,['COMPUTERS'],"Theoretical backgrounds and digital techniques for a class of image processing problems are presented. Image formation in the context of linear system theory, image evaluation, noise characteristics, mathematical operations on image and their implementation are discussed. Various techniques for image restoration and image enhancement are presented. Methods for object extraction and the problem of pictorial pattern recognition and classification are discussed."
NASA total quality management 1989 accomplishments report,82.45888,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"Described here are the accomplishments of NASA as a result of the use of Total Quality Management (TQM). The principles in practice which led to these process refinements are important cultural elements to any organization's productivity and quality efforts. The categories of TQM discussed here are top management leadership and support, strategic planning, focus on the customer, employee training and recognition, employee empowerment and teamwork, measurement and analysis, and quality assurance."
ON A SATELLITE SURVEILLANCE CAMERA USING COMPOUND OPTICS AND IMAGE ORTHICON PHOTOSENSORS FACET EYE CAMERA,82.25175,image quality detection,['NAVIGATION'],Satellite tracking camera with optics and image orthicons
New nondestructive techniques for the detection and quantification of corrosion in aircraft structures,82.20065,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"An overview is presented of several techniques under development at NASA Langley Research Center for detection and quantification of corrosion in aircraft structures. The techniques have been developed as part of the NASA Airframe Structural Integrity Program. The techniques focus on the detection of subsurface corrosion in thin laminated structures. Results are presented on specimens with both manufactured defects, for calibration of the techniques, and on specimens removed from aircraft."
Video image processing,82.0427,image quality detection,['COMPUTER PROGRAMMING AND SOFTWARE'],"Current technology projections indicate a lack of availability of special purpose computing for Space Station applications. Potential functions for video image special purpose processing are being investigated, such as smoothing, enhancement, restoration and filtering, data compression, feature extraction, object detection and identification, pixel interpolation/extrapolation, spectral estimation and factorization, and vision synthesis. Also, architectural approaches are being identified and a conceptual design generated. Computationally simple algorithms will be research and their image/vision effectiveness determined. Suitable algorithms will be implimented into an overall architectural approach that will provide image/vision processing at video rates that are flexible, selectable, and programmable. Information is given in the form of charts, diagrams and outlines."
Automated System for Early Breast Cancer Detection in Mammograms,81.93198,image quality detection,['AEROSPACE MEDICINE'],"The increasing demand on mammographic screening for early breast cancer detection, and the subtlety of early breast cancer signs on mammograms, suggest an automated image processing system that can serve as a diagnostic aid in radiology clinics. We present a fully automated algorithm for detecting clusters of microcalcifications that are the most common signs of early, potentially curable breast cancer. By using the contour map of the mammogram, the algorithm circumvents some of the difficulties encountered with standard image processing methods. The clinical implementation of an automated instrument based on this algorithm is also discussed."
Comparison of Transesophageal and Transthoracic Contrast Echocardiography for Detection of a Patent Foramen Ovale,81.891495,image quality detection,['Life Sciences (General)'],"Presence of a patent foramen ovale may indicate paradoxic embolism in patients with otherwise unexplained embolic disease. Transthoracic contrast echocardiography has been used as a simple technique for detecting patent foramen ovale. However, particularly in patients with poor transthoracic image quality, presence of a patent foramen ovale might be missed. Transesophageal contrast echocardiography provides superior visualization of the atrial septum and therefore is believed to improve diagnostic accuracy. The present study investigates the influence of image quality on the detection of a patent foramen ovale by both transthoracic and transesophageal contrast echocardiography."
The Corralitos Observatory program for the detection of lunar transient phenomena,81.34301,image quality detection,['LUNAR AND PLANETARY EXPLORATION'],"This is a final report on the establishment, observing procedures, and observational results of a survey program for the detection of lunar transient phenomena (LTP's) by electro-optical image conversion means. For survey, a unique detection system with an image orthicon was used as the primary element in conjunction with a 24-in. f/20 Cassegrainian telescope. Observations in three spectral ranges, with 6,466 man-hours of observing, were actually performed during the period from October 27, 1965, to April 26, 1972. Within this entire period, no color or feature change within the detection capabilities of the instrumentation was observed, either independently or in follow up of amateur LTP reports, with the exception of one general bluing and several localized bluings (probably ascribable to the effects of the terrestrial atmosphere) that were observed solely by the Corralitos system. A table is presented indicating amateur and professional reports of LTP's and the results of efforts to confirm these reports through the Corralitos system."
Multi-Sensor Mud Detection,81.19363,image quality detection,['Man/System Technology and Life Support'],"Robust mud detection is a critical perception requirement for Unmanned Ground Vehicle (UGV) autonomous offroad navigation. A military UGV stuck in a mud body during a mission may have to be sacrificed or rescued, both of which are unattractive options. There are several characteristics of mud that may be detectable with appropriate UGV-mounted sensors. For example, mud only occurs on the ground surface, is cooler than surrounding dry soil during the daytime under nominal weather conditions, is generally darker than surrounding dry soil in visible imagery, and is highly polarized. However, none of these cues are definitive on their own. Dry soil also occurs on the ground surface, shadows, snow, ice, and water can also be cooler than surrounding dry soil, shadows are also darker than surrounding dry soil in visible imagery, and cars, water, and some vegetation are also highly polarized. Shadows, snow, ice, water, cars, and vegetation can all be disambiguated from mud by using a suite of sensors that span multiple bands in the electromagnetic spectrum. Because there are military operations when it is imperative for UGV's to operate without emitting strong, detectable electromagnetic signals, passive sensors are desirable. JPL has developed a daytime mud detection capability using multiple passive imaging sensors. Cues for mud from multiple passive imaging sensors are fused into a single mud detection image using a rule base, and the resultant mud detection is localized in a terrain map using range data generated from a stereo pair of color cameras."
NASA total quality management 1989 accomplishments report,80.86276,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"NASA and contractor employees achieved many notable improvements in 1989. The highlights of those improvements, described in this seventh annual Accomplishments Report, demonstrate that the people who support NASA's activities are getting more involved in quality and continuous improvement efforts. Their gains solidly support NASA's and this Nation's goal to remain a leader in space exploration and in world-wide market competition, and, when communicated to others through avenues such as this report, foster improvement efforts across government and industry. The principles in practice which led to these process refinements are important cultural elements to any organization's productivity and quality efforts. The categories in this report reflect NASA principles set forth in the 1980's and are more commonly known today as Total Quality Management (TQM): top management leadership and support; strategic planning; focus on the customer; employee training and recognition; employee empowerment and teamwork; measurement and analysis; and quality assurance."
Quality Assessment of Landsat Surface Reflectance Products Using MODIS Data,80.46009,image quality detection,['Earth Resources and Remote Sensing'],"Surface reflectance adjusted for atmospheric effects is a primary input for land cover change detection and for developing many higher level surface geophysical parameters. With the development of automated atmospheric correction algorithms, it is now feasible to produce large quantities of surface reflectance products using Landsat images. Validation of these products requires in situ measurements, which either do not exist or are difficult to obtain for most Landsat images. The surface reflectance products derived using data acquired by the Moderate Resolution Imaging Spectroradiometer (MODIS), however, have been validated more comprehensively. Because the MODIS on the Terra platform and the Landsat 7 are only half an hour apart following the same orbit, and each of the 6 Landsat spectral bands overlaps with a MODIS band, good agreements between MODIS and Landsat surface reflectance values can be considered indicators of the reliability of the Landsat products, while disagreements may suggest potential quality problems that need to be further investigated. Here we develop a system called Landsat-MODIS Consistency Checking System (LMCCS). This system automatically matches Landsat data with MODIS observations acquired on the same date over the same locations and uses them to calculate a set of agreement metrics. To maximize its portability, Java and open-source libraries were used in developing this system, and object-oriented programming (OOP) principles were followed to make it more flexible for future expansion. As a highly automated system designed to run as a stand-alone package or as a component of other Landsat data processing systems, this system can be used to assess the quality of essentially every Landsat surface reflectance image where spatially and temporally matching MODIS data are available. The effectiveness of this system was demonstrated using it to assess preliminary surface reflectance products derived using the Global Land Survey (GLS) Landsat images for the 2000 epoch. As surface reflectance likely will be a standard product for future Landsat missions, the approach developed in this study can be adapted as an operational quality assessment system for those missions."
Seed viability detection using computerized false-color radiographic image enhancement,80.348915,image quality detection,['LIFE SCIENCES (GENERAL)'],"Seed radiographs are divided into density zones which are related to seed germination. The seeds which germinate have densities relating to false-color red. In turn, a seed sorter may be designed which rejects those seeds not having sufficient red to activate a gate along a moving belt containing the seed source. This results in separating only seeds with the preselected densities representing biological viability lending to germination. These selected seeds demand a higher market value. Actual false-coloring isn't required for a computer to distinguish the significant gray-zone range. This range can be predetermined and screened without the necessity of red imaging. Applying false-color enhancement is a means of emphasizing differences in densities of gray within any subject from photographic, radiographic, or video imaging. Within the 0-255 range of gray levels, colors can be assigned to any single level or group of gray levels. Densitometric values then become easily recognized colors which relate to the image density. Choosing a color to identify any given density allows separation by morphology or composition (form or function). Additionally, relative areas of each color are readily available for determining distribution of that density by comparison with other densities within the image."
An efficient system for reliably transmitting image and video data over low bit rate noisy channels,80.22745,image quality detection,['COMPUTER PROGRAMMING AND SOFTWARE'],"This research project is intended to develop an efficient system for reliably transmitting image and video data over low bit rate noisy channels. The basic ideas behind the proposed approach are the following: employ statistical-based image modeling to facilitate pre- and post-processing and error detection, use spare redundancy that the source compression did not remove to add robustness, and implement coded modulation to improve bandwidth efficiency and noise rejection. Over the last six months, progress has been made on various aspects of the project. Through our studies of the integrated system, a list-based iterative Trellis decoder has been developed. The decoder accepts feedback from a post-processor which can detect channel errors in the reconstructed image. The error detection is based on the Huber Markov random field image model for the compressed image. The compression scheme used here is that of JPEG (Joint Photographic Experts Group). Experiments were performed and the results are quite encouraging. The principal ideas here are extendable to other compression techniques. In addition, research was also performed on unequal error protection channel coding, subband vector quantization as a means of source coding, and post processing for reducing coding artifacts. Our studies on unequal error protection (UEP) coding for image transmission focused on examining the properties of the UEP capabilities of convolutional codes. The investigation of subband vector quantization employed a wavelet transform with special emphasis on exploiting interband redundancy. The outcome of this investigation included the development of three algorithms for subband vector quantization. The reduction of transform coding artifacts was studied with the aid of a non-Gaussian Markov random field model. This results in improved image decompression. These studies are summarized and the technical papers included in the appendices."
Iterative edge- and wavelet-based image registration of AVHRR and GOES satellite imagery,80.006,image quality detection,['Computer Programming and Software'],"Most automatic registration methods are either correlation-based, feature-based, or a combination of both. Examples of features which can be utilized for automatic image registration are edges, regions, corners, or wavelet-extracted features. In this paper, we describe two proposed approaches, based on edge or edge-like features, which are very appropriate to highlight regions of interest such as coastlines. The two iterative methods utilize the Normalized Cross-Correlation of edge and wavelet features and are applied to such problems as image-to-map registration, landmarking, and channel-to-channel co-registration, utilizing test data, AVHRR data, as well as GOES image data."
Simulating the X-Ray Image Contrast to Set-Up Techniques with Desired Flaw Detectability,79.92825,image quality detection,"['Instrumentation and Photography', 'Quality Assurance and Reliability']","The paper provides simulation data of previous work by the author in developing a model for estimating detectability of crack-like flaws in radiography. The methodology is being developed to help in implementation of NASA Special x-ray radiography qualification, but is generically applicable to radiography. The paper describes a method for characterizing X-ray detector resolution for crack detection. Applicability of ASTM E 2737 resolution requirements to the model are also discussed. The paper describes a model for simulating the detector resolution. A computer calculator application, discussed here, also performs predicted contrast and signal-to-noise ratio calculations. Results of various simulation runs in calculating x-ray flaw size parameter and image contrast for varying input parameters such as crack depth, crack width, part thickness, x-ray angle, part-to-detector distance, part-to-source distance, source sizes, and detector sensitivity and resolution are given as 3D surfaces. These results demonstrate effect of the input parameters on the flaw size parameter and the simulated image contrast of the crack. These simulations demonstrate utility of the flaw size parameter model in setting up x-ray techniques that provide desired flaw detectability in radiography. The method is applicable to film radiography, computed radiography, and digital radiography."
Analysis of Particle Image Velocimetry (PIV) Data for Acoustic Velocity Measurements,79.8224,image quality detection,['Lasers and Masers'],"Acoustic velocity measurements were taken using Particle Image Velocimetry (PIV) in a Normal Incidence Tube configuration at various frequency, phase, and amplitude levels. This report presents the results of the PIV analysis and data reduction portions of the test and details the processing that was done. Estimates of lower measurement sensitivity levels were determined based on PIV image quality, correlation, and noise level parameters used in the test. Comparison of measurements with linear acoustic theory are presented. The onset of nonlinear, harmonic frequency acoustic levels were also studied for various decibel and frequency levels ranging from 90 to 132 dB and 500 to 3000 Hz, respectively."
Science-based Region-of-Interest Image Compression,79.64428,image quality detection,['Computer Programming and Software'],"As the number of currently active space missions increases, so does competition for Deep Space Network (DSN) resources. Even given unbounded DSN time, power and weight constraints onboard the spacecraft limit the maximum possible data transmission rate. These factors highlight a critical need for very effective data compression schemes. Images tend to be the most bandwidth-intensive data, so image compression methods are particularly valuable. In this paper, we describe a method for prioritizing regions in an image based on their scientific value. Using a wavelet compression method that can incorporate priority information, we ensure that the highest priority regions are transmitted with the highest fidelity."
A visual detection model for DCT coefficient quantization,79.33812,image quality detection,['NUMERICAL ANALYSIS'],"The discrete cosine transform (DCT) is widely used in image compression and is part of the JPEG and MPEG compression standards. The degree of compression and the amount of distortion in the decompressed image are controlled by the quantization of the transform coefficients. The standards do not specify how the DCT coefficients should be quantized. One approach is to set the quantization level for each coefficient so that the quantization error is near the threshold of visibility. Results from previous work are combined to form the current best detection model for DCT coefficient quantization noise. This model predicts sensitivity as a function of display parameters, enabling quantization matrices to be designed for display situations varying in luminance, veiling light, and spatial frequency related conditions (pixel size, viewing distance, and aspect ratio). It also allows arbitrary color space directions for the representation of color. A model-based method of optimizing the quantization matrix for an individual image was developed. The model described above provides visual thresholds for each DCT frequency. These thresholds are adjusted within each block for visual light adaptation and contrast masking. For given quantization matrix, the DCT quantization errors are scaled by the adjusted thresholds to yield perceptual errors. These errors are pooled nonlinearly over the image to yield total perceptual error. With this model one may estimate the quantization matrix for a particular image that yields minimum bit rate for a given total perceptual error, or minimum perceptual error for a given bit rate. Custom matrices for a number of images show clear improvement over image-independent matrices. Custom matrices are compatible with the JPEG standard, which requires transmission of the quantization matrix."
Investigation of image enhancement techniques for the development of a self-contained airborne radar navigation system,79.12107,image quality detection,['AIRCRAFT COMMUNICATIONS AND NAVIGATION'],"This study was devoted to an investigation of the feasibility of applying advanced image processing techniques to enhance radar image characteristics that are pertinent to the pilot's navigation and guidance task. Millimeter (95 GHz) wave radar images for the overwater (i.e., offshore oil rigs) and overland (Heliport) scenario were used as a data base. The purpose of the study was to determine the applicability of image enhancement and scene analysis algorithms to detect and improve target characteristics (i.e., manmade objects such as buildings, parking lots, cars, roads, helicopters, towers, landing pads, etc.) that would be helpful to the pilot in determining his own position/orientation with respect to the outside world and assist him in the navigation task. Results of this study show that significant improvements in the raw radar image may be obtained using two dimensional image processing algorithms. In the overwater case, it is possible to remove the ocean clutter by thresholding the image data, and furthermore to extract the target boundary as well as the tower and catwalk locations using noise cleaning (e.g., median filter) and edge detection (e.g., Sobel operator) algorithms."
Image Analysis Based on Soft Computing and Applied on Space Shuttle During the Liftoff Process,78.75246,image quality detection,['Instrumentation and Photography'],"Imaging techniques based on Soft Computing (SC) and developed at Kennedy Space Center (KSC) have been implemented on a variety of prototype applications related to the safety operation of the Space Shuttle during the liftoff process. These SC-based prototype applications include detection and tracking of moving Foreign Objects Debris (FOD) during the Space Shuttle liftoff, visual anomaly detection on slidewires used in the emergency egress system for the Space Shuttle at the laJlIlch pad, and visual detection of distant birds approaching the Space Shuttle launch pad. This SC-based image analysis capability developed at KSC was also used to analyze images acquired during the accident of the Space Shuttle Columbia and estimate the trajectory and velocity of the foam that caused the accident."
A multiple objective optimization approach to quality control,78.64511,image quality detection,['QUALITY ASSURANCE AND RELIABILITY'],"The use of product quality as the performance criteria for manufacturing system control is explored. The goal in manufacturing, for economic reasons, is to optimize product quality. The problem is that since quality is a rather nebulous product characteristic, there is seldom an analytic function that can be used as a measure. Therefore standard control approaches, such as optimal control, cannot readily be applied. A second problem with optimizing product quality is that it is typically measured along many dimensions: there are many apsects of quality which must be optimized simultaneously. Very often these different aspects are incommensurate and competing. The concept of optimality must now include accepting tradeoffs among the different quality characteristics. These problems are addressed using multiple objective optimization. It is shown that the quality control problem can be defined as a multiple objective optimization problem. A controller structure is defined using this as the basis. Then, an algorithm is presented which can be used by an operator to interactively find the best operating point. Essentially, the algorithm uses process data to provide the operator with two pieces of information: (1) if it is possible to simultaneously improve all quality criteria, then determine what changes to the process input or controller parameters should be made to do this; and (2) if it is not possible to improve all criteria, and the current operating point is not a desirable one, select a criteria in which a tradeoff should be made, and make input changes to improve all other criteria. The process is not operating at an optimal point in any sense if no tradeoff has to be made to move to a new operating point. This algorithm ensures that operating points are optimal in some sense and provides the operator with information about tradeoffs when seeking the best operating point. The multiobjective algorithm was implemented in two different injection molding scenarios: tuning of process controllers to meet specified performance objectives and tuning of process inputs to meet specified quality objectives. Five case studies are presented."
Detection and Characterization of Boundary-Layer Transition in Flight at Supersonic Conditions Using Infrared Thermography,78.59668,image quality detection,['Aerodynamics'],"Infrared thermography is a powerful tool for investigating fluid mechanics on flight vehicles. (Can be used to visualize and characterize transition, shock impingement, separation etc.). Updated onboard F-15 based system was used to visualize supersonic boundary layer transition test article. (Tollmien-Schlichting and cross-flow dominant flow fields). Digital Recording improves image quality and analysis capability. (Allows accurate quantitative (temperature) measurements, Greater enhancement through image processing allows analysis of smaller scale phenomena)."
Low-Cost Quality Control and Nondestructive Evaluation Technologies for General Aviation Structures,78.550896,image quality detection,['Quality Assurance and Reliability'],"NASA's Advanced General Aviation Transport Experiments (AGATE) Program has as a goal to reduce the overall cost of producing private aviation aircraft while maintaining the safety of these aircraft. In order to successfully meet this goal, it is necessary to develop nondestructive inspection techniques which will facilitate the production of the materials used in these aircraft and assure the quality necessary to maintain airworthiness. This paper will discuss a particular class of general aviation materials and several nondestructive inspection techniques that have proven effective for making these inspections. Additionally, this paper will discuss the investigation and application of other commercially available quality control techniques applicable to these structures."
